{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Comprehensive Evaluation Framework for Wisent Guard\n",
    "\n",
    "This notebook provides an interactive interface for running comprehensive evaluations that properly separate:\n",
    "\n",
    "1. **🎯 Benchmark Performance**: How well the model solves mathematical problems\n",
    "2. **🔍 Probe Performance**: How well probes detect correctness from model activations\n",
    "3. **⚙️ DAC Hyperparameter Optimization**: Grid search to find optimal DAC configurations\n",
    "\n",
    "## Key Features:\n",
    "- **Real Data Integration**: Uses GSM8KExtractor to get contrastive pairs from training data\n",
    "- **DAC Hyperparameter Grid Search**: Systematic optimization of entropy_threshold, ptop, and max_alpha\n",
    "- **Real-time Progress**: Live updates during evaluation with tqdm\n",
    "- **Rich Visualizations**: Comprehensive plots and analysis\n",
    "- **Modular Design**: Clean separation of concerns\n",
    "- **Export Results**: Save results and generate reports\n",
    "\n",
    "## DAC Hyperparameters:\n",
    "- **entropy_threshold**: Controls dynamic steering based on entropy (default: 1.0)\n",
    "- **ptop**: Probability threshold for KL-based dynamic control (default: 0.4)\n",
    "- **max_alpha**: Maximum steering intensity (default: 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wandb connection cleaned up\n",
      "✅ All imports successful!\n",
      "📍 Working directory: /workspace/wisent-guard/comprehensive_evaluation\n",
      "🐍 Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
      "💾 HuggingFace cache: /workspace/.cache/huggingface\n",
      "🔗 Wandb status: ✅ Ready\n",
      "\n",
      "✅ Logged into HuggingFace as: jfpio\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set HuggingFace cache to permanent directory\n",
    "os.environ['HF_HOME'] = '/workspace/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/.cache/huggingface/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/.cache/huggingface/datasets'\n",
    "\n",
    "# Create cache directories if they don't exist\n",
    "os.makedirs('/workspace/.cache/huggingface/transformers', exist_ok=True)\n",
    "os.makedirs('/workspace/.cache/huggingface/datasets', exist_ok=True)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = '/workspace/wisent-guard'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Fix wandb connection issues\n",
    "def fix_wandb_connection():\n",
    "    \"\"\"Fix wandb connection issues by properly initializing or disabling it.\"\"\"\n",
    "    try:\n",
    "        import wandb\n",
    "        \n",
    "        # Check if wandb is already initialized\n",
    "        if wandb.run is not None:\n",
    "            print(\"⚠️ Cleaning up existing wandb run...\")\n",
    "            wandb.finish()\n",
    "        \n",
    "        # Clear any broken connections\n",
    "        import subprocess\n",
    "        import signal\n",
    "        try:\n",
    "            # Kill any hanging wandb processes\n",
    "            subprocess.run(['pkill', '-f', 'wandb'], capture_output=True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"✅ Wandb connection cleaned up\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Wandb cleanup warning: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check HuggingFace authentication\n",
    "def check_hf_auth():\n",
    "    \"\"\"Check if user is logged into HuggingFace and show login instructions if needed.\"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['huggingface-cli', 'whoami'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            username = result.stdout.strip()\n",
    "            print(f\"✅ Logged into HuggingFace as: {username}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"⚠️ Not logged into HuggingFace!\")\n",
    "            print(\"🔐 Please run: huggingface-cli login\")\n",
    "            print(\"   This is required to access datasets like AIME 2024/2025\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not check HuggingFace authentication: {e}\")\n",
    "        print(\"🔐 If you encounter dataset loading issues, try: huggingface-cli login\")\n",
    "        return False\n",
    "\n",
    "# Clean up wandb first\n",
    "wandb_ok = fix_wandb_connection()\n",
    "\n",
    "# Import comprehensive evaluation framework\n",
    "from wisent_guard.core.evaluation.comprehensive import (\n",
    "    ComprehensiveEvaluationConfig,\n",
    "    ComprehensiveEvaluationPipeline,\n",
    "    plot_evaluation_results,\n",
    "    create_results_dashboard,\n",
    "    generate_summary_report,\n",
    "    calculate_comprehensive_metrics,\n",
    "    generate_performance_summary\n",
    ")\n",
    "\n",
    "# Visualization and interactivity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Utilities\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"📍 Working directory: {os.getcwd()}\")\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "print(f\"💾 HuggingFace cache: {os.environ['HF_HOME']}\")\n",
    "print(f\"🔗 Wandb status: {'✅ Ready' if wandb_ok else '⚠️ May have issues'}\")\n",
    "print()\n",
    "\n",
    "# Check authentication\n",
    "hf_authenticated = check_hf_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Configuration\n",
    "\n",
    "Edit the constants in the next cell to customize your evaluation. All parameters are clearly documented with examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Available math tasks: ['aime', 'aime2024', 'aime2025', 'gsm8k', 'hendrycks_math', 'hmmt', 'hmmt_feb_2025', 'livemathbench', 'livemathbench_cnmo_en', 'livemathbench_cnmo_zh', 'math', 'math500', 'polymath', 'polymath_en_high', 'polymath_en_medium', 'polymath_zh_high', 'polymath_zh_medium']\n",
      "📋 CONFIGURATION SUMMARY\n",
      "==================================================\n",
      "🤖 Model: /workspace/models/llama31-8b-instruct-hf\n",
      "📊 Datasets: gsm8k → gsm8k → gsm8k\n",
      "🔢 Samples: 5 + 5 + 30 = 40 total\n",
      "🎯 Probe layers: [15]\n",
      "⚙️ Steering layers: [15]\n",
      "🎛️ Steering method: DAC (Dynamic Activation Composition)\n",
      "📊 DAC Hyperparameters:\n",
      "   • Entropy thresholds: [1.0]\n",
      "   • Ptop values: [0.5]\n",
      "   • Max alpha values: [2.0]\n",
      "📚 Using tasks from MATH_TASKS: ✓\n",
      "🧪 Total hyperparameter combinations: 3\n",
      "📈 Wandb enabled: False\n",
      "🏗️ Model layers: 32\n",
      "\n",
      "✅ Configuration validated successfully!\n",
      "💡 This evaluation now uses REAL mathematical training data instead of synthetic generation!\n",
      "🎯 DAC will be trained on actual math questions from your training dataset.\n",
      "💡 Configuration updated for quick testing with distilgpt2 and GSM8K dataset.\n",
      "🎯 Currently configured for: gsm8k\n",
      "📚 Available math tasks: 17 tasks including GSM8K, MATH-500, AIME, etc.\n",
      "💡 To change datasets, edit TRAIN_DATASET, VAL_DATASET, TEST_DATASET above.\n"
     ]
    }
   ],
   "source": [
    "# Configuration Constants - Edit these values to customize your evaluation\n",
    "\n",
    "# Import math tasks from our task configuration\n",
    "import sys\n",
    "sys.path.append('/workspace/wisent-guard')\n",
    "from wisent_guard.parameters.task_config import MATH_TASKS\n",
    "\n",
    "# Import DAC steering method\n",
    "from wisent_guard.core.steering_methods.dac import DAC\n",
    "\n",
    "# Convert MATH_TASKS set to sorted list for easier selection\n",
    "MATH_TASKS_LIST = sorted(list(MATH_TASKS))\n",
    "print(f\"📚 Available math tasks: {MATH_TASKS_LIST}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN CONFIGURATION - Edit these constants to customize your evaluation\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'distilbert/distilgpt2'  # Using smaller model for quick testing - Examples: 'distilbert/distilgpt2', 'gpt2', '/workspace/models/llama31-8b-instruct-hf', 'Qwen/Qwen3-8B'\n",
    "MODEL_NAME = \"/workspace/models/llama31-8b-instruct-hf\"\n",
    "\n",
    "# Dataset configuration - Choose from MATH_TASKS\n",
    "TRAIN_DATASET = 'gsm8k'     # Training dataset - Change to any task from MATH_TASKS_LIST\n",
    "VAL_DATASET = 'gsm8k'       # Validation dataset - Change to any task from MATH_TASKS_LIST  \n",
    "TEST_DATASET = 'gsm8k'      # Test dataset - Change to any task from MATH_TASKS_LIST\n",
    "\n",
    "# Validate dataset choices\n",
    "for dataset, name in [(TRAIN_DATASET, 'TRAIN'), (VAL_DATASET, 'VAL'), (TEST_DATASET, 'TEST')]:\n",
    "    if dataset not in MATH_TASKS:\n",
    "        raise ValueError(f\"{name}_DATASET '{dataset}' not in MATH_TASKS. Choose from: {MATH_TASKS_LIST}\")\n",
    "\n",
    "# Sample limits (small for quick testing)\n",
    "TRAIN_LIMIT = 5   # Number of training samples\n",
    "VAL_LIMIT = 5    # Number of validation samples  \n",
    "TEST_LIMIT = 30     # Number of test samples\n",
    "\n",
    "# Layer configuration - specify which layers to search during optimization\n",
    "PROBE_LAYERS = [3]     # Examples: [2, 3, 4, 5], [8, 16, 24, 32], [5, 6, 7, 8] \n",
    "STEERING_LAYERS = [3]  # Same as probe layers for now - Examples: [3, 4, 5], [16, 24, 32], [6, 8, 10]\n",
    "\n",
    "PROBE_LAYERS = [15]     # Examples: [2, 3, 4, 5], [8, 16, 24, 32], [5, 6, 7, 8] \n",
    "STEERING_LAYERS = [15]  # Same as probe layers for now - Examples: [3, 4, 5], [16, 24, 32], [6, 8, 10]\n",
    "\n",
    "\n",
    "# DAC Hyperparameters - specify arrays of values to search\n",
    "ENTROPY_THRESHOLDS = [1.0]    # Examples: [0.5, 1.0, 1.5], [1.0, 2.0], [0.8, 1.2]\n",
    "PTOP_VALUES = [0.5]            # Examples: [0.3, 0.4, 0.5], [0.4], [0.2, 0.6]  \n",
    "MAX_ALPHA_VALUES = [2.0]       # Examples: [1.5, 2.0, 2.5], [2.0], [1.0, 3.0]\n",
    "\n",
    "# Options\n",
    "ENABLE_WANDB = False                        # Disable for quick testing\n",
    "EXPERIMENT_NAME = 'dac_hyperparameter_search'  # Experiment name for logging\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET SIZE MAPPING (for validation - don't edit unless adding new datasets)\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-VALIDATION AND INFO\n",
    "# ============================================================================\n",
    "\n",
    "def detect_model_layers(model_name):\n",
    "    \"\"\"Detect number of layers in a model without loading it fully.\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoConfig\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        # Different models store layer count differently\n",
    "        if hasattr(config, 'n_layer'):\n",
    "            return config.n_layer\n",
    "        elif hasattr(config, 'num_hidden_layers'):\n",
    "            return config.num_hidden_layers\n",
    "        elif hasattr(config, 'num_layers'):\n",
    "            return config.num_layers\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Validate configuration\n",
    "print(\"📋 CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"🤖 Model: {MODEL_NAME}\")\n",
    "print(f\"📊 Datasets: {TRAIN_DATASET} → {VAL_DATASET} → {TEST_DATASET}\")\n",
    "print(f\"🔢 Samples: {TRAIN_LIMIT} + {VAL_LIMIT} + {TEST_LIMIT} = {TRAIN_LIMIT + VAL_LIMIT + TEST_LIMIT} total\")\n",
    "print(f\"🎯 Probe layers: {PROBE_LAYERS}\")\n",
    "print(f\"⚙️ Steering layers: {STEERING_LAYERS}\")\n",
    "print(f\"🎛️ Steering method: DAC (Dynamic Activation Composition)\")\n",
    "print(f\"📊 DAC Hyperparameters:\")\n",
    "print(f\"   • Entropy thresholds: {ENTROPY_THRESHOLDS}\")  \n",
    "print(f\"   • Ptop values: {PTOP_VALUES}\")\n",
    "print(f\"   • Max alpha values: {MAX_ALPHA_VALUES}\")\n",
    "print(f\"📚 Using tasks from MATH_TASKS: ✓\")\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = (len(STEERING_LAYERS) * \n",
    "                     len(ENTROPY_THRESHOLDS) *\n",
    "                     len(PTOP_VALUES) *\n",
    "                     len(MAX_ALPHA_VALUES) *\n",
    "                     len(PROBE_LAYERS) * \n",
    "                     3)  # Assuming 3 probe C values\n",
    "\n",
    "print(f\"🧪 Total hyperparameter combinations: {total_combinations}\")\n",
    "print(f\"📈 Wandb enabled: {ENABLE_WANDB}\")\n",
    "\n",
    "# Model info\n",
    "try:\n",
    "    num_layers = detect_model_layers(MODEL_NAME)\n",
    "    print(f\"🏗️ Model layers: {num_layers}\")\n",
    "    if isinstance(num_layers, int):\n",
    "        max_probe_layer = max(PROBE_LAYERS) if PROBE_LAYERS else 0\n",
    "        max_steering_layer = max(STEERING_LAYERS) if STEERING_LAYERS else 0\n",
    "        if max_probe_layer >= num_layers or max_steering_layer >= num_layers:\n",
    "            print(\"⚠️ WARNING: Some configured layers exceed model size!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not detect model layers: {e}\")\n",
    "\n",
    "print(\"\\n✅ Configuration validated successfully!\")\n",
    "print(\"💡 This evaluation now uses REAL mathematical training data instead of synthetic generation!\")\n",
    "print(\"🎯 DAC will be trained on actual math questions from your training dataset.\")\n",
    "print(\"💡 Configuration updated for quick testing with distilgpt2 and GSM8K dataset.\")\n",
    "print(f\"🎯 Currently configured for: {TRAIN_DATASET}\")\n",
    "print(f\"📚 Available math tasks: {len(MATH_TASKS_LIST)} tasks including GSM8K, MATH-500, AIME, etc.\")\n",
    "print(\"💡 To change datasets, edit TRAIN_DATASET, VAL_DATASET, TEST_DATASET above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Create Configuration\n",
    "\n",
    "Configuration is automatically created from the constants defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create configuration from constants\n\nconfig = ComprehensiveEvaluationConfig(\n    model_name=MODEL_NAME,\n    train_dataset=TRAIN_DATASET,\n    val_dataset=VAL_DATASET,\n    test_dataset=TEST_DATASET,\n    train_limit=TRAIN_LIMIT,\n    val_limit=VAL_LIMIT,\n    test_limit=TEST_LIMIT,\n    probe_layers=PROBE_LAYERS,\n    steering_layers=STEERING_LAYERS,\n    steering_methods=[\"dac\"],  # Fixed to DAC only\n    # DAC hyperparameters\n    dac_entropy_thresholds=ENTROPY_THRESHOLDS,\n    dac_ptop_values=PTOP_VALUES,\n    dac_max_alpha_values=MAX_ALPHA_VALUES,\n    enable_wandb=ENABLE_WANDB,\n    experiment_name=EXPERIMENT_NAME,\n    batch_size=16,\n    max_length=512,\n    max_new_tokens=256  # Increased from default 50 for GSM8K chain-of-thought\n)\n\nprint(\"✅ Configuration object created successfully!\")\nprint(\"🚀 Ready to run comprehensive evaluation!\")\nprint(\"\\n💡 All configuration is now controlled by the constants in the previous cell.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Run Comprehensive Evaluation\n",
    "\n",
    "This is the main evaluation cell. It will:\n",
    "\n",
    "1. **🎯 Train Probes**: Train correctness classifiers on all specified layers\n",
    "2. **⚙️ Optimize Hyperparameters**: Grid search for best steering + probe combinations\n",
    "3. **🏆 Final Evaluation**: Test optimized configuration on held-out test set\n",
    "\n",
    "**Note**: This may take several minutes depending on your configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 STARTING COMPREHENSIVE EVALUATION WITH REAL MATHEMATICAL DATA\n",
      "================================================================================\n",
      "✅ DAC will be trained on actual GSM8K mathematical questions from training data\n",
      "🎯 Using task extractors for proper format handling across datasets\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef991d48a73648afa5e503ef811e3ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🚀 STARTING COMPREHENSIVE EVALUATION WITH REAL MATHEMATICAL DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✅ DAC will be trained on actual {TRAIN_DATASET.upper()} mathematical questions from training data\")\n",
    "print(f\"🎯 Using task extractors for proper format handling across datasets\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ComprehensiveEvaluationPipeline(config)\n",
    "\n",
    "# Run evaluation with progress tracking\n",
    "try:\n",
    "    results = pipeline.run_comprehensive_evaluation()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"✅ Evaluation completed successfully!\")\n",
    "    \n",
    "    # Store results for analysis\n",
    "    evaluation_results = results\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Evaluation failed: {str(e)}\")\n",
    "    print(\"Check the logs above for more details.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Results Analysis\n",
    "\n",
    "Now let's analyze the results with comprehensive metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "comprehensive_metrics = calculate_comprehensive_metrics(evaluation_results)\n",
    "\n",
    "# Generate performance summary\n",
    "performance_summary = generate_performance_summary(comprehensive_metrics)\n",
    "print(performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Interactive Visualizations\n",
    "\n",
    "Explore your results with interactive plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "dashboard = create_results_dashboard(evaluation_results)\n",
    "dashboard.show()\n",
    "\n",
    "print(\"\\n📊 Interactive dashboard displayed above!\")\n",
    "print(\"💡 Hover over points and bars for detailed information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Detailed Analysis: Benchmark Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract benchmark results\n",
    "if \"test_results\" in evaluation_results:\n",
    "    test_results = evaluation_results[\"test_results\"]\n",
    "    \n",
    "    base_benchmark = test_results.get(\"base_model_benchmark_results\", {})\n",
    "    steered_benchmark = test_results.get(\"steered_model_benchmark_results\", {})\n",
    "    \n",
    "    print(\"🎯 BENCHMARK PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"\\n📊 Base Model:\")\n",
    "    print(f\"  ✓ Accuracy: {base_benchmark.get('accuracy', 0):.3f} ({base_benchmark.get('accuracy', 0)*100:.1f}%)\")\n",
    "    print(f\"  ✓ Correct: {base_benchmark.get('correct', 0)}/{base_benchmark.get('total_samples', 0)}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Steered Model:\")\n",
    "    print(f\"  ✓ Accuracy: {steered_benchmark.get('accuracy', 0):.3f} ({steered_benchmark.get('accuracy', 0)*100:.1f}%)\")\n",
    "    print(f\"  ✓ Correct: {steered_benchmark.get('correct', 0)}/{steered_benchmark.get('total_samples', 0)}\")\n",
    "    \n",
    "    improvement = steered_benchmark.get('accuracy', 0) - base_benchmark.get('accuracy', 0)\n",
    "    improvement_percent = (improvement / max(base_benchmark.get('accuracy', 0.001), 0.001)) * 100\n",
    "    \n",
    "    print(f\"\\n📈 Improvement:\")\n",
    "    print(f\"  {'✅' if improvement > 0 else '❌'} {improvement:+.3f} absolute ({improvement_percent:+.1f}% relative)\")\n",
    "    \n",
    "    if improvement > 0.05:\n",
    "        print(\"  🎉 Significant improvement! Steering is working well.\")\n",
    "    elif improvement > 0.01:\n",
    "        print(\"  👍 Moderate improvement. Consider tuning hyperparameters.\")\n",
    "    elif improvement > -0.01:\n",
    "        print(\"  ⚪ Minimal change. Steering may not be effective for this configuration.\")\n",
    "    else:\n",
    "        print(\"  ⚠️ Performance decreased. Check steering implementation.\")\n",
    "else:\n",
    "    print(\"❌ No test results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Detailed Analysis: Probe Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probe results\n",
    "if \"test_results\" in evaluation_results:\n",
    "    base_probe = test_results.get(\"base_model_probe_results\", {})\n",
    "    steered_probe = test_results.get(\"steered_model_probe_results\", {})\n",
    "    \n",
    "    print(\"🔍 PROBE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    print(f\"\\n📊 Base Model Probe:\")\n",
    "    print(f\"  ✓ AUC: {base_probe.get('auc', 0.5):.3f}\")\n",
    "    print(f\"  ✓ Accuracy: {base_probe.get('accuracy', 0.5):.3f}\")\n",
    "    print(f\"  ✓ Precision: {base_probe.get('precision', 0.5):.3f}\")\n",
    "    print(f\"  ✓ Recall: {base_probe.get('recall', 0.5):.3f}\")\n",
    "    print(f\"  ✓ F1-Score: {base_probe.get('f1', 0.5):.3f}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Steered Model Probe:\")\n",
    "    print(f\"  ✓ AUC: {steered_probe.get('auc', 0.5):.3f}\")\n",
    "    print(f\"  ✓ Accuracy: {steered_probe.get('accuracy', 0.5):.3f}\")\n",
    "    print(f\"  ✓ Precision: {steered_probe.get('precision', 0.5):.3f}\")\n",
    "    print(f\"  ✓ Recall: {steered_probe.get('recall', 0.5):.3f}\")\n",
    "    print(f\"  ✓ F1-Score: {steered_probe.get('f1', 0.5):.3f}\")\n",
    "    \n",
    "    auc_improvement = steered_probe.get('auc', 0.5) - base_probe.get('auc', 0.5)\n",
    "    \n",
    "    print(f\"\\n📈 AUC Improvement:\")\n",
    "    print(f\"  {'✅' if auc_improvement > 0 else '❌'} {auc_improvement:+.3f}\")\n",
    "    \n",
    "    # Interpret probe performance\n",
    "    best_auc = max(base_probe.get('auc', 0.5), steered_probe.get('auc', 0.5))\n",
    "    \n",
    "    if best_auc > 0.9:\n",
    "        print(\"  🎉 Excellent probe performance! Activations strongly predict correctness.\")\n",
    "    elif best_auc > 0.8:\n",
    "        print(\"  👍 Good probe performance. Activations are informative.\")\n",
    "    elif best_auc > 0.7:\n",
    "        print(\"  ⚪ Moderate probe performance. Some signal present.\")\n",
    "    elif best_auc > 0.6:\n",
    "        print(\"  ⚠️ Weak probe performance. Limited interpretability.\")\n",
    "    else:\n",
    "        print(\"  ❌ Poor probe performance. Activations may not encode correctness.\")\n",
    "else:\n",
    "    print(\"❌ No test results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Hyperparameter Optimization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hyperparameter optimization results\n",
    "if \"steering_optimization_results\" in evaluation_results:\n",
    "    opt_results = evaluation_results[\"steering_optimization_results\"]\n",
    "    all_configs = opt_results.get(\"all_configs\", [])\n",
    "    best_config = opt_results.get(\"best_config\", {})\n",
    "    \n",
    "    print(\"⚙️ HYPERPARAMETER OPTIMIZATION ANALYSIS\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    print(f\"\\n📊 Search Statistics:\")\n",
    "    print(f\"  ✓ Configurations tested: {len(all_configs)}\")\n",
    "    print(f\"  ✓ Best combined score: {opt_results.get('best_combined_score', 0):.3f}\")\n",
    "    \n",
    "    if best_config:\n",
    "        steering_config = best_config.get(\"steering_config\", {})\n",
    "        probe_config = best_config.get(\"best_probe_config\", {})\n",
    "        \n",
    "        print(f\"\\n🏆 Best Configuration:\")\n",
    "        print(f\"  ✓ Steering method: {steering_config.get('method', 'N/A')}\")\n",
    "        print(f\"  ✓ Steering layer: {steering_config.get('layer', 'N/A')}\")\n",
    "        print(f\"  ✓ Steering strength: {steering_config.get('strength', 'N/A')}\")\n",
    "        print(f\"  ✓ Probe layer: {probe_config.get('layer', 'N/A')}\")\n",
    "        print(f\"  ✓ Probe C value: {probe_config.get('C', 'N/A')}\")\n",
    "        \n",
    "        benchmark_metrics = best_config.get(\"benchmark_metrics\", {})\n",
    "        probe_metrics = best_config.get(\"probe_metrics\", {})\n",
    "        \n",
    "        print(f\"\\n📈 Best Configuration Performance:\")\n",
    "        print(f\"  ✓ Benchmark accuracy: {benchmark_metrics.get('accuracy', 0):.3f}\")\n",
    "        print(f\"  ✓ Probe AUC: {probe_metrics.get('auc', 0.5):.3f}\")\n",
    "        print(f\"  ✓ Combined score: {best_config.get('combined_score', 0):.3f}\")\n",
    "    \n",
    "    # Analyze score distribution\n",
    "    if all_configs:\n",
    "        scores = [config.get(\"combined_score\", 0) for config in all_configs]\n",
    "        benchmark_scores = [config.get(\"benchmark_metrics\", {}).get(\"accuracy\", 0) for config in all_configs]\n",
    "        probe_scores = [config.get(\"probe_metrics\", {}).get(\"auc\", 0.5) for config in all_configs]\n",
    "        \n",
    "        print(f\"\\n📊 Score Distribution:\")\n",
    "        print(f\"  ✓ Combined score: {np.mean(scores):.3f} ± {np.std(scores):.3f}\")\n",
    "        print(f\"  ✓ Benchmark score: {np.mean(benchmark_scores):.3f} ± {np.std(benchmark_scores):.3f}\")\n",
    "        print(f\"  ✓ Probe score: {np.mean(probe_scores):.3f} ± {np.std(probe_scores):.3f}\")\n",
    "        \n",
    "        # Check if optimization was effective\n",
    "        score_range = max(scores) - min(scores)\n",
    "        if score_range > 0.1:\n",
    "            print(\"  🎯 Good optimization! Significant variation in scores.\")\n",
    "        elif score_range > 0.05:\n",
    "            print(\"  👍 Moderate optimization. Some configurations better than others.\")\n",
    "        else:\n",
    "            print(\"  ⚪ Limited optimization benefit. Most configurations perform similarly.\")\n",
    "else:\n",
    "    print(\"❌ No optimization results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Training Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probe training performance by layer\n",
    "if \"probe_training_results\" in evaluation_results:\n",
    "    training_results = evaluation_results[\"probe_training_results\"]\n",
    "    \n",
    "    print(\"📊 PROBE TRAINING ANALYSIS\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    # Create summary table\n",
    "    training_data = []\n",
    "    \n",
    "    for layer_key, layer_results in training_results.items():\n",
    "        layer_num = int(layer_key.split('_')[1])\n",
    "        \n",
    "        best_auc = 0\n",
    "        best_config = None\n",
    "        \n",
    "        for c_key, metrics in layer_results.items():\n",
    "            if isinstance(metrics, dict) and \"auc\" in metrics:\n",
    "                if metrics[\"auc\"] > best_auc:\n",
    "                    best_auc = metrics[\"auc\"]\n",
    "                    best_config = c_key\n",
    "        \n",
    "        training_data.append({\n",
    "            'Layer': layer_num,\n",
    "            'Best AUC': best_auc,\n",
    "            'Best C': best_config.replace('C_', '') if best_config else 'N/A'\n",
    "        })\n",
    "    \n",
    "    # Display as formatted table\n",
    "    df_training = pd.DataFrame(training_data).sort_values('Layer')\n",
    "    \n",
    "    print(f\"\\n{'Layer':<8} {'Best AUC':<10} {'Best C':<10}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for _, row in df_training.iterrows():\n",
    "        print(f\"{row['Layer']:<8} {row['Best AUC']:<10.3f} {row['Best C']:<10}\")\n",
    "    \n",
    "    # Find best performing layer\n",
    "    best_layer_row = df_training.loc[df_training['Best AUC'].idxmax()]\n",
    "    worst_layer_row = df_training.loc[df_training['Best AUC'].idxmin()]\n",
    "    \n",
    "    print(f\"\\n🏆 Best performing layer: {best_layer_row['Layer']} (AUC: {best_layer_row['Best AUC']:.3f})\")\n",
    "    print(f\"⚠️ Worst performing layer: {worst_layer_row['Layer']} (AUC: {worst_layer_row['Best AUC']:.3f})\")\n",
    "    \n",
    "    # Layer performance insights\n",
    "    auc_std = df_training['Best AUC'].std()\n",
    "    if auc_std > 0.1:\n",
    "        print(\"\\n💡 High variation across layers - layer choice matters!\")\n",
    "    elif auc_std > 0.05:\n",
    "        print(\"\\n💡 Moderate variation - some layers work better than others.\")\n",
    "    else:\n",
    "        print(\"\\n💡 Consistent performance across layers.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No training results found in evaluation data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Static Visualizations\n",
    "\n",
    "Create comprehensive static plots for reports and publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive static visualization\n",
    "fig = plot_evaluation_results(evaluation_results)\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Comprehensive evaluation plots displayed above.\")\n",
    "print(\"💾 Plots are saved automatically in the results directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Export Results\n",
    "\n",
    "Save your results in various formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Export and Storage Options\n",
    "print(\"📊 RESULTS STORAGE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if wandb is enabled and results were logged\n",
    "if config.enable_wandb:\n",
    "    print(\"✅ Weights & Biases logging is ENABLED\")\n",
    "    print(\"📈 All evaluation results have been automatically logged to wandb including:\")\n",
    "    print(\"   • Configuration parameters\")\n",
    "    print(\"   • Probe training metrics\")\n",
    "    print(\"   • Hyperparameter optimization results\") \n",
    "    print(\"   • Final test performance\")\n",
    "    print(\"   • Comprehensive metrics and visualizations\")\n",
    "    print()\n",
    "    print(\"🔗 Access your results on the wandb dashboard:\")\n",
    "    print(\"   https://wandb.ai/\")\n",
    "else:\n",
    "    print(\"⚠️ Weights & Biases logging is DISABLED\")\n",
    "    print(\"💾 Creating local backup files...\")\n",
    "    \n",
    "    # Create results directory in outputs/ (excluded from git)\n",
    "    results_dir = Path(\"outputs/notebook_results\")\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # 1. Save raw results as JSON\n",
    "    json_file = results_dir / f\"evaluation_results_{timestamp}.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        # Remove non-serializable objects\n",
    "        import copy\n",
    "        results_copy = copy.deepcopy(evaluation_results)\n",
    "        \n",
    "        def remove_non_serializable(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {k: remove_non_serializable(v) for k, v in obj.items() if k != 'probe'}\n",
    "            elif isinstance(obj, list):\n",
    "                return [remove_non_serializable(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        clean_results = remove_non_serializable(results_copy)\n",
    "        json.dump(clean_results, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"✅ Raw results saved to: {json_file}\")\n",
    "    \n",
    "    # 2. Save comprehensive metrics as CSV\n",
    "    csv_file = results_dir / f\"comprehensive_metrics_{timestamp}.csv\"\n",
    "    metrics_df = pd.DataFrame([comprehensive_metrics])\n",
    "    metrics_df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"✅ Comprehensive metrics saved to: {csv_file}\")\n",
    "    \n",
    "    # 3. Generate HTML report\n",
    "    html_report = generate_summary_report(evaluation_results, config.to_dict())\n",
    "    html_file = results_dir / f\"evaluation_report_{timestamp}.html\"\n",
    "    with open(html_file, 'w') as f:\n",
    "        f.write(html_report)\n",
    "    \n",
    "    print(f\"✅ HTML report saved to: {html_file}\")\n",
    "    \n",
    "    # 4. Save configuration\n",
    "    config_file = results_dir / f\"configuration_{timestamp}.json\"\n",
    "    with open(config_file, 'w') as f:\n",
    "        json.dump(config.to_dict(), f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Configuration saved to: {config_file}\")\n",
    "    \n",
    "    print(f\"\\n📁 All results saved in: {results_dir.absolute()}\")\n",
    "\n",
    "print(\"\\n💡 Recommendation:\")\n",
    "if config.enable_wandb:\n",
    "    print(\"   Use wandb dashboard for comprehensive result analysis and comparison.\")\n",
    "    print(\"   Results are automatically versioned and shareable via wandb.\")\n",
    "else:\n",
    "    print(\"   Enable wandb logging for better experiment tracking and result management.\")\n",
    "    print(\"   Set enable_wandb=True in the configuration for future runs.\")\n",
    "\n",
    "print(\"\\n🎉 Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Optional: Detailed Data Exploration\n",
    "\n",
    "Use this section to explore specific aspects of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive data exploration\n",
    "@interact\n",
    "def explore_results(\n",
    "    section=widgets.Dropdown(\n",
    "        options=['Configuration', 'Training Results', 'Optimization Results', 'Test Results'],\n",
    "        value='Configuration'\n",
    "    )\n",
    "):\n",
    "    if section == 'Configuration':\n",
    "        display(Markdown(\"### 📋 Configuration Details\"))\n",
    "        # Fix UnboundLocalError by accessing config from global scope\n",
    "        if 'config' in globals():\n",
    "            config_df = pd.DataFrame(list(config.to_dict().items()), columns=['Parameter', 'Value'])\n",
    "            display(config_df)\n",
    "        else:\n",
    "            print(\"⚠️ Configuration not available. Please run the configuration cell first.\")\n",
    "        \n",
    "    elif section == 'Training Results':\n",
    "        display(Markdown(\"### 🎯 Probe Training Results\"))\n",
    "        if \"probe_training_results\" in evaluation_results:\n",
    "            training_data = []\n",
    "            for layer_key, layer_results in evaluation_results[\"probe_training_results\"].items():\n",
    "                layer_num = int(layer_key.split('_')[1])\n",
    "                for c_key, metrics in layer_results.items():\n",
    "                    if isinstance(metrics, dict) and \"auc\" in metrics:\n",
    "                        training_data.append({\n",
    "                            'Layer': layer_num,\n",
    "                            'C': float(c_key.replace('C_', '')),\n",
    "                            'Accuracy': metrics.get('accuracy', 0),\n",
    "                            'AUC': metrics.get('auc', 0.5),\n",
    "                            'Precision': metrics.get('precision', 0),\n",
    "                            'Recall': metrics.get('recall', 0),\n",
    "                            'F1': metrics.get('f1', 0)\n",
    "                        })\n",
    "            if training_data:\n",
    "                training_df = pd.DataFrame(training_data)\n",
    "                display(training_df.round(3))\n",
    "        else:\n",
    "            print(\"No training results available.\")\n",
    "            \n",
    "    elif section == 'Optimization Results':\n",
    "        display(Markdown(\"### ⚙️ Hyperparameter Optimization Results\"))\n",
    "        if \"steering_optimization_results\" in evaluation_results:\n",
    "            opt_results = evaluation_results[\"steering_optimization_results\"]\n",
    "            all_configs = opt_results.get(\"all_configs\", [])\n",
    "            \n",
    "            if all_configs:\n",
    "                opt_data = []\n",
    "                for i, config_item in enumerate(all_configs):\n",
    "                    steering_config = config_item.get(\"steering_config\", {})\n",
    "                    probe_config = config_item.get(\"best_probe_config\", {})\n",
    "                    \n",
    "                    opt_data.append({\n",
    "                        'Config': i + 1,\n",
    "                        'Steering Method': steering_config.get('method', 'N/A'),\n",
    "                        'Steering Layer': steering_config.get('layer', 'N/A'),\n",
    "                        'Steering Strength': steering_config.get('strength', 'N/A'),\n",
    "                        'Probe Layer': probe_config.get('layer', 'N/A'),\n",
    "                        'Probe C': probe_config.get('C', 'N/A'),\n",
    "                        'Benchmark Accuracy': config_item.get('benchmark_metrics', {}).get('accuracy', 0),\n",
    "                        'Probe AUC': config_item.get('probe_metrics', {}).get('auc', 0.5),\n",
    "                        'Combined Score': config_item.get('combined_score', 0)\n",
    "                    })\n",
    "                \n",
    "                opt_df = pd.DataFrame(opt_data)\n",
    "                display(opt_df.round(3))\n",
    "        else:\n",
    "            print(\"No optimization results available.\")\n",
    "            \n",
    "    elif section == 'Test Results':\n",
    "        display(Markdown(\"### 🏆 Final Test Results\"))\n",
    "        if \"test_results\" in evaluation_results:\n",
    "            test_results = evaluation_results[\"test_results\"]\n",
    "            \n",
    "            # Create summary table\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Base Benchmark Accuracy',\n",
    "                    'Steered Benchmark Accuracy', \n",
    "                    'Base Probe AUC',\n",
    "                    'Steered Probe AUC',\n",
    "                    'Validation Combined Score'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    test_results.get('base_model_benchmark_results', {}).get('accuracy', 0),\n",
    "                    test_results.get('steered_model_benchmark_results', {}).get('accuracy', 0),\n",
    "                    test_results.get('base_model_probe_results', {}).get('auc', 0.5),\n",
    "                    test_results.get('steered_model_probe_results', {}).get('auc', 0.5),\n",
    "                    test_results.get('validation_combined_score', 0)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            display(summary_df.round(3))\n",
    "        else:\n",
    "            print(\"No test results available.\")\n",
    "\n",
    "print(\"🔍 Use the dropdown above to explore different sections of your results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Conclusion\n",
    "\n",
    "Congratulations! You've successfully run a comprehensive evaluation that separates:\n",
    "\n",
    "1. **🎯 Benchmark Performance**: How well your model solves problems\n",
    "2. **🔍 Probe Performance**: How well we can detect when the model is wrong\n",
    "3. **⚙️ Optimization**: Finding the best configurations through proper validation\n",
    "\n",
    "### Next Steps:\n",
    "- 📊 Analyze the results above to understand your model's behavior\n",
    "- 🔧 Try different configurations to see how they affect performance\n",
    "- 📈 Use the exported results for further analysis or reporting\n",
    "- 🚀 Scale up to larger models and datasets when ready\n",
    "\n",
    "### Key Insights:\n",
    "- The framework properly separates model capability from interpretability\n",
    "- Hyperparameter optimization validates on actual performance, not just probe metrics\n",
    "- Results are saved and visualized for easy interpretation\n",
    "\n",
    "Happy experimenting! 🧪✨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}