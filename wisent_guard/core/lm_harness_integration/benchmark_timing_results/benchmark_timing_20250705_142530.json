{
  "timestamp": "2025-07-05 14:25:30",
  "summary": {
    "total_benchmarks": 71,
    "successful_loads": 59,
    "failed_loads": 12,
    "success_rate": 0.8309859154929577,
    "timing_stats": {
      "mean_time": 49.9847677198507,
      "median_time": 13.493192911148071,
      "min_time": 9.450568914413452,
      "max_time": 752.98805809021,
      "std_dev": 104.77584936304208,
      "total_time": 2949.1012954711914
    },
    "category_stats": {
      "reasoning": {
        "count": 54,
        "mean_time": 49.36284396825013,
        "median_time": 13.02497923374176,
        "min_time": 9.450568914413452,
        "max_time": 752.98805809021
      },
      "general knowledge": {
        "count": 43,
        "mean_time": 53.66055881145389,
        "median_time": 12.962058305740356,
        "min_time": 9.450568914413452,
        "max_time": 752.98805809021
      },
      "science": {
        "count": 24,
        "mean_time": 58.77404662966728,
        "median_time": 12.456324100494385,
        "min_time": 9.450568914413452,
        "max_time": 752.98805809021
      },
      "long context": {
        "count": 25,
        "mean_time": 38.44468134880066,
        "median_time": 16.573480129241943,
        "min_time": 9.484601974487305,
        "max_time": 209.47818613052368
      },
      "hallucination": {
        "count": 3,
        "mean_time": 11.111344734827677,
        "median_time": 10.641019105911255,
        "min_time": 10.366236925125122,
        "max_time": 12.326778173446655
      },
      "adversarial robustness": {
        "count": 6,
        "mean_time": 49.46517141660055,
        "median_time": 21.587363600730896,
        "min_time": 9.484601974487305,
        "max_time": 157.92335319519043
      },
      "mathematics": {
        "count": 8,
        "mean_time": 34.13540804386139,
        "median_time": 14.830690145492554,
        "min_time": 9.484601974487305,
        "max_time": 76.11083912849426
      },
      "medical": {
        "count": 3,
        "mean_time": 32.500473976135254,
        "median_time": 30.81258201599121,
        "min_time": 18.896270036697388,
        "max_time": 47.79256987571716
      },
      "multilingual": {
        "count": 7,
        "mean_time": 99.79952876908439,
        "median_time": 76.11083912849426,
        "min_time": 30.81258201599121,
        "max_time": 210.62598586082458
      },
      "coding": {
        "count": 2,
        "mean_time": 12.814417600631714,
        "median_time": 12.814417600631714,
        "min_time": 12.540935039520264,
        "max_time": 13.087900161743164
      },
      "creative writing": {
        "count": 1,
        "mean_time": 66.39091396331787,
        "median_time": 66.39091396331787,
        "min_time": 66.39091396331787,
        "max_time": 66.39091396331787
      },
      "history": {
        "count": 1,
        "mean_time": 11.285930871963501,
        "median_time": 11.285930871963501,
        "min_time": 11.285930871963501,
        "max_time": 11.285930871963501
      }
    },
    "slowest_benchmarks": [
      {
        "name": "belebele",
        "time": 157.92335319519043,
        "samples": 5,
        "tags": [
          "multilingual",
          "adversarial robustness",
          "long context"
        ]
      },
      {
        "name": "superglue",
        "time": 169.15970706939697,
        "samples": 4,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "blimp",
        "time": 209.47818613052368,
        "samples": 5,
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "xnli",
        "time": 210.62598586082458,
        "samples": 4,
        "tags": [
          "multilingual",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "big_bench",
        "time": 752.98805809021,
        "samples": 2,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      }
    ],
    "fastest_benchmarks": [
      {
        "name": "mmlu",
        "time": 9.450568914413452,
        "samples": 5,
        "tags": [
          "general knowledge",
          "science",
          "reasoning"
        ]
      },
      {
        "name": "asdiv",
        "time": 9.484601974487305,
        "samples": 5,
        "tags": [
          "mathematics",
          "adversarial robustness",
          "long context"
        ]
      },
      {
        "name": "logiqa",
        "time": 9.700928926467896,
        "samples": 5,
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "wsc273",
        "time": 9.830662965774536,
        "samples": 5,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "race",
        "time": 9.885234117507935,
        "samples": 5,
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      }
    ],
    "failed_benchmarks": [
      {
        "name": "storycloze",
        "task": "storycloze_2016",
        "error": "Task 'storycloze_2016' not found: individual (The repository for story_cloze contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https:...",
        "tags": [
          "long context",
          "creative writing",
          "reasoning"
        ]
      },
      {
        "name": "math_qa",
        "task": "mathqa",
        "error": "Task 'mathqa' not found: individual (The repository for math_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datas...",
        "tags": [
          "mathematics",
          "reasoning",
          "science"
        ]
      },
      {
        "name": "crows_pairs",
        "task": "crows_pairs",
        "error": "No samples could be retrieved from any subtasks of crows_pairs",
        "tags": [
          "bias",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "hendrycks_ethics",
        "task": "hendrycks_ethics",
        "error": "Task 'hendrycks_ethics' not found: individual (The repository for EleutherAI/hendrycks_ethics contains custom code which must be executed to correctly load the dataset. You can inspect the repository ...",
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "paws_x",
        "task": "pawsx",
        "error": "Task 'pawsx' not found: individual ('pawsx'), group (No group expansion found), large group (Task 'pawsx' not found at any level), README (No README groups found for 'pawsx')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "mmmlu",
        "task": "mmmlu",
        "error": "Task 'mmmlu' not found: individual ('mmmlu'), group (No group expansion found), large group (Task 'mmmlu' not found at any level), README (No README groups found for 'mmmlu')",
        "tags": [
          "general knowledge",
          "science",
          "reasoning"
        ]
      },
      {
        "name": "pubmedqa",
        "task": "pubmedqa",
        "error": "Task 'pubmedqa' not found: individual (The repository for bigbio/pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://...",
        "tags": [
          "medical",
          "science",
          "reasoning"
        ]
      },
      {
        "name": "narrativeqa",
        "task": "scrolls_narrativeqa",
        "error": "Task 'scrolls_narrativeqa' not found: individual (The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at ht...",
        "tags": [
          "reasoning",
          "long context",
          "general knowledge"
        ]
      },
      {
        "name": "scrolls",
        "task": "scrolls",
        "error": "No samples could be retrieved from any subtasks of scrolls",
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "mctaco",
        "task": "mc_taco",
        "error": "Task 'mc_taco' not found: individual (The repository for mc_taco contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/data...",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "wmt",
        "task": "wmt2016",
        "error": "Task 'wmt2016' not found: individual ('wmt2016'), group (No group expansion found), large group (Task 'wmt2016' not found at any level), README (No README groups found for 'wmt2016')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "babi",
        "task": "babi",
        "error": "No documents found for this task",
        "tags": [
          "reasoning",
          "general knowledge",
          "long context"
        ]
      }
    ],
    "total_script_time": 3737.905881166458
  },
  "detailed_results": [
    {
      "benchmark_name": "glue",
      "task_name": "glue",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 129.79149317741394,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'glue' containing subtasks: cola, mrpc, qqp, wnli, sst2"
      }
    },
    {
      "benchmark_name": "superglue",
      "task_name": "superglue",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 169.15970706939697,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {}
    },
    {
      "benchmark_name": "cb",
      "task_name": "cb",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.965142965316772,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 56,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "copa",
      "task_name": "copa",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.362804889678955,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 100,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "multirc",
      "task_name": "multirc",
      "tags": [
        "reasoning",
        "long context",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.580556154251099,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 4848,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "record",
      "task_name": "record",
      "tags": [
        "reasoning",
        "long context",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 20.166049003601074,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 10000,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "wic",
      "task_name": "wic",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.713310956954956,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 638,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "wsc",
      "task_name": "wsc",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.115768194198608,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 104,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "truthfulqa_mc1",
      "task_name": "truthfulqa_mc1",
      "tags": [
        "hallucination",
        "general knowledge",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.326778173446655,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 817,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "truthfulqa_mc2",
      "task_name": "truthfulqa_mc2",
      "tags": [
        "hallucination",
        "general knowledge",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.641019105911255,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 817,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "truthfulqa_gen",
      "task_name": "truthfulqa_gen",
      "tags": [
        "hallucination",
        "general knowledge",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.366236925125122,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 817,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "hellaswag",
      "task_name": "hellaswag",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.22358512878418,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 10042,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "piqa",
      "task_name": "piqa",
      "tags": [
        "reasoning",
        "science",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.058856010437012,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1838,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "winogrande",
      "task_name": "winogrande",
      "tags": [
        "reasoning",
        "general knowledge",
        "adversarial robustness"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.985606908798218,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1267,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "openbookqa",
      "task_name": "openbookqa",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 13.493192911148071,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 500,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "swag",
      "task_name": "swag",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 16.218629121780396,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 20006,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "storycloze",
      "task_name": "storycloze_2016",
      "tags": [
        "long context",
        "creative writing",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 42.45222020149231,
      "samples_retrieved": 0,
      "error": "Task 'storycloze_2016' not found: individual (The repository for story_cloze contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/story_cloze.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'storycloze_2016' not found at any level), README (No README groups found for 'storycloze_2016')",
      "metadata": {}
    },
    {
      "benchmark_name": "logiqa",
      "task_name": "logiqa",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 9.700928926467896,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 651,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "wsc273",
      "task_name": "wsc273",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 9.830662965774536,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 273,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "coqa",
      "task_name": "coqa",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.262293100357056,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 500,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "drop",
      "task_name": "drop",
      "tags": [
        "mathematics",
        "reasoning",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 16.573480129241943,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 9536,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "boolq",
      "task_name": "boolq",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.561009883880615,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 3270,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "race",
      "task_name": "race",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 9.885234117507935,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1045,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "squad2",
      "task_name": "squadv2",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 16.435141801834106,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 11873,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "triviaqa",
      "task_name": "triviaqa",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 25.584350109100342,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 17944,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "naturalqs",
      "task_name": "nq_open",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 13.601925134658813,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 3610,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "webqs",
      "task_name": "webqs",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.962058305740356,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 2032,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "headqa",
      "task_name": "headqa",
      "tags": [
        "medical",
        "multilingual",
        "adversarial robustness"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 30.81258201599121,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'headqa' containing subtasks: headqa_es, headqa_en"
      }
    },
    {
      "benchmark_name": "qasper",
      "task_name": "qasper",
      "tags": [
        "science",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 29.357444047927856,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'qasper' containing subtasks: qasper_bool, qasper_freeform"
      }
    },
    {
      "benchmark_name": "qa4mre",
      "task_name": "qa4mre",
      "tags": [
        "medical",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 47.79256987571716,
      "samples_retrieved": 3,
      "error": null,
      "metadata": {
        "description": "Group task 'qa4mre' containing subtasks: qa4mre_2012, qa4mre_2013, qa4mre_2011"
      }
    },
    {
      "benchmark_name": "mutual",
      "task_name": "mutual",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 9.900049209594727,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 886,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "mmlu",
      "task_name": "mmlu_abstract_algebra",
      "tags": [
        "general knowledge",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 9.450568914413452,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 100,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "ai2_arc",
      "task_name": "ai2_arc",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 33.03347706794739,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'ai2_arc' containing subtasks: arc_challenge, arc_easy"
      }
    },
    {
      "benchmark_name": "arc_easy",
      "task_name": "arc_easy",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.369141101837158,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 570,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "arc_challenge",
      "task_name": "arc_challenge",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.841612100601196,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 299,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "sciq",
      "task_name": "sciq",
      "tags": [
        "long context",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.68906307220459,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1000,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "social_i_qa",
      "task_name": "siqa",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 19.334192276000977,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'siqa' containing subtasks: siqa_ca"
      }
    },
    {
      "benchmark_name": "gsm8k",
      "task_name": "gsm8k",
      "tags": [
        "mathematics",
        "reasoning",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.060017824172974,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1319,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "math_qa",
      "task_name": "mathqa",
      "tags": [
        "mathematics",
        "reasoning",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 66.66554594039917,
      "samples_retrieved": 0,
      "error": "Task 'mathqa' not found: individual (The repository for math_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/math_qa.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'mathqa' not found at any level), README (No README groups found for 'mathqa')",
      "metadata": {}
    },
    {
      "benchmark_name": "hendrycks_math",
      "task_name": "hendrycks_math",
      "tags": [
        "mathematics",
        "reasoning",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 69.461186170578,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'hendrycks_math' containing subtasks: hendrycks_math_intermediate_algebra, hendrycks_math_algebra, hendrycks_math_geometry, hendrycks_math_counting_and_prob, hendrycks_math_num_theory"
      }
    },
    {
      "benchmark_name": "arithmetic",
      "task_name": "arithmetic",
      "tags": [
        "mathematics",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 63.7643039226532,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'arithmetic' containing subtasks: arithmetic_4ds, arithmetic_2da, arithmetic_2dm, arithmetic_2ds, arithmetic_1dc"
      }
    },
    {
      "benchmark_name": "asdiv",
      "task_name": "asdiv",
      "tags": [
        "mathematics",
        "adversarial robustness",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 9.484601974487305,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 2305,
        "output_type": "loglikelihood",
        "description": null
      }
    },
    {
      "benchmark_name": "humaneval",
      "task_name": "humaneval",
      "tags": [
        "coding",
        "reasoning",
        "mathematics"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.540935039520264,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 164,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "mbpp",
      "task_name": "mbpp",
      "tags": [
        "coding",
        "reasoning",
        "mathematics"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 13.087900161743164,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 500,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "toxigen",
      "task_name": "toxigen",
      "tags": [
        "adversarial robustness",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.362145185470581,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 940,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "crows_pairs",
      "task_name": "crows_pairs",
      "tags": [
        "bias",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 146.5217251777649,
      "samples_retrieved": 0,
      "error": "No samples could be retrieved from any subtasks of crows_pairs",
      "metadata": {}
    },
    {
      "benchmark_name": "hendrycks_ethics",
      "task_name": "hendrycks_ethics",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 55.740638971328735,
      "samples_retrieved": 0,
      "error": "Task 'hendrycks_ethics' not found: individual (The repository for EleutherAI/hendrycks_ethics contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/EleutherAI/hendrycks_ethics.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'hendrycks_ethics' not found at any level), README (No README groups found for 'hendrycks_ethics')",
      "metadata": {}
    },
    {
      "benchmark_name": "anli",
      "task_name": "anli",
      "tags": [
        "adversarial robustness",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 75.22273921966553,
      "samples_retrieved": 3,
      "error": null,
      "metadata": {
        "description": "Group task 'anli' containing subtasks: anli_r1, anli_r2, anli_r3"
      }
    },
    {
      "benchmark_name": "xnli",
      "task_name": "xnli",
      "tags": [
        "multilingual",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 210.62598586082458,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'xnli' containing subtasks: xnli_de, xnli_ca, xnli_ar, xnli_bg"
      }
    },
    {
      "benchmark_name": "xcopa",
      "task_name": "xcopa",
      "tags": [
        "multilingual",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 91.5767970085144,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'xcopa' containing subtasks: xcopa_id, xcopa_eu, xcopa_et, xcopa_ht, xcopa_it"
      }
    },
    {
      "benchmark_name": "xstorycloze",
      "task_name": "xstorycloze",
      "tags": [
        "multilingual",
        "long context",
        "creative writing"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 66.39091396331787,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'xstorycloze' containing subtasks: xstorycloze_es, xstorycloze_eu, xstorycloze_en, xstorycloze_ca, xstorycloze_ar"
      }
    },
    {
      "benchmark_name": "xwinograd",
      "task_name": "xwinograd",
      "tags": [
        "multilingual",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 65.15623021125793,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'xwinograd' containing subtasks: xwinograd_fr, xwinograd_ru, xwinograd_en, xwinograd_pt, xwinograd_jp"
      }
    },
    {
      "benchmark_name": "paws_x",
      "task_name": "pawsx",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 97.73230409622192,
      "samples_retrieved": 0,
      "error": "Task 'pawsx' not found: individual ('pawsx'), group (No group expansion found), large group (Task 'pawsx' not found at any level), README (No README groups found for 'pawsx')",
      "metadata": {}
    },
    {
      "benchmark_name": "mmmlu",
      "task_name": "mmmlu",
      "tags": [
        "general knowledge",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 25.70197296142578,
      "samples_retrieved": 0,
      "error": "Task 'mmmlu' not found: individual ('mmmlu'), group (No group expansion found), large group (Task 'mmmlu' not found at any level), README (No README groups found for 'mmmlu')",
      "metadata": {}
    },
    {
      "benchmark_name": "mgsm",
      "task_name": "mgsm",
      "tags": [
        "multilingual",
        "mathematics",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 76.11083912849426,
      "samples_retrieved": 3,
      "error": null,
      "metadata": {
        "description": "Group task 'mgsm' containing subtasks: mgsm_direct_de, mgsm_direct_bn, mgsm_direct_ca"
      }
    },
    {
      "benchmark_name": "belebele",
      "task_name": "belebele",
      "tags": [
        "multilingual",
        "adversarial robustness",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 157.92335319519043,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'belebele' containing subtasks: belebele_afr_Latn, belebele_acm_Arab, belebele_amh_Ethi, belebele_apc_Arab, belebele_als_Latn"
      }
    },
    {
      "benchmark_name": "medqa",
      "task_name": "medqa",
      "tags": [
        "medical",
        "science",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 18.896270036697388,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'medqa' containing subtasks: medqa_4options"
      }
    },
    {
      "benchmark_name": "pubmedqa",
      "task_name": "pubmedqa",
      "tags": [
        "medical",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 59.93681979179382,
      "samples_retrieved": 0,
      "error": "Task 'pubmedqa' not found: individual (The repository for bigbio/pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/pubmed_qa.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'pubmedqa' not found at any level), README (No README groups found for 'pubmedqa')",
      "metadata": {}
    },
    {
      "benchmark_name": "lambada",
      "task_name": "lambada",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 34.37561011314392,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'lambada' containing subtasks: lambada_openai, lambada_standard"
      }
    },
    {
      "benchmark_name": "lambada_cloze",
      "task_name": "lambada_cloze",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 32.23414182662964,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'lambada_cloze' containing subtasks: lambada_openai_cloze_yaml, lambada_standard_cloze_yaml"
      }
    },
    {
      "benchmark_name": "lambada_multilingual",
      "task_name": "lambada_multilingual",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 59.20638990402222,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'lambada_multilingual' containing subtasks: lambada_openai_mt_es, lambada_openai_mt_it, lambada_openai_mt_fr, lambada_openai_mt_de, lambada_openai_mt_en"
      }
    },
    {
      "benchmark_name": "wikitext",
      "task_name": "wikitext",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.958117723464966,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 60,
        "output_type": "loglikelihood_rolling",
        "description": null
      }
    },
    {
      "benchmark_name": "narrativeqa",
      "task_name": "scrolls_narrativeqa",
      "tags": [
        "reasoning",
        "long context",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 41.610275983810425,
      "samples_retrieved": 0,
      "error": "Task 'scrolls_narrativeqa' not found: individual (The repository for tau/scrolls contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tau/scrolls.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'scrolls_narrativeqa' not found at any level), README (No README groups found for 'scrolls_narrativeqa')",
      "metadata": {}
    },
    {
      "benchmark_name": "scrolls",
      "task_name": "scrolls",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 129.79873490333557,
      "samples_retrieved": 0,
      "error": "No samples could be retrieved from any subtasks of scrolls",
      "metadata": {}
    },
    {
      "benchmark_name": "mctaco",
      "task_name": "mc_taco",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 60.90924310684204,
      "samples_retrieved": 0,
      "error": "Task 'mc_taco' not found: individual (The repository for mc_taco contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mc_taco.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'mc_taco' not found at any level), README (No README groups found for 'mc_taco')",
      "metadata": {}
    },
    {
      "benchmark_name": "prost",
      "task_name": "prost",
      "tags": [
        "long context",
        "history",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.285930871963501,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 18736,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "blimp",
      "task_name": "blimp",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 209.47818613052368,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'blimp' containing subtasks: blimp_anaphor_number_agreement, blimp_adjunct_island, blimp_animate_subject_trans, blimp_anaphor_gender_agreement, blimp_animate_subject_passive"
      }
    },
    {
      "benchmark_name": "unscramble",
      "task_name": "unscramble",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 59.76412105560303,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'unscramble' containing subtasks: reversed_words, anagrams2, anagrams1, random_insertion, cycle_letters"
      }
    },
    {
      "benchmark_name": "wmt",
      "task_name": "wmt2016",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 43.635308027267456,
      "samples_retrieved": 0,
      "error": "Task 'wmt2016' not found: individual ('wmt2016'), group (No group expansion found), large group (Task 'wmt2016' not found at any level), README (No README groups found for 'wmt2016')",
      "metadata": {}
    },
    {
      "benchmark_name": "big_bench",
      "task_name": "bigbench",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 752.98805809021,
      "samples_retrieved": 2,
      "error": null,
      "metadata": {
        "description": "Group task 'bigbench' containing subtasks: bigbench_abstract_narrative_understanding_generate_until, bigbench_abstract_narrative_understanding_multiple_choice"
      }
    },
    {
      "benchmark_name": "babi",
      "task_name": "babi",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 10.526440143585205,
      "samples_retrieved": 0,
      "error": "No documents found for this task",
      "metadata": {}
    }
  ]
}