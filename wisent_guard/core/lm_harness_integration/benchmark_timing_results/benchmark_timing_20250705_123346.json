{
  "timestamp": "2025-07-05 12:33:46",
  "summary": {
    "total_benchmarks": 71,
    "successful_loads": 54,
    "failed_loads": 17,
    "success_rate": 0.7605633802816901,
    "timing_stats": {
      "mean_time": 42.87005671306893,
      "median_time": 15.191923022270203,
      "min_time": 10.596436977386475,
      "max_time": 272.47432708740234,
      "std_dev": 53.936005274581035,
      "total_time": 2314.983062505722
    },
    "category_stats": {
      "reasoning": {
        "count": 49,
        "mean_time": 41.1100429953361,
        "median_time": 14.764731884002686,
        "min_time": 10.596436977386475,
        "max_time": 272.47432708740234
      },
      "general knowledge": {
        "count": 38,
        "mean_time": 42.70135236413855,
        "median_time": 14.187602996826172,
        "min_time": 10.596436977386475,
        "max_time": 272.47432708740234
      },
      "science": {
        "count": 20,
        "mean_time": 32.08587880134583,
        "median_time": 13.817641973495483,
        "min_time": 10.596436977386475,
        "max_time": 167.74465608596802
      },
      "long context": {
        "count": 24,
        "mean_time": 42.32453193267187,
        "median_time": 21.68524444103241,
        "min_time": 11.5046968460083,
        "max_time": 206.89719104766846
      },
      "hallucination": {
        "count": 3,
        "mean_time": 11.616595029830933,
        "median_time": 11.359737157821655,
        "min_time": 10.984169960021973,
        "max_time": 12.50587797164917
      },
      "adversarial robustness": {
        "count": 6,
        "mean_time": 50.9494483868281,
        "median_time": 23.72010600566864,
        "min_time": 10.759735107421875,
        "max_time": 161.31223702430725
      },
      "mathematics": {
        "count": 8,
        "mean_time": 37.574059426784515,
        "median_time": 19.706915974617004,
        "min_time": 11.63782286643982,
        "max_time": 81.93674182891846
      },
      "medical": {
        "count": 3,
        "mean_time": 35.589687983194985,
        "median_time": 33.7524139881134,
        "min_time": 21.59283685684204,
        "max_time": 51.42381310462952
      },
      "multilingual": {
        "count": 7,
        "mean_time": 112.7818272454398,
        "median_time": 81.93674182891846,
        "min_time": 33.7524139881134,
        "max_time": 272.47432708740234
      },
      "coding": {
        "count": 2,
        "mean_time": 14.20658791065216,
        "median_time": 14.20658791065216,
        "min_time": 13.648443937301636,
        "max_time": 14.764731884002686
      },
      "creative writing": {
        "count": 1,
        "mean_time": 72.29564499855042,
        "median_time": 72.29564499855042,
        "min_time": 72.29564499855042,
        "max_time": 72.29564499855042
      },
      "history": {
        "count": 1,
        "mean_time": 12.309722185134888,
        "median_time": 12.309722185134888,
        "min_time": 12.309722185134888,
        "max_time": 12.309722185134888
      }
    },
    "slowest_benchmarks": [
      {
        "name": "glue",
        "time": 121.28185200691223,
        "samples": 5,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "belebele",
        "time": 161.31223702430725,
        "samples": 5,
        "tags": [
          "multilingual",
          "adversarial robustness",
          "long context"
        ]
      },
      {
        "name": "superglue",
        "time": 167.74465608596802,
        "samples": 4,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "blimp",
        "time": 206.89719104766846,
        "samples": 5,
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "xnli",
        "time": 272.47432708740234,
        "samples": 4,
        "tags": [
          "multilingual",
          "reasoning",
          "general knowledge"
        ]
      }
    ],
    "fastest_benchmarks": [
      {
        "name": "arc_easy",
        "time": 10.596436977386475,
        "samples": 5,
        "tags": [
          "science",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "winogrande",
        "time": 10.759735107421875,
        "samples": 5,
        "tags": [
          "reasoning",
          "general knowledge",
          "adversarial robustness"
        ]
      },
      {
        "name": "truthfulqa_gen",
        "time": 10.984169960021973,
        "samples": 5,
        "tags": [
          "hallucination",
          "general knowledge",
          "reasoning"
        ]
      },
      {
        "name": "wsc",
        "time": 11.06455397605896,
        "samples": 5,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "copa",
        "time": 11.150030136108398,
        "samples": 5,
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      }
    ],
    "failed_benchmarks": [
      {
        "name": "storycloze",
        "task": "storycloze",
        "error": "No samples could be retrieved from any subtasks of storycloze",
        "tags": [
          "long context",
          "creative writing",
          "reasoning"
        ]
      },
      {
        "name": "wsc273",
        "task": "wsc273",
        "error": "Task 'wsc273' not found: individual (The repository for winograd_wsc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/...",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "squad2",
        "task": "squad2",
        "error": "Task 'squad2' not found: individual ('squad2'), group (No group expansion found), large group (Task 'squad2' not found at any level), README (No README groups found for 'squad2')",
        "tags": [
          "reasoning",
          "general knowledge",
          "long context"
        ]
      },
      {
        "name": "mmlu",
        "task": "mmlu",
        "error": "No samples could be retrieved from any subtasks of mmlu",
        "tags": [
          "general knowledge",
          "science",
          "reasoning"
        ]
      },
      {
        "name": "social_i_qa",
        "task": "social_i_qa",
        "error": "Task 'social_i_qa' not found: individual ('social_i_qa'), group (No group expansion found), large group (Task 'social_i_qa' not found at any level), README (No README groups found for 'social_i_qa')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "math_qa",
        "task": "math_qa",
        "error": "Task 'math_qa' not found: individual ('math_qa'), group (No group expansion found), large group (Task 'math_qa' not found at any level), README (No README groups found for 'math_qa')",
        "tags": [
          "mathematics",
          "reasoning",
          "science"
        ]
      },
      {
        "name": "crows_pairs",
        "task": "crows_pairs",
        "error": "No samples could be retrieved from any subtasks of crows_pairs",
        "tags": [
          "bias",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "hendrycks_ethics",
        "task": "hendrycks_ethics",
        "error": "Task 'hendrycks_ethics' not found: individual (The repository for EleutherAI/hendrycks_ethics contains custom code which must be executed to correctly load the dataset. You can inspect the repository ...",
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "paws_x",
        "task": "paws_x",
        "error": "Task 'paws_x' not found: individual ('paws_x'), group (No group expansion found), large group (Task 'paws_x' not found at any level), README (No README groups found for 'paws_x')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "mmmlu",
        "task": "mmmlu",
        "error": "Task 'mmmlu' not found: individual ('mmmlu'), group (No group expansion found), large group (Task 'mmmlu' not found at any level), README (No README groups found for 'mmmlu')",
        "tags": [
          "general knowledge",
          "science",
          "reasoning"
        ]
      },
      {
        "name": "pubmedqa",
        "task": "pubmedqa",
        "error": "Task 'pubmedqa' not found: individual (The repository for bigbio/pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://...",
        "tags": [
          "medical",
          "science",
          "reasoning"
        ]
      },
      {
        "name": "narrativeqa",
        "task": "narrativeqa",
        "error": "Task 'narrativeqa' not found: individual ('narrativeqa'), group (No group expansion found), large group (Task 'narrativeqa' not found at any level), README (No README groups found for 'narrativeqa')",
        "tags": [
          "reasoning",
          "long context",
          "general knowledge"
        ]
      },
      {
        "name": "scrolls",
        "task": "scrolls",
        "error": "No samples could be retrieved from any subtasks of scrolls",
        "tags": [
          "long context",
          "reasoning",
          "general knowledge"
        ]
      },
      {
        "name": "mctaco",
        "task": "mctaco",
        "error": "Task 'mctaco' not found: individual ('mctaco'), group (No group expansion found), large group (Task 'mctaco' not found at any level), README (No README groups found for 'mctaco')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "wmt",
        "task": "wmt",
        "error": "Task 'wmt' not found: individual ('wmt'), group (No group expansion found), large group (Task 'wmt' not found at any level), README (No README groups found for 'wmt')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "big_bench",
        "task": "big_bench",
        "error": "Task 'big_bench' not found: individual ('big_bench'), group (No group expansion found), large group (Task 'big_bench' not found at any level), README (No README groups found for 'big_bench')",
        "tags": [
          "reasoning",
          "general knowledge",
          "science"
        ]
      },
      {
        "name": "babi",
        "task": "babi",
        "error": "No documents found for this task",
        "tags": [
          "reasoning",
          "general knowledge",
          "long context"
        ]
      }
    ],
    "total_script_time": 3367.4538209438324
  },
  "detailed_results": [
    {
      "benchmark_name": "glue",
      "task_name": "glue",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 121.28185200691223,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'glue' containing subtasks: qqp, cola, wnli, sst2, mrpc"
      }
    },
    {
      "benchmark_name": "superglue",
      "task_name": "superglue",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 167.74465608596802,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {}
    },
    {
      "benchmark_name": "cb",
      "task_name": "cb",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.344300031661987,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 56,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "copa",
      "task_name": "copa",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.150030136108398,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 100,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "multirc",
      "task_name": "multirc",
      "tags": [
        "reasoning",
        "long context",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.592549085617065,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 4848,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "record",
      "task_name": "record",
      "tags": [
        "reasoning",
        "long context",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 18.721388816833496,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 10000,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "wic",
      "task_name": "wic",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.785259008407593,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 638,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "wsc",
      "task_name": "wsc",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.06455397605896,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 104,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "truthfulqa_mc1",
      "task_name": "truthfulqa_mc1",
      "tags": [
        "hallucination",
        "general knowledge",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.50587797164917,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 817,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "truthfulqa_mc2",
      "task_name": "truthfulqa_mc2",
      "tags": [
        "hallucination",
        "general knowledge",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.359737157821655,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 817,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "truthfulqa_gen",
      "task_name": "truthfulqa_gen",
      "tags": [
        "hallucination",
        "general knowledge",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.984169960021973,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 817,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "hellaswag",
      "task_name": "hellaswag",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.970609903335571,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 10042,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "piqa",
      "task_name": "piqa",
      "tags": [
        "reasoning",
        "science",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.652615308761597,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1838,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "winogrande",
      "task_name": "winogrande",
      "tags": [
        "reasoning",
        "general knowledge",
        "adversarial robustness"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.759735107421875,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1267,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "openbookqa",
      "task_name": "openbookqa",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 26.59700918197632,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 500,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "swag",
      "task_name": "swag",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 17.647130012512207,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 20006,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "storycloze",
      "task_name": "storycloze",
      "tags": [
        "long context",
        "creative writing",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 75.56758093833923,
      "samples_retrieved": 0,
      "error": "No samples could be retrieved from any subtasks of storycloze",
      "metadata": {}
    },
    {
      "benchmark_name": "logiqa",
      "task_name": "logiqa",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.216100215911865,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 651,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "wsc273",
      "task_name": "wsc273",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 54.45185875892639,
      "samples_retrieved": 0,
      "error": "Task 'wsc273' not found: individual (The repository for winograd_wsc contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winograd_wsc.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'wsc273' not found at any level), README (No README groups found for 'wsc273')",
      "metadata": {}
    },
    {
      "benchmark_name": "coqa",
      "task_name": "coqa",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.855568885803223,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 500,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "drop",
      "task_name": "drop",
      "tags": [
        "mathematics",
        "reasoning",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 24.649100065231323,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 9536,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "boolq",
      "task_name": "boolq",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.479889154434204,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 3270,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "race",
      "task_name": "race",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.5046968460083,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1045,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "squad2",
      "task_name": "squad2",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.432562112808228,
      "samples_retrieved": 0,
      "error": "Task 'squad2' not found: individual ('squad2'), group (No group expansion found), large group (Task 'squad2' not found at any level), README (No README groups found for 'squad2')",
      "metadata": {}
    },
    {
      "benchmark_name": "triviaqa",
      "task_name": "triviaqa",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 28.943449020385742,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 17944,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "naturalqs",
      "task_name": "nq_open",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 15.148761987686157,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 3610,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "webqs",
      "task_name": "webqs",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 13.226444005966187,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 2032,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "headqa",
      "task_name": "headqa",
      "tags": [
        "medical",
        "multilingual",
        "adversarial robustness"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 33.7524139881134,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'headqa' containing subtasks: headqa_es, headqa_en"
      }
    },
    {
      "benchmark_name": "qasper",
      "task_name": "qasper",
      "tags": [
        "science",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 32.29289388656616,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'qasper' containing subtasks: qasper_bool, qasper_freeform"
      }
    },
    {
      "benchmark_name": "qa4mre",
      "task_name": "qa4mre",
      "tags": [
        "medical",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 51.42381310462952,
      "samples_retrieved": 3,
      "error": null,
      "metadata": {
        "description": "Group task 'qa4mre' containing subtasks: qa4mre_2012, qa4mre_2011, qa4mre_2013"
      }
    },
    {
      "benchmark_name": "mutual",
      "task_name": "mutual",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 15.235084056854248,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 886,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "mmlu",
      "task_name": "mmlu",
      "tags": [
        "general knowledge",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 256.25232195854187,
      "samples_retrieved": 0,
      "error": "No samples could be retrieved from any subtasks of mmlu",
      "metadata": {}
    },
    {
      "benchmark_name": "ai2_arc",
      "task_name": "ai2_arc",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 34.177852392196655,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'ai2_arc' containing subtasks: arc_challenge, arc_easy"
      }
    },
    {
      "benchmark_name": "arc_easy",
      "task_name": "arc_easy",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 10.596436977386475,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 570,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "arc_challenge",
      "task_name": "arc_challenge",
      "tags": [
        "science",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.31036901473999,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 299,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "sciq",
      "task_name": "sciq",
      "tags": [
        "long context",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 14.664674043655396,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1000,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "social_i_qa",
      "task_name": "social_i_qa",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.704836130142212,
      "samples_retrieved": 0,
      "error": "Task 'social_i_qa' not found: individual ('social_i_qa'), group (No group expansion found), large group (Task 'social_i_qa' not found at any level), README (No README groups found for 'social_i_qa')",
      "metadata": {}
    },
    {
      "benchmark_name": "gsm8k",
      "task_name": "gsm8k",
      "tags": [
        "mathematics",
        "reasoning",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.992494106292725,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 1319,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "math_qa",
      "task_name": "math_qa",
      "tags": [
        "mathematics",
        "reasoning",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.813122987747192,
      "samples_retrieved": 0,
      "error": "Task 'math_qa' not found: individual ('math_qa'), group (No group expansion found), large group (Task 'math_qa' not found at any level), README (No README groups found for 'math_qa')",
      "metadata": {}
    },
    {
      "benchmark_name": "hendrycks_math",
      "task_name": "hendrycks_math",
      "tags": [
        "mathematics",
        "reasoning",
        "science"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 74.22335195541382,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'hendrycks_math' containing subtasks: hendrycks_math_counting_and_prob, hendrycks_math_geometry, hendrycks_math_num_theory, hendrycks_math_algebra, hendrycks_math_intermediate_algebra"
      }
    },
    {
      "benchmark_name": "arithmetic",
      "task_name": "arithmetic",
      "tags": [
        "mathematics",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 67.73978877067566,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'arithmetic' containing subtasks: arithmetic_2dm, arithmetic_4ds, arithmetic_2da, arithmetic_2ds, arithmetic_1dc"
      }
    },
    {
      "benchmark_name": "asdiv",
      "task_name": "asdiv",
      "tags": [
        "mathematics",
        "adversarial robustness",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.63782286643982,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 2305,
        "output_type": "loglikelihood",
        "description": null
      }
    },
    {
      "benchmark_name": "humaneval",
      "task_name": "humaneval",
      "tags": [
        "coding",
        "reasoning",
        "mathematics"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 13.648443937301636,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 164,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "mbpp",
      "task_name": "mbpp",
      "tags": [
        "coding",
        "reasoning",
        "mathematics"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 14.764731884002686,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 500,
        "output_type": "generate_until",
        "description": null
      }
    },
    {
      "benchmark_name": "toxigen",
      "task_name": "toxigen",
      "tags": [
        "adversarial robustness",
        "long context",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 13.687798023223877,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 940,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "crows_pairs",
      "task_name": "crows_pairs",
      "tags": [
        "bias",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 148.14425897598267,
      "samples_retrieved": 0,
      "error": "No samples could be retrieved from any subtasks of crows_pairs",
      "metadata": {}
    },
    {
      "benchmark_name": "hendrycks_ethics",
      "task_name": "hendrycks_ethics",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 61.80103611946106,
      "samples_retrieved": 0,
      "error": "Task 'hendrycks_ethics' not found: individual (The repository for EleutherAI/hendrycks_ethics contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/EleutherAI/hendrycks_ethics.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'hendrycks_ethics' not found at any level), README (No README groups found for 'hendrycks_ethics')",
      "metadata": {}
    },
    {
      "benchmark_name": "anli",
      "task_name": "anli",
      "tags": [
        "adversarial robustness",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 74.5466833114624,
      "samples_retrieved": 3,
      "error": null,
      "metadata": {
        "description": "Group task 'anli' containing subtasks: anli_r1, anli_r2, anli_r3"
      }
    },
    {
      "benchmark_name": "xnli",
      "task_name": "xnli",
      "tags": [
        "multilingual",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 272.47432708740234,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'xnli' containing subtasks: xnli_de, xnli_ca, xnli_bg, xnli_ar"
      }
    },
    {
      "benchmark_name": "xcopa",
      "task_name": "xcopa",
      "tags": [
        "multilingual",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 101.56586790084839,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'xcopa' containing subtasks: xcopa_et, xcopa_ht, xcopa_eu, xcopa_id, xcopa_it"
      }
    },
    {
      "benchmark_name": "xstorycloze",
      "task_name": "xstorycloze",
      "tags": [
        "multilingual",
        "long context",
        "creative writing"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 72.29564499855042,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'xstorycloze' containing subtasks: xstorycloze_eu, xstorycloze_es, xstorycloze_ca, xstorycloze_en, xstorycloze_ar"
      }
    },
    {
      "benchmark_name": "xwinograd",
      "task_name": "xwinograd",
      "tags": [
        "multilingual",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 66.13555788993835,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'xwinograd' containing subtasks: xwinograd_pt, xwinograd_fr, xwinograd_en, xwinograd_ru, xwinograd_jp"
      }
    },
    {
      "benchmark_name": "paws_x",
      "task_name": "paws_x",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.04454016685486,
      "samples_retrieved": 0,
      "error": "Task 'paws_x' not found: individual ('paws_x'), group (No group expansion found), large group (Task 'paws_x' not found at any level), README (No README groups found for 'paws_x')",
      "metadata": {}
    },
    {
      "benchmark_name": "mmmlu",
      "task_name": "mmmlu",
      "tags": [
        "general knowledge",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.26138186454773,
      "samples_retrieved": 0,
      "error": "Task 'mmmlu' not found: individual ('mmmlu'), group (No group expansion found), large group (Task 'mmmlu' not found at any level), README (No README groups found for 'mmmlu')",
      "metadata": {}
    },
    {
      "benchmark_name": "mgsm",
      "task_name": "mgsm",
      "tags": [
        "multilingual",
        "mathematics",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 81.93674182891846,
      "samples_retrieved": 3,
      "error": null,
      "metadata": {
        "description": "Group task 'mgsm' containing subtasks: mgsm_direct_ca, mgsm_direct_de, mgsm_direct_bn"
      }
    },
    {
      "benchmark_name": "belebele",
      "task_name": "belebele",
      "tags": [
        "multilingual",
        "adversarial robustness",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 161.31223702430725,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'belebele' containing subtasks: belebele_als_Latn, belebele_apc_Arab, belebele_afr_Latn, belebele_amh_Ethi, belebele_acm_Arab"
      }
    },
    {
      "benchmark_name": "medqa",
      "task_name": "medqa",
      "tags": [
        "medical",
        "science",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 21.59283685684204,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'medqa' containing subtasks: medqa_4options"
      }
    },
    {
      "benchmark_name": "pubmedqa",
      "task_name": "pubmedqa",
      "tags": [
        "medical",
        "science",
        "reasoning"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 62.04237508773804,
      "samples_retrieved": 0,
      "error": "Task 'pubmedqa' not found: individual (The repository for bigbio/pubmed_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/bigbio/pubmed_qa.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.), group (No group expansion found), large group (Task 'pubmedqa' not found at any level), README (No README groups found for 'pubmedqa')",
      "metadata": {}
    },
    {
      "benchmark_name": "lambada",
      "task_name": "lambada",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 49.930814027786255,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'lambada' containing subtasks: lambada_standard, lambada_openai"
      }
    },
    {
      "benchmark_name": "lambada_cloze",
      "task_name": "lambada_cloze",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 33.59743928909302,
      "samples_retrieved": 4,
      "error": null,
      "metadata": {
        "description": "Group task 'lambada_cloze' containing subtasks: lambada_standard_cloze_yaml, lambada_openai_cloze_yaml"
      }
    },
    {
      "benchmark_name": "lambada_multilingual",
      "task_name": "lambada_multilingual",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 62.445470094680786,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'lambada_multilingual' containing subtasks: lambada_openai_mt_en, lambada_openai_mt_it, lambada_openai_mt_de, lambada_openai_mt_es, lambada_openai_mt_fr"
      }
    },
    {
      "benchmark_name": "wikitext",
      "task_name": "wikitext",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 11.808789014816284,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 60,
        "output_type": "loglikelihood_rolling",
        "description": null
      }
    },
    {
      "benchmark_name": "narrativeqa",
      "task_name": "narrativeqa",
      "tags": [
        "reasoning",
        "long context",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.435909032821655,
      "samples_retrieved": 0,
      "error": "Task 'narrativeqa' not found: individual ('narrativeqa'), group (No group expansion found), large group (Task 'narrativeqa' not found at any level), README (No README groups found for 'narrativeqa')",
      "metadata": {}
    },
    {
      "benchmark_name": "scrolls",
      "task_name": "scrolls",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 134.74488615989685,
      "samples_retrieved": 0,
      "error": "No samples could be retrieved from any subtasks of scrolls",
      "metadata": {}
    },
    {
      "benchmark_name": "mctaco",
      "task_name": "mctaco",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 27.127694129943848,
      "samples_retrieved": 0,
      "error": "Task 'mctaco' not found: individual ('mctaco'), group (No group expansion found), large group (Task 'mctaco' not found at any level), README (No README groups found for 'mctaco')",
      "metadata": {}
    },
    {
      "benchmark_name": "prost",
      "task_name": "prost",
      "tags": [
        "long context",
        "history",
        "reasoning"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 12.309722185134888,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "total_docs": 18736,
        "output_type": "multiple_choice",
        "description": null
      }
    },
    {
      "benchmark_name": "blimp",
      "task_name": "blimp",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 206.89719104766846,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'blimp' containing subtasks: blimp_anaphor_number_agreement, blimp_anaphor_gender_agreement, blimp_animate_subject_trans, blimp_animate_subject_passive, blimp_adjunct_island"
      }
    },
    {
      "benchmark_name": "unscramble",
      "task_name": "unscramble",
      "tags": [
        "long context",
        "reasoning",
        "general knowledge"
      ],
      "num_samples": 5,
      "success": true,
      "loading_time_seconds": 63.80028700828552,
      "samples_retrieved": 5,
      "error": null,
      "metadata": {
        "description": "Group task 'unscramble' containing subtasks: reversed_words, anagrams1, anagrams2, cycle_letters, random_insertion"
      }
    },
    {
      "benchmark_name": "wmt",
      "task_name": "wmt",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.785402297973633,
      "samples_retrieved": 0,
      "error": "Task 'wmt' not found: individual ('wmt'), group (No group expansion found), large group (Task 'wmt' not found at any level), README (No README groups found for 'wmt')",
      "metadata": {}
    },
    {
      "benchmark_name": "big_bench",
      "task_name": "big_bench",
      "tags": [
        "reasoning",
        "general knowledge",
        "science"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 26.72892427444458,
      "samples_retrieved": 0,
      "error": "Task 'big_bench' not found: individual ('big_bench'), group (No group expansion found), large group (Task 'big_bench' not found at any level), README (No README groups found for 'big_bench')",
      "metadata": {}
    },
    {
      "benchmark_name": "babi",
      "task_name": "babi",
      "tags": [
        "reasoning",
        "general knowledge",
        "long context"
      ],
      "num_samples": 5,
      "success": false,
      "loading_time_seconds": 12.64228081703186,
      "samples_retrieved": 0,
      "error": "No documents found for this task",
      "metadata": {}
    }
  ]
}