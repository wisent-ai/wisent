#!/usr/bin/env python3
"""
Test DAC vector generation correctness by comparing our implementation
with reference data generated by the original DAC implementation.

This test loads the ITA and ENG datasets, generates DAC steering vectors
using our wisent-guard tensor-based implementation, and compares with reference
vectors generated by the original DAC implementation.
"""

import json
import time
import torch
import torch.nn.functional as F
import pytest
import sys
from pathlib import Path
from typing import List, Dict, Any, Tuple

# Add wisent-guard to path
WISENT_PATH = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(WISENT_PATH))

# Import wisent-guard tensor-based DAC
from wisent_guard.core.steering_methods_tensor.dac_attention import DAC
from wisent_guard.core.contrastive_pairs import ContrastivePairSet, ContrastivePair
from wisent_guard.core.response import PositiveResponse, NegativeResponse

# Add current directory to path for local imports
sys.path.insert(0, str(Path(__file__).parent))

from const import (
    MODEL_NAME,
    DIFF_ACTIVATIONS_PATH,
    DATASET_A_NAME,
    DATASET_B_NAME,
    MAX_EXAMPLES,
    MAX_NEW_TOKENS,
    ICL_EXAMPLES,
    REFERENCE_DATA_PATH,
    TORCH_DTYPE,
    COSINE_SIMILARITY_THRESHOLD,
    DYNAMIC_CONFIG,
)

# Import aggressive_memory_cleanup
utils_path = Path(__file__).parent.parent / "utils.py"
sys.path.insert(0, str(Path(__file__).parent.parent))
from utils import aggressive_memory_cleanup


def load_datasets() -> Tuple[List[Dict], List[Dict]]:
    """Load ITA and ENG datasets from reference_data directory."""
    # Load ITA dataset
    ita_path = REFERENCE_DATA_PATH / "ita_train.json"
    with open(ita_path, "r", encoding="utf-8") as f:
        ita_data = json.load(f)

    # Load ENG dataset
    eng_path = REFERENCE_DATA_PATH / "eng_train.json"
    with open(eng_path, "r", encoding="utf-8") as f:
        eng_data = json.load(f)

    return ita_data, eng_data


def create_contrastive_pairs(ita_data: List[Dict], eng_data: List[Dict]) -> ContrastivePairSet:
    """
    Create ContrastivePairSet where ITA responses are positive and ENG responses are negative.

    Args:
        ita_data: Italian dataset
        eng_data: English dataset

    Returns:
        ContrastivePairSet with language pairs
    """
    pairs = []
    max_pairs = min(len(ita_data), len(eng_data), MAX_EXAMPLES)

    for i in range(max_pairs):
        ita_item = ita_data[i]
        eng_item = eng_data[i]

        # Ensure both have the same input
        assert ita_item["input"] == eng_item["input"], f"Mismatched inputs at index {i}"

        prompt = ita_item["input"]
        ita_response = ita_item["output"]
        eng_response = eng_item["output"]

        # Create responses (ITA as positive, ENG as negative for steering toward Italian)
        pos_response = PositiveResponse(text=ita_response)
        neg_response = NegativeResponse(text=eng_response)

        # Create contrastive pair
        pair = ContrastivePair(
            prompt=prompt, positive_response=pos_response, negative_response=neg_response, label="language_steering"
        )

        pairs.append(pair)

    # Create ContrastivePairSet
    pair_set = ContrastivePairSet(name="ita_eng_language_steering", pairs=pairs, task_type="language_steering")

    return pair_set


def compare_with_reference(our_tensor: torch.Tensor) -> Dict[str, Any]:
    """
    Compare our generated tensor with reference vectors.

    Args:
        our_tensor: Our generated DAC tensor

    Returns:
        Dictionary with comparison metrics
    """
    results = {}

    # Load and compare with difference activations (the main steering tensor)
    if DIFF_ACTIVATIONS_PATH.exists():
        ref_tensor = torch.load(DIFF_ACTIVATIONS_PATH, map_location="cpu")

        # Ensure tensors are on same device
        our_tensor_cpu = our_tensor.cpu()
        ref_tensor_cpu = ref_tensor.cpu()

        # Calculate metrics
        cosine_sim = F.cosine_similarity(our_tensor_cpu.flatten(), ref_tensor_cpu.flatten(), dim=0).item()
        mse = F.mse_loss(our_tensor_cpu, ref_tensor_cpu).item()

        results["diff_tensor"] = {
            "cosine_similarity": cosine_sim,
            "mse": mse,
            "our_shape": list(our_tensor.shape),
            "ref_shape": list(ref_tensor.shape),
            "our_norm": torch.norm(our_tensor_cpu).item(),
            "ref_norm": torch.norm(ref_tensor_cpu).item(),
            "shapes_match": our_tensor.shape == ref_tensor.shape,
        }
    else:
        pytest.skip(f"Reference tensor not found at {DIFF_ACTIVATIONS_PATH}")

    return results


@pytest.mark.slow
class TestDACTensorGeneration:
    """Test suite for DAC tensor generation validation."""

    @pytest.mark.slow
    @pytest.mark.heavy
    def test_dac_tensor_generation_with_reference(self):
        """Test DAC tensor generation and compare with reference vectors.

        Uses icl0_tok10 configuration (10-token sequences) which produces non-zero
        difference vectors between Italian and English activations, enabling proper steering.
        """
        # Skip if no GPU available (DAC is resource-intensive)
        if not torch.cuda.is_available():
            pytest.skip("GPU not available for DAC tensor generation")

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        try:
            print("\n" + "=" * 70)
            print("DAC Tensor Generation Test")
            print("=" * 70)
            print(f"Model: {MODEL_NAME}")
            print(f"Datasets: {DATASET_A_NAME} vs {DATASET_B_NAME}")
            print(f"Examples: {MAX_EXAMPLES}")
            print(f"Max new tokens: {MAX_NEW_TOKENS} (icl0_tok10 config)")
            print(f"Starting alpha: {DYNAMIC_CONFIG['starting_alpha']}")
            print("=" * 70)

            # Step 1: Load datasets
            print("\n[1/5] Loading datasets...")
            ita_data, eng_data = load_datasets()
            assert len(ita_data) >= MAX_EXAMPLES, f"Insufficient ITA data: {len(ita_data)}"
            assert len(eng_data) >= MAX_EXAMPLES, f"Insufficient ENG data: {len(eng_data)}"
            print(f"‚úì Loaded {len(ita_data)} ITA and {len(eng_data)} ENG examples")

            # Step 2: Create contrastive pairs
            print("\n[2/5] Creating contrastive pairs...")
            contrastive_pairs = create_contrastive_pairs(ita_data, eng_data)
            assert len(contrastive_pairs.pairs) == MAX_EXAMPLES
            print(f"‚úì Created {len(contrastive_pairs.pairs)} contrastive pairs")

            # Step 3: Initialize DAC method
            print("\n[3/5] Initializing DAC tensor-based method...")
            dac_method = DAC(
                device="cuda:0" if torch.cuda.is_available() else "cpu",
                model_name=MODEL_NAME,
                max_examples=MAX_EXAMPLES,
                max_new_tokens=MAX_NEW_TOKENS,
                torch_dtype=TORCH_DTYPE,
            )
            print(f"‚úì DAC method initialized: {dac_method}")

            # Step 4: Train DAC using contrastive pairs
            print("\n[4/5] Training DAC with tensor extraction...")
            start_time = time.time()
            training_stats = dac_method.train_property("language_steering", contrastive_pairs)
            training_time = time.time() - start_time

            assert training_stats["success"], "DAC training failed"
            assert "tensor_norm" in training_stats
            assert "tensor_shape" in training_stats

            print(f"‚úì DAC training completed in {training_time:.1f} seconds")
            print(f"  Tensor shape: {training_stats['tensor_shape']}")
            print(f"  Tensor norm: {training_stats['tensor_norm']:.4f}")

            # Step 5: Compare with reference
            print("\n[5/5] Comparing with reference vectors...")
            steering_tensor = dac_method.get_steering_tensor()
            comparison_results = compare_with_reference(steering_tensor)

            # Display comparison results
            if "diff_tensor" in comparison_results:
                metrics = comparison_results["diff_tensor"]
                cosine_sim = metrics["cosine_similarity"]
                mse = metrics["mse"]
                shapes_match = metrics["shapes_match"]

                print(f"\nüìä Comparison Results:")
                print(f"  Cosine similarity: {cosine_sim:.6f}")
                print(f"  MSE: {mse:.8f}")
                print(f"  Our shape: {metrics['our_shape']}")
                print(f"  Ref shape: {metrics['ref_shape']}")
                print(f"  Our norm: {metrics['our_norm']:.4f}")
                print(f"  Ref norm: {metrics['ref_norm']:.4f}")
                print(f"  Shapes match: {shapes_match}")

                # STRICT ASSERTIONS
                assert cosine_sim > 0.99, f"‚ùå Cosine similarity {cosine_sim:.6f} is below threshold 0.99"
                assert mse < 1e-6, f"‚ùå MSE {mse:.8f} is too high (should be < 1e-6)"
                assert shapes_match, f"‚ùå Tensor shapes don't match: {metrics['our_shape']} vs {metrics['ref_shape']}"

                # Additional quality checks
                norm_ratio = metrics["our_norm"] / metrics["ref_norm"]
                assert 0.95 < norm_ratio < 1.05, (
                    f"‚ùå Norm ratio {norm_ratio:.4f} is outside acceptable range [0.95, 1.05]"
                )

                print(f"\n‚úÖ All assertions passed!")
                print(f"‚úÖ DAC tensor generation matches reference with cosine similarity > 0.99")

            print("\n" + "=" * 70)
            print("SUCCESS: DAC tensor generation test completed!")
            print("=" * 70)

        finally:
            # Clean up GPU memory
            aggressive_memory_cleanup()

    def test_dac_tensor_save_load(self):
        """Test saving and loading DAC tensor."""
        if not torch.cuda.is_available():
            pytest.skip("GPU not available for DAC tensor generation")

        save_path = REFERENCE_DATA_PATH / "test_dac_method.pt"

        try:
            # Initialize DAC
            dac_method = DAC(device="cuda:0" if torch.cuda.is_available() else "cpu", model_name=MODEL_NAME)

            # Try to load existing tensor
            if save_path.exists():
                success = dac_method.load_steering_tensor(str(save_path))
                assert success, "Failed to load DAC tensor"

                # Verify loaded tensor
                tensor = dac_method.get_steering_tensor()
                assert tensor is not None
                assert tensor.shape[0] == MAX_NEW_TOKENS  # Check steps dimension

                # Get statistics
                stats = dac_method.get_statistics()
                assert stats["is_trained"]
                assert stats["method"] == "DAC"
                assert "steering_tensor_shape" in stats

                print(f"‚úÖ Successfully loaded DAC tensor from {save_path}")
                print(f"  Shape: {stats['steering_tensor_shape']}")
                print(f"  Norm: {stats['steering_tensor_norm']:.4f}")
            else:
                pytest.skip(f"No saved tensor found at {save_path}")

        finally:
            aggressive_memory_cleanup()

    @pytest.mark.slow
    @pytest.mark.heavy
    def test_dac_dynamic_generation_comparison(self):
        """Test DAC dynamic generation against reference completions from original DAC.

        Uses icl0_tok10 tensors (10-token sequences) with stronger steering (Œ±=4.0)
        to ensure non-zero alpha values and effective Italian language steering.
        """
        # Skip if no GPU available
        if not torch.cuda.is_available():
            pytest.skip("GPU not available for DAC generation testing")

        # Check if reference text completions exist
        unsteered_path = REFERENCE_DATA_PATH / "text_completions_unsteered.json"
        dynamic_path = REFERENCE_DATA_PATH / "text_completions_dynamic_steering.json"

        if not (unsteered_path.exists() and dynamic_path.exists()):
            pytest.skip(
                f"Reference text completions not found. Run generate_data_with_original_dac_implementation.py first."
            )

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        try:
            print("\n" + "=" * 70)
            print("DAC Dynamic Generation Comparison Test")
            print("=" * 70)

            # Load reference data
            with open(unsteered_path, "r", encoding="utf-8") as f:
                reference_unsteered = json.load(f)

            with open(dynamic_path, "r", encoding="utf-8") as f:
                reference_dynamic = json.load(f)

            print(f"Reference unsteered: {len(reference_unsteered)} completions")
            print(f"Reference dynamic: {len(reference_dynamic)} completions")

            # Initialize and load DAC method
            print("\n[1/4] Initializing DAC and loading saved tensor...")
            dac_method = DAC(
                device="cuda:0",
                model_name=MODEL_NAME,
                max_examples=MAX_EXAMPLES,
                max_new_tokens=MAX_NEW_TOKENS,
                torch_dtype=TORCH_DTYPE,
            )

            # Try to load existing tensor first, or train if needed
            save_path = REFERENCE_DATA_PATH / "dac_method.pt"
            if save_path.exists():
                print(f"   Loading existing DAC tensor from {save_path}")
                success = dac_method.load_steering_tensor(str(save_path))
                if not success:
                    pytest.skip("Failed to load DAC tensor")
            else:
                # Need to train the DAC method
                print("   No saved DAC tensor found, training from datasets...")
                ita_data, eng_data = load_datasets()
                contrastive_pairs = create_contrastive_pairs(ita_data, eng_data)
                training_stats = dac_method.train_property("language_steering", contrastive_pairs)
                assert training_stats["success"], "DAC training failed"

                # Save for future use
                dac_method.save_steering_tensor(str(save_path))
                print(f"   Saved DAC tensor to {save_path}")

            # Test unsteered generation
            print("\n[2/4] Testing unsteered generation...")
            unsteered_matches = 0

            test_prompts = [item["prompt"] for item in reference_unsteered]
            for i, (prompt, ref_item) in enumerate(zip(test_prompts, reference_unsteered)):
                print(f"   Prompt {i + 1}/{len(test_prompts)}: {prompt[:40]}...")

                # Generate with our DAC (no steering)
                our_result = dac_method.generate_with_steering(
                    prompt=prompt,
                    max_new_tokens=MAX_NEW_TOKENS,
                    steering_strength=0.0,  # No steering for baseline
                    timing_strategy="normal",
                )

                ref_text = ref_item["generated_text"]

                # Compare results (basic text comparison)
                similarity_score = self._calculate_text_similarity(our_result, ref_text)
                print(f"      Our:       {our_result[:60]}...")
                print(f"      Reference: {ref_text[:60]}...")
                print(f"      Similarity: {similarity_score:.3f}")

                if similarity_score > 0.3:  # Reasonable threshold for text similarity
                    unsteered_matches += 1

            unsteered_quality = unsteered_matches / len(reference_unsteered)
            print(
                f"   Unsteered generation quality: {unsteered_matches}/{len(reference_unsteered)} ({unsteered_quality:.2%})"
            )

            # Test dynamic steering generation
            print("\n[3/4] Testing dynamic steering generation...")
            dynamic_matches = 0
            adaptation_scores = []

            # Filter dynamic reference to one top_p value for testing
            test_dynamic_refs = [item for item in reference_dynamic if item.get("top_p") == 0.9]

            for i, ref_item in enumerate(test_dynamic_refs[:5]):  # Test subset
                prompt = ref_item["prompt"]
                print(f"   Dynamic prompt {i + 1}/5: {prompt[:40]}...")

                # Generate with our DAC dynamic steering
                try:
                    our_result = dac_method.generate_with_dynamic_steering(
                        prompt=prompt,
                        property_weights={"language_steering": 1.0},
                        max_new_tokens=MAX_NEW_TOKENS,
                        starting_alpha=DYNAMIC_CONFIG["starting_alpha"],
                        top_p=0.9,
                    )

                    our_text = our_result["generated_text"]
                    our_adaptation = our_result["adaptation_effectiveness"]
                    ref_text = ref_item["generated_text"]

                    # Compare text similarity
                    similarity_score = self._calculate_text_similarity(our_text, ref_text)
                    adaptation_scores.append(our_adaptation)

                    print(f"      Our:           {our_text[:50]}...")
                    print(f"      Reference:     {ref_text[:50]}...")
                    print(f"      Similarity:    {similarity_score:.3f}")
                    print(f"      Adaptation:    {our_adaptation:.3f}")

                    if similarity_score > 0.2:  # Lower threshold for dynamic (more variability expected)
                        dynamic_matches += 1

                except Exception as e:
                    print(f"      ‚ö†Ô∏è Error in dynamic generation: {e}")

            dynamic_quality = dynamic_matches / len(test_dynamic_refs) if test_dynamic_refs else 0
            avg_adaptation = sum(adaptation_scores) / len(adaptation_scores) if adaptation_scores else 0

            print(f"   Dynamic generation quality: {dynamic_matches}/{len(test_dynamic_refs)} ({dynamic_quality:.2%})")
            print(f"   Average adaptation effectiveness: {avg_adaptation:.3f}")

            # Steering effectiveness test
            print("\n[4/4] Testing steering effectiveness...")
            steering_effects = []

            for prompt in test_prompts[:3]:  # Test few prompts
                # Generate without steering
                unsteered = dac_method.generate_with_steering(
                    prompt=prompt,
                    steering_strength=0.0,
                    max_new_tokens=10,
                )

                # Generate with steering
                steered = dac_method.generate_with_steering(
                    prompt=prompt,
                    property_weights={"language_steering": 1.0},
                    steering_strength=1.0,
                    max_new_tokens=10,
                )

                # Check if steering changed the output
                is_different = unsteered != steered
                steering_effects.append(is_different)

                print(f"   Prompt: {prompt[:30]}...")
                print(f"      Unsteered: {unsteered[:40]}...")
                print(f"      Steered:   {steered[:40]}...")
                print(f"      Changed:   {is_different}")

            steering_rate = sum(steering_effects) / len(steering_effects) if steering_effects else 0
            print(f"   Steering effectiveness: {sum(steering_effects)}/{len(steering_effects)} ({steering_rate:.2%})")

            # Final assertions
            print("\n" + "=" * 70)
            print("GENERATION COMPARISON RESULTS")
            print("=" * 70)
            print(f"Unsteered quality: {unsteered_quality:.2%}")
            print(f"Dynamic quality: {dynamic_quality:.2%}")
            print(f"Adaptation effectiveness: {avg_adaptation:.3f}")
            print(f"Steering effectiveness: {steering_rate:.2%}")
            print("=" * 70)

            # Basic quality assertions
            assert unsteered_quality > 0.1, f"Very low unsteered generation quality: {unsteered_quality:.2%}"
            assert steering_rate > 0.3, f"Steering not effective enough: {steering_rate:.2%}"

            if adaptation_scores:
                assert avg_adaptation > 0.0, f"No adaptation detected: {avg_adaptation:.3f}"

            print("‚úÖ DAC dynamic generation comparison test passed!")

        finally:
            aggressive_memory_cleanup()

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """Calculate simple text similarity score."""
        if not text1 or not text2:
            return 0.0

        # Simple word-based similarity
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())

        if not words1 and not words2:
            return 1.0
        if not words1 or not words2:
            return 0.0

        intersection = words1.intersection(words2)
        union = words1.union(words2)

        return len(intersection) / len(union) if union else 0.0
