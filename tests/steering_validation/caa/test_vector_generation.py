#!/usr/bin/env python3
"""
Test vector generation correctness by comparing our implementation
with reference data.

This test loads the same dataset used by the reference implementation,
generates steering vectors using our CAA implementation, and compares
the results.
"""

import json
import torch
import pytest
from pathlib import Path
import sys

# Add wisent-guard to path
WISENT_PATH = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(WISENT_PATH))

from wisent_guard.core.steering_methods.caa_original import CAA
from wisent_guard.core.aggregation import ControlVectorAggregationMethod

from .const import (
    MODEL_NAME,
    MODEL_HIDDEN_DIM,
    LAYER_INDEX,
    DEVICE,
    HALLUCINATION_DATASET_PATH,
    HALLUCINATION_VECTOR_PATH,
    MAX_EXAMPLES,
    NORMALIZATION_METHOD,
)
from .model_utils import RealModelWrapper, create_real_contrastive_pairs, create_caa_original_contrastive_pairs
from ..utils import aggressive_memory_cleanup


def load_reference_data():
    """Load reference vector and activations."""
    if not HALLUCINATION_VECTOR_PATH.exists():
        pytest.skip(f"Reference data not found at {HALLUCINATION_VECTOR_PATH}")

    data = torch.load(HALLUCINATION_VECTOR_PATH, map_location=DEVICE, weights_only=False)
    return data


def load_test_dataset():
    """Load the hallucination dataset."""
    dataset_path = HALLUCINATION_DATASET_PATH

    with open(dataset_path, "r") as f:
        data = json.load(f)

    return data


@pytest.mark.slow
class TestVectorGeneration:
    """Test suite for CAA vector generation validation."""

    def test_compare_with_reference_vector(self):
        """Compare our vector with reference vector generated by CAA implementation."""

        # Load reference data
        try:
            ref_data = load_reference_data()
            ref_vector = ref_data["vector"]

        except Exception as e:
            pytest.skip(f"Reference data not available: {e}")

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        # Create contrastive pairs using EXACT CAA method (same dataset file)
        print("ðŸ”§ Using original CAA implementation for consistency...")
        pair_set = create_caa_original_contrastive_pairs(layer_idx=LAYER_INDEX)

        # Train our implementation
        caa = CAA(
            device=DEVICE,
            aggregation_method=ControlVectorAggregationMethod.CAA,
            normalization_method=NORMALIZATION_METHOD,
        )

        _ = caa.train(pair_set, layer_index=LAYER_INDEX)
        our_vector = caa.get_steering_vector()

        # Compare with reference
        cosine_sim = torch.nn.functional.cosine_similarity(our_vector, ref_vector, dim=0).item()
        norm_ratio = torch.norm(our_vector).item() / torch.norm(ref_vector).item()

        print(f"Comparison with reference:")
        print(f"  Our vector norm:    {torch.norm(our_vector).item():.4f}")
        print(f"  Reference norm:     {torch.norm(ref_vector).item():.4f}")
        print(f"  Cosine similarity:  {cosine_sim:.4f}")
        print(f"  Norm ratio:         {norm_ratio:.4f}")

        # Assertions for correctness
        assert cosine_sim > 0.99, f"Low cosine similarity with reference: {cosine_sim}"
        assert 0.99 < norm_ratio < 1.01, f"Large difference in vector magnitude: {norm_ratio}"

    def test_vector_computation_method(self):
        """Test the vector computation method directly with real model activations."""

        # Load test dataset (small subset)
        dataset = load_test_dataset()
        dataset_subset = dataset[:10]

        # Initialize real model
        real_model = RealModelWrapper(MODEL_NAME)

        # Create contrastive pairs
        pair_set = create_real_contrastive_pairs(
            dataset_subset, real_model, layer_idx=LAYER_INDEX, max_pairs=len(dataset_subset)
        )

        # Extract positive and negative activations
        pos_activations = []
        neg_activations = []

        for pair in pair_set.pairs:
            pos_activations.append(pair.positive_response.activations)
            neg_activations.append(pair.negative_response.activations)

        pos_stack = torch.stack(pos_activations)
        neg_stack = torch.stack(neg_activations)

        # Expected result (reference method)
        expected_vector = (pos_stack - neg_stack).mean(dim=0)

        # Test our aggregation function
        from wisent_guard.core.aggregation import create_control_vector_from_contrastive_pairs

        our_vector, _ = create_control_vector_from_contrastive_pairs(
            pos_stack, neg_stack, ControlVectorAggregationMethod.CAA, DEVICE
        )

        # Ensure both vectors are on the same device for comparison
        expected_vector = expected_vector.to(DEVICE)

        # Should match exactly for CAA method
        assert torch.allclose(our_vector, expected_vector, atol=1e-6), "Vector computation mismatch"

    def test_real_model_vector_generation(self):
        """Test vector generation using real Llama2 model."""

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        # Load test dataset (small subset for speed)
        dataset = load_test_dataset()

        # Initialize real model
        real_model = RealModelWrapper(MODEL_NAME)

        try:
            # Create contrastive pairs with real activations
            pair_set = create_real_contrastive_pairs(
                dataset, real_model, layer_idx=LAYER_INDEX, max_pairs=min(20, len(dataset))
            )

            # Train our CAA implementation
            caa = CAA(
                device=DEVICE,
                aggregation_method=ControlVectorAggregationMethod.CAA,
                normalization_method=NORMALIZATION_METHOD,
            )

            training_stats = caa.train(pair_set, layer_index=LAYER_INDEX)
            our_vector = caa.get_steering_vector()

            # Basic checks
            assert our_vector.shape[0] == MODEL_HIDDEN_DIM, f"Wrong vector dimension: {our_vector.shape}"
            assert torch.norm(our_vector).item() > 0.1, "Vector norm too small"
            assert not torch.any(torch.isnan(our_vector)), "Vector contains NaN"
            assert not torch.any(torch.isinf(our_vector)), "Vector contains Inf"

            # Check training stats
            assert "method" in training_stats
            assert training_stats["method"] == "CAA"
            assert "layer_index" in training_stats
            assert training_stats["layer_index"] == LAYER_INDEX

        finally:
            # Clean up model to free GPU memory
            del real_model
            aggressive_memory_cleanup()

    def test_normalization_effects(self):
        """Test that normalization affects vectors as expected with real model."""

        # Aggressive memory cleanup before starting
        aggressive_memory_cleanup()

        # Load test dataset (small subset)
        dataset = load_test_dataset()
        dataset_subset = dataset[:10]

        # Initialize real model
        real_model = RealModelWrapper(MODEL_NAME)

        try:
            # Create contrastive pairs
            pair_set = create_real_contrastive_pairs(
                dataset_subset, real_model, layer_idx=LAYER_INDEX, max_pairs=len(dataset_subset)
            )

            # Test supported normalization methods
            normalizations = ["none"]  # Just test basic functionality for now
            vectors = {}

            for norm_method in normalizations:
                caa = CAA(
                    device=DEVICE,
                    aggregation_method=ControlVectorAggregationMethod.CAA,
                    normalization_method=norm_method,
                )

                caa.train(pair_set, layer_index=LAYER_INDEX)
                vectors[norm_method] = caa.get_steering_vector()

            # Basic checks
            for method, vector in vectors.items():
                assert vector.shape[0] > 0, f"Empty vector for {method}"
                assert torch.norm(vector).item() > 0, f"Zero norm vector for {method}"
                assert not torch.any(torch.isnan(vector)), f"NaN values in {method} vector"
                assert not torch.any(torch.isinf(vector)), f"Inf values in {method} vector"

        finally:
            # Clean up model to free GPU memory
            del real_model
            aggressive_memory_cleanup()
