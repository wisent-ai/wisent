\begin{thebibliography}{10}

\bibitem{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.
\newblock Program synthesis with large language models.
\newblock {\em arXiv preprint arXiv:2108.07732}, 2021.

\bibitem{berant2013webquestions}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In {\em Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, pages 1533--1544, Seattle, Washington, USA, 2013. Association for Computational Linguistics.

\bibitem{bisk2019piqa}
Yonatan Bisk, Rowan Zellers, Ronan Le~Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock {\em arXiv preprint arXiv:1911.11641}, 2019.

\bibitem{brown2020gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{cassano2022multipl-e}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
\newblock Multipl-e: A scalable and extensible approach to benchmarking neural code generation.
\newblock {\em arXiv preprint arXiv:2208.08227}, 2022.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde~de Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock 2021.

\bibitem{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock {\em arXiv preprint arXiv:1905.10044}, 2019.

\bibitem{clark2018arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock {\em arXiv preprint arXiv:1803.05457}, 2018.

\bibitem{cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, and Jerry Tworek.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{codeparrot_instructhumaneval_2023}
{CodeParrot}.
\newblock Instructhumaneval, 2023.

\bibitem{demarneffe2019commitmentbank}
Marie-Catherine de~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock The commitmentbank: Investigating projection in naturally occurring discourse.
\newblock In {\em Proceedings of Sinn und Bedeutung 23}, volume~2, pages 107--124, Bellaterra (Cerdanyola del Vall{\`e}s), 2019. Universitat Aut{\`o}noma de Barcelona.

\bibitem{du2024mercury}
Mingzhe Du, Anh~Tuan Luu, Bin Ji, Liu Qian, and See-Kiong Ng.
\newblock Mercury: A code efficiency benchmark for code llms.
\newblock {\em arXiv preprint arXiv:2402.07844}, 2024.

\bibitem{du2025supergpqa}
Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, and {et al.}
\newblock Supergpqa: Scaling llm evaluation across 285 graduate disciplines.
\newblock {\em arXiv preprint arXiv:2502.14739}, 2025.

\bibitem{dua2019drop}
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
\newblock Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.
\newblock {\em arXiv preprint arXiv:1903.00161}, 2019.

\bibitem{hendrycks2021apps}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt.
\newblock Measuring coding challenge competence with apps.
\newblock {\em arXiv preprint arXiv:2105.09938}, 2021.

\bibitem{hendrycks2021mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{hendrycks2021math}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock {\em arXiv preprint arXiv:2103.03874}, 2021.

\bibitem{huggingfaceh4_math500_2024}
{HuggingFaceH4}.
\newblock Math-500, 2024.

\bibitem{iyer2018mapping}
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer.
\newblock Mapping language to code in programmatic context.
\newblock {\em arXiv preprint arXiv:1808.09588}, 2018.

\bibitem{jain2024livecodebench}
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.
\newblock Livecodebench: Holistic and contamination free evaluation of large language models for code.
\newblock {\em arXiv preprint arXiv:2403.07974}, 2024.

\bibitem{maxwelljia2024aime}
Maxwell Jia.
\newblock Aime problem set 2024, 2024.

\bibitem{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1601--1611, Vancouver, Canada, 2017. Association for Computational Linguistics.

\bibitem{kwiatkowski2019naturalquestions}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7:452--466, 2019.

\bibitem{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock {\em arXiv preprint arXiv:1704.04683}, 2017.

\bibitem{lai2022ds1000}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
\newblock Ds-1000: A natural and reliable benchmark for data science code generation.
\newblock {\em arXiv preprint arXiv:2211.11501}, 2022.

\bibitem{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock {\em arXiv preprint arXiv:2109.07958}, 2021.

\bibitem{liu2023evalplus}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock In {\em NeurIPS 2023 Datasets and Benchmarks Track}, 2023.

\bibitem{liu2024livemathbench}
Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen.
\newblock Are your llms capable of stable reasoning?
\newblock {\em arXiv preprint arXiv:2412.13147}, 2024.

\bibitem{lu2021codexglue}
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge~Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, Shao~Kun Deng, Shengyu Fu, and Shujie Liu.
\newblock Codexglue: A machine learning benchmark dataset for code understanding and generation.
\newblock {\em arXiv preprint arXiv:2102.04664}, 2021.

\bibitem{mathai2025aime25}
{Math-AI}.
\newblock Aime problem set 2025, 2025.

\bibitem{matharena2025hmmt}
{MathArena}.
\newblock Hmmt february 2025, 2025.

\bibitem{merity2016pointersentinel}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2016.

\bibitem{miao2021asdiv}
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.
\newblock A diverse corpus for evaluating and developing english math word problem solvers.
\newblock {\em arXiv preprint arXiv:2106.15772}, 2021.

\bibitem{mihaylov2018openbookqa}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, 2018.

\bibitem{phan2025hle}
Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, and {et al.}
\newblock Humanity's last exam.
\newblock {\em arXiv preprint arXiv:2501.14249}, 2025.

\bibitem{rajpurkar2018squad2}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock {\em arXiv preprint arXiv:1806.03822}, 2018.

\bibitem{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock Coqa: A conversational question answering challenge.
\newblock {\em arXiv preprint arXiv:1808.07042}, 2019.

\bibitem{rein2023gpqa}
David Rein, Betty~Li Hou, Asa Cooper~Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R. Bowman.
\newblock Gpqa: A graduate-level google-proof q{\&}a benchmark.
\newblock {\em arXiv preprint arXiv:2311.12022}, 2023.

\bibitem{roemmele2011copa}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S. Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In {\em AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning}, Stanford, CA, 2011.

\bibitem{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em arXiv preprint arXiv:1907.10641}, 2019.

\bibitem{wang2022recode}
Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali~Krishna Ramanathan, Dan Roth, and Bing Xiang.
\newblock Recode: Robustness evaluation of code generation models.
\newblock {\em arXiv preprint arXiv:2212.10264}, 2022.

\bibitem{wang2025polymath}
Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, and Jingren Zhou.
\newblock Polymath: Evaluating mathematical reasoning in multilingual contexts.
\newblock {\em arXiv preprint arXiv:2504.18428}, 2025.

\bibitem{yin2018conala}
Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig.
\newblock Learning to mine aligned code and natural language pairs from stack overflow.
\newblock In {\em Proceedings of the 15th IEEE/ACM International Conference on Mining Software Repositories (MSR)}, pages 476--486. IEEE, 2018.

\bibitem{zellers2018swag}
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi.
\newblock Swag: A large-scale adversarial dataset for grounded commonsense inference.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 93--104, Brussels, Belgium, 2018. Association for Computational Linguistics.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock {\em arXiv preprint arXiv:1905.07830}, 2019.

\bibitem{zhang2018record}
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van~Durme.
\newblock Record: Bridging the gap between human and machine commonsense reading comprehension.
\newblock {\em arXiv preprint arXiv:1810.12885}, 2018.

\end{thebibliography}
