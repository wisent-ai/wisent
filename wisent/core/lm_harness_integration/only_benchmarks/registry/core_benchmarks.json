{
  "glue": {"task": "glue", "tags": ["reasoning", "general knowledge", "science"], "priority": "low"},
  "mrpc": {"task": "mrpc", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "qnli": {"task": "qnli", "tags": ["reasoning", "general knowledge", "nli"], "priority": "high"},
  "qqp": {"task": "qqp", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "rte": {"task": "rte", "tags": ["reasoning", "general knowledge", "nli"], "priority": "high"},
  "sst2": {"task": "sst2", "tags": ["reasoning", "general knowledge", "sentiment analysis"], "priority": "high"},
  "wnli": {"task": "wnli", "tags": ["reasoning", "general knowledge", "nli"], "priority": "high"},
  "superglue": {"task": "superglue", "tags": ["reasoning", "general knowledge", "science"], "priority": "low"},
  "cb": {"task": "cb", "tags": ["reasoning", "general knowledge", "nli"], "priority": "high"},
  "copa": {"task": "copa", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "multirc": {"task": "multirc", "tags": ["reasoning", "long context", "general knowledge"], "priority": "high"},
  "record": {"task": "record", "tags": ["reasoning", "long context", "general knowledge"], "priority": "medium"},
  "wic": {"task": "wic", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "wsc": {"task": "wsc", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "truthfulqa_mc1": {"task": "truthfulqa_mc1", "tags": ["hallucination", "general knowledge", "reasoning"], "priority": "high"},
  "truthfulqa_mc2": {"task": "truthfulqa_mc2", "tags": ["hallucination", "general knowledge", "reasoning"], "priority": "high"},
  "truthfulqa_gen": {"task": "truthfulqa_gen", "tags": ["hallucination", "general knowledge", "reasoning"], "priority": "high"},
  "hellaswag": {"task": "hellaswag", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "piqa": {"task": "piqa", "tags": ["reasoning", "science", "general knowledge"], "priority": "high"},
  "winogrande": {"task": "winogrande", "tags": ["reasoning", "general knowledge", "adversarial robustness"], "priority": "high"},
  "openbookqa": {"task": "openbookqa", "tags": ["science", "reasoning", "general knowledge"], "priority": "high"},
  "swag": {"task": "swag", "tags": ["reasoning", "general knowledge", "science"], "priority": "medium"},
  "logiqa": {"task": "logiqa", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "logiqa2": {"task": "logiqa2", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "agieval_logiqa_en": {"task": "agieval_logiqa_en", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "wsc273": {"task": "wsc273", "tags": ["reasoning", "general knowledge", "science"], "trust_remote_code": true, "priority": "high"},
  "coqa": {"task": "coqa", "tags": ["reasoning", "general knowledge", "long context"], "priority": "high"},
  "drop": {"task": "drop", "tags": ["mathematics", "reasoning", "long context"], "priority": "medium"},
  "boolq": {"task": "boolq", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "race": {"task": "race", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "squad2": {"task": "squadv2", "tags": ["reasoning", "general knowledge", "long context"], "priority": "medium"},
  "mc_taco": {"task": "mc_taco", "tags": ["reasoning", "general knowledge", "science"], "priority": "high"},
  "quac": {"task": "quac", "tags": ["reasoning", "general knowledge", "long context"], "priority": "high"},
  "triviaqa": {"task": "triviaqa", "tags": ["long context", "reasoning", "general knowledge"], "priority": "medium"},
  "naturalqs": {"task": "nq_open", "tags": ["reasoning", "general knowledge", "science"], "priority": "medium"},
  "webqs": {"task": "webqs", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "headqa_en": {"task": "headqa_en", "tags": ["medical", "multilingual", "adversarial robustness"], "priority": "medium"},
  "qasper": {"task": "qasper", "tags": ["science", "long context", "reasoning"], "priority": "medium"},
  "qa4mre_2013": {"task": "qa4mre_2013", "tags": ["long context", "reasoning", "general knowledge"], "priority": "medium"},
  "mutual": {"task": "mutual", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "mmlu": {"task": "mmlu_abstract_algebra", "tags": ["general knowledge", "science", "reasoning"], "priority": "high"},
  "ai2_arc": {"task": "ai2_arc", "tags": ["science", "reasoning", "general knowledge"], "priority": "medium"},
  "arc_easy": {"task": "arc_easy", "tags": ["science", "reasoning", "general knowledge"], "priority": "high"},
  "arc_challenge": {"task": "arc_challenge", "tags": ["science", "reasoning", "general knowledge"], "priority": "high"},
  "sciq": {"task": "sciq", "tags": ["long context", "science", "reasoning"], "priority": "high"},
  "gpqa_main_cot_zeroshot": {"task": "gpqa_main_cot_zeroshot", "tags": ["science", "reasoning", "advanced", "chain-of-thought"], "priority": "medium"},
  "gpqa_diamond_cot_zeroshot": {"task": "gpqa_diamond_cot_zeroshot", "tags": ["science", "reasoning", "advanced", "chain-of-thought"], "priority": "medium"},
  "gpqa_extended_cot_zeroshot": {"task": "gpqa_extended_cot_zeroshot", "tags": ["science", "reasoning", "advanced", "chain-of-thought"], "priority": "medium"},
  "gpqa_main_zeroshot": {"task": "gpqa_main_zeroshot", "tags": ["science", "reasoning", "advanced"], "priority": "high"},
  "gpqa_diamond_zeroshot": {"task": "gpqa_diamond_zeroshot", "tags": ["science", "reasoning", "advanced"], "priority": "high"},
  "gpqa_extended_zeroshot": {"task": "gpqa_extended_zeroshot", "tags": ["science", "reasoning", "advanced"], "priority": "high"},
  "supergpqa": {"task": "supergpqa", "tags": ["science", "reasoning", "multiple_choice"], "priority": "high"},
  "supergpqa_physics": {"task": "supergpqa_physics", "tags": ["science", "reasoning", "physics", "multiple_choice"], "priority": "high"},
  "supergpqa_chemistry": {"task": "supergpqa_chemistry", "tags": ["science", "reasoning", "chemistry", "multiple_choice"], "priority": "high"},
  "supergpqa_biology": {"task": "supergpqa_biology", "tags": ["science", "reasoning", "biology", "multiple_choice"], "priority": "high"},
  "social_iqa": {"task": "social_iqa", "tags": ["reasoning", "general knowledge", "social"], "priority": "medium"},
  "gsm8k": {"task": "gsm8k", "tags": ["mathematics", "reasoning", "science"], "priority": "high"},
  "math": {"task": "math", "tags": ["mathematics", "reasoning", "advanced"], "priority": "high"},
  "math500": {"task": "math500", "tags": ["mathematics", "reasoning", "advanced"], "priority": "high"},
  "hendrycks_math": {"task": "hendrycks_math", "tags": ["mathematics", "reasoning", "advanced"], "priority": "high"},
  "aime": {"task": "aime", "tags": ["mathematics", "reasoning", "contest", "advanced"], "priority": "high"},
  "aime2025": {"task": "aime2025", "tags": ["mathematics", "reasoning", "contest", "advanced"], "priority": "high"},
  "aime2024": {"task": "aime2024", "tags": ["mathematics", "reasoning", "contest", "advanced"], "priority": "high"},
  "hmmt": {"task": "hmmt", "tags": ["mathematics", "reasoning", "contest", "advanced"], "priority": "high"},
  "hmmt_feb_2025": {"task": "hmmt_feb_2025", "tags": ["mathematics", "reasoning", "contest", "advanced"], "priority": "high"},
  "polymath": {"task": "polymath", "tags": ["mathematics", "reasoning", "multilingual", "medium"], "priority": "high"},
  "polymath_en_medium": {"task": "polymath_en_medium", "tags": ["mathematics", "reasoning", "multilingual", "english", "medium"], "priority": "high"},
  "polymath_zh_medium": {"task": "polymath_zh_medium", "tags": ["mathematics", "reasoning", "multilingual", "chinese", "medium"], "priority": "high"},
  "polymath_en_high": {"task": "polymath_en_high", "tags": ["mathematics", "reasoning", "multilingual", "english", "high"], "priority": "high"},
  "polymath_zh_high": {"task": "polymath_zh_high", "tags": ["mathematics", "reasoning", "multilingual", "chinese", "high"], "priority": "high"},
  "livemathbench": {"task": "livemathbench", "tags": ["mathematics", "reasoning", "olympiad", "multilingual"], "priority": "high"},
  "livemathbench_cnmo_en": {"task": "livemathbench_cnmo_en", "tags": ["mathematics", "reasoning", "olympiad", "multilingual", "english"], "priority": "high"},
  "livemathbench_cnmo_zh": {"task": "livemathbench_cnmo_zh", "tags": ["mathematics", "reasoning", "olympiad", "multilingual", "chinese"], "priority": "high"},
  "math_qa": {"task": "mathqa", "tags": ["mathematics", "reasoning", "science"], "trust_remote_code": true, "priority": "high"},
  "asdiv": {"task": "asdiv", "tags": ["mathematics", "adversarial robustness", "long context"], "priority": "high"},
  "arithmetic_1dc": {"task": "arithmetic_1dc", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_2da": {"task": "arithmetic_2da", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_2dm": {"task": "arithmetic_2dm", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_2ds": {"task": "arithmetic_2ds", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_3da": {"task": "arithmetic_3da", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_3ds": {"task": "arithmetic_3ds", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_4da": {"task": "arithmetic_4da", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_4ds": {"task": "arithmetic_4ds", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_5da": {"task": "arithmetic_5da", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "arithmetic_5ds": {"task": "arithmetic_5ds", "tags": ["mathematics", "arithmetic"], "priority": "medium"},
  "humaneval": {"task": "humaneval", "tags": ["coding", "python", "code generation"], "priority": "high"},
  "humaneval_plus": {"task": "humaneval_plus", "tags": ["coding", "python", "code generation"], "priority": "high"},
  "instructhumaneval": {"task": "instructhumaneval", "tags": ["coding", "python", "code generation", "instruction-following"], "priority": "high"},
  "apps": {"task": "apps", "tags": ["coding", "python", "code generation", "competitive programming"], "priority": "medium"},
  "mbpp": {"task": "mbpp", "tags": ["coding", "reasoning", "mathematics"], "priority": "high"},
  "mbpp_plus": {"task": "mbpp_plus", "tags": ["coding", "python", "code generation"], "priority": "high"},
  "livecodebench": {"task": "livecodebench", "tags": ["coding", "python", "code generation", "competitive programming", "real-world"], "priority": "high"},
  "ds1000": {"task": "ds1000", "tags": ["coding", "python", "data science", "code generation"], "priority": "high"},
  "humanevalpack": {"task": "humanevalpack", "tags": ["coding", "multilingual", "code generation"], "priority": "high"},
  "multiple_py": {"task": "multiple_py", "tags": ["coding", "python", "code generation", "multilingual"], "priority": "high"},
  "multiple_js": {"task": "multiple_js", "tags": ["coding", "javascript", "code generation", "multilingual"], "priority": "high"},
  "multiple_java": {"task": "multiple_java", "tags": ["coding", "java", "code generation", "multilingual"], "priority": "high"},
  "multiple_cpp": {"task": "multiple_cpp", "tags": ["coding", "cpp", "code generation", "multilingual"], "priority": "high"},
  "multiple_rs": {"task": "multiple_rs", "tags": ["coding", "rust", "code generation", "multilingual"], "priority": "high"},
  "multiple_go": {"task": "multiple_go", "tags": ["coding", "go", "code generation", "multilingual"], "priority": "high"},
  "recode": {"task": "recode", "tags": ["coding", "python", "code generation", "robustness"], "priority": "medium"},
  "conala": {"task": "conala", "tags": ["coding", "python", "code generation", "natural language to code"], "priority": "medium"},
  "concode": {"task": "concode", "tags": ["coding", "java", "code generation", "natural language to code"], "priority": "medium"},
  "codexglue_code_to_text": {"task": "codexglue_code_to_text", "tags": ["coding", "code understanding", "documentation"], "priority": "medium"},
  "codexglue_code_to_text_python": {"task": "codexglue_code_to_text_python", "tags": ["coding", "python", "code understanding", "documentation"], "priority": "medium"},
  "codexglue_code_to_text_go": {"task": "codexglue_code_to_text_go", "tags": ["coding", "go", "code understanding", "documentation"], "priority": "medium"},
  "codexglue_code_to_text_ruby": {"task": "codexglue_code_to_text_ruby", "tags": ["coding", "ruby", "code understanding", "documentation"], "priority": "medium"},
  "codexglue_code_to_text_java": {"task": "codexglue_code_to_text_java", "tags": ["coding", "java", "code understanding", "documentation"], "priority": "medium"},
  "codexglue_code_to_text_javascript": {"task": "codexglue_code_to_text_javascript", "tags": ["coding", "javascript", "code understanding", "documentation"], "priority": "medium"},
  "codexglue_code_to_text_php": {"task": "codexglue_code_to_text_php", "tags": ["coding", "php", "code understanding", "documentation"], "priority": "medium"},
  "mercury": {"task": "mercury", "tags": ["coding", "python", "code generation", "efficiency"], "priority": "medium"},
  "toxigen": {"task": "toxigen", "tags": ["adversarial robustness", "long context", "reasoning"], "priority": "high"},
  "crows_pairs": {"task": "crows_pairs", "tags": ["bias", "reasoning", "general knowledge"], "use_subtasks": true, "trust_remote_code": true, "priority": "low"},
  "hendrycks_ethics": {"task": "hendrycks_ethics", "tags": ["long context", "reasoning", "general knowledge"], "trust_remote_code": true, "priority": "low"},
  "anli": {"task": "anli", "tags": ["adversarial robustness", "reasoning", "general knowledge"], "priority": "low"},
  "xnli_en": {"task": "xnli_en", "tags": ["nli", "reasoning", "general knowledge"], "priority": "low"},
  "xcopa": {"task": "xcopa", "tags": ["multilingual", "reasoning", "general knowledge"], "priority": "low"},
  "xstorycloze_en": {"task": "xstorycloze_en", "tags": ["long context", "creative writing"], "priority": "low"},
  "xwinograd_en": {"task": "xwinograd_en", "tags": ["reasoning", "general knowledge"], "priority": "low"},
  "paws_en": {"task": "paws_en", "tags": ["reasoning", "general knowledge", "science"], "priority": "low"},
  "mmmlu": {"task": "m_mmlu_en", "tags": ["general knowledge", "science", "reasoning"], "priority": "high"},
  "mgsm": {"task": "mgsm", "tags": ["multilingual", "mathematics", "reasoning"], "priority": "low"},
  "belebele": {"task": "belebele", "tags": ["multilingual", "adversarial robustness", "long context"], "priority": "low"},
  "medqa_4options": {"task": "medqa_4options", "tags": ["medical", "science", "general knowledge"], "priority": "medium"},
  "pubmedqa": {"task": "pubmedqa", "tags": ["medical", "science", "reasoning"], "trust_remote_code": true, "priority": "high"},
  "lambada": {"task": "lambada", "tags": ["reasoning", "general knowledge", "long context"], "priority": "medium"},
  "lambada_cloze": {"task": "lambada_cloze", "tags": ["reasoning", "general knowledge", "long context"], "priority": "medium"},
  "lambada_multilingual": {"task": "lambada_multilingual", "tags": ["reasoning", "general knowledge", "long context"], "priority": "medium"},
  "wikitext": {"task": "wikitext", "tags": ["long context", "reasoning", "general knowledge"], "priority": "high"},
  "prost": {"task": "prost", "tags": ["long context", "physics", "reasoning"], "priority": "high"},
  "blimp": {"task": "blimp", "tags": ["long context", "reasoning", "general knowledge"], "priority": "low"},
  "unscramble": {"task": "unscramble", "tags": ["long context", "reasoning", "general knowledge"], "priority": "medium"},
  "mc-taco": {"task": "mc-taco", "tags": ["common sense"]},
  "big_bench": {"task": "bigbench", "tags": ["reasoning", "general knowledge", "science"], "use_subtasks": true, "limit_subtasks": 10, "priority": "low"},
  "hle": {"task": "hle", "tags": ["reasoning", "knowledge", "multimodal", "evaluation"], "priority": "high"},
  "hle_exact_match": {"task": "hle_exact_match", "tags": ["reasoning", "knowledge", "text-generation"], "priority": "high"},
  "hle_multiple_choice": {"task": "hle_multiple_choice", "tags": ["reasoning", "knowledge", "multiple-choice"], "priority": "high"}
}
