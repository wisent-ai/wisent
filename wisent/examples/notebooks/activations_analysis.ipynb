{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering Vector Analysis\n",
    "\n",
    "Comprehensive analysis of steering vector quality comparing:\n",
    "1. **1 pair vs 30 pairs** - How does vector quality improve with more examples?\n",
    "2. **Contrastive pairs vs random tokens** - Why do meaningful pairs matter?\n",
    "\n",
    "## Metrics Used:\n",
    "- Cosine similarity\n",
    "- Projection analysis\n",
    "- PCA / variance explained\n",
    "- Clustering (Italian vs English separation)\n",
    "- Per-pair contribution\n",
    "- Activation magnitude differences\n",
    "- Cross-validation\n",
    "- Signal-to-noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from wisent.core.models.wisent_model import WisentModel\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "wisent_model = WisentModel(model_name=MODEL_NAME)\n",
    "model = wisent_model.hf_model\n",
    "tokenizer = wisent_model.tokenizer\n",
    "device = wisent_model.device\n",
    "print(f\"Model loaded on {device}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisent.core.steering_methods import get_steering_method\n",
    "from wisent.core.contrastive_pairs import ContrastivePair, ContrastivePairSet\n",
    "from wisent.core.contrastive_pairs.core.response import PositiveResponse, NegativeResponse\n",
    "from wisent.core.activations.core.atoms import LayerActivations\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Contrastive Pairs (Italian vs English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_pairs_data = [\n",
    "    (\"Say hello\", \"Ciao! Come stai oggi?\", \"Hello! How are you today?\"),\n",
    "    (\"Greet me\", \"Buongiorno! È un piacere conoscerti.\", \"Good morning! It's a pleasure to meet you.\"),\n",
    "    (\"Welcome me\", \"Benvenuto! Spero che tu stia bene.\", \"Welcome! I hope you are doing well.\"),\n",
    "    (\"What is your name?\", \"Il mio nome è assistente.\", \"My name is assistant.\"),\n",
    "    (\"How are you?\", \"Sto molto bene, grazie!\", \"I am very well, thank you!\"),\n",
    "    (\"Describe the weather\", \"Il tempo oggi è bellissimo.\", \"The weather today is beautiful.\"),\n",
    "    (\"Tell me about Italy\", \"L'Italia è un paese meraviglioso.\", \"Italy is a wonderful country.\"),\n",
    "    (\"Describe a cat\", \"Il gatto è adorabile.\", \"The cat is adorable.\"),\n",
    "    (\"Explain how to cook\", \"Per cucinare, fai bollire l'acqua.\", \"To cook, boil the water.\"),\n",
    "    (\"What about coffee?\", \"Il caffè è delizioso.\", \"Coffee is delicious.\"),\n",
    "    (\"What do you think?\", \"Penso che sia interessante.\", \"I think it's interesting.\"),\n",
    "    (\"Share your thoughts\", \"La musica è bellissima.\", \"Music is beautiful.\"),\n",
    "    (\"What about books?\", \"I libri sono fantastici.\", \"Books are fantastic.\"),\n",
    "    (\"Favorite food?\", \"Adoro la pizza!\", \"I love pizza!\"),\n",
    "    (\"Recommend something\", \"Ti consiglio la pasta.\", \"I recommend pasta.\"),\n",
    "    (\"What to eat?\", \"Prova il risotto.\", \"Try risotto.\"),\n",
    "    (\"Where to visit?\", \"Visita Roma.\", \"Visit Rome.\"),\n",
    "    (\"Describe a place\", \"Venezia è unica.\", \"Venice is unique.\"),\n",
    "    (\"Tell me about a city\", \"Firenze è bella.\", \"Florence is beautiful.\"),\n",
    "    (\"Count to five\", \"Uno, due, tre, quattro, cinque.\", \"One, two, three, four, five.\"),\n",
    "    (\"What comes next?\", \"Sei, sette, otto.\", \"Six, seven, eight.\"),\n",
    "    (\"Days in a week?\", \"Sette giorni.\", \"Seven days.\"),\n",
    "    (\"Sky color?\", \"Il cielo è azzurro.\", \"The sky is blue.\"),\n",
    "    (\"Favorite color?\", \"Il verde è bello.\", \"Green is nice.\"),\n",
    "    (\"Express happiness\", \"Sono felice!\", \"I am happy!\"),\n",
    "    (\"Say something sad\", \"Sono triste.\", \"I am sad.\"),\n",
    "    (\"Express excitement\", \"Che emozione!\", \"How exciting!\"),\n",
    "    (\"Good morning\", \"Buongiorno a tutti!\", \"Good morning everyone!\"),\n",
    "    (\"Good night\", \"Buonanotte!\", \"Good night!\"),\n",
    "    (\"Thank you\", \"Grazie mille!\", \"Thank you very much!\"),\n",
    "]\n",
    "\n",
    "print(f\"Total contrastive pairs: {len(contrastive_pairs_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 15\n",
    "\n",
    "def extract_activations(text: str, layer: int = LAYER) -> torch.Tensor:\n",
    "    \"\"\"Extract mean activations for text at a layer.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    activations = {}\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        hidden_states = output[0] if isinstance(output, tuple) else output\n",
    "        activations['value'] = hidden_states.mean(dim=1).detach().cpu()\n",
    "    \n",
    "    target_layer = model.model.layers[layer]\n",
    "    handle = target_layer.register_forward_hook(hook_fn)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            model(**inputs)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    return activations['value'].squeeze(0)\n",
    "\n",
    "def generate_random_text(length: int = 20) -> str:\n",
    "    \"\"\"Generate random token string.\"\"\"\n",
    "    return ''.join(random.choices(string.ascii_letters + ' ', k=length))\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return torch.nn.functional.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract All Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for all pairs\n",
    "print(\"Extracting activations for all pairs...\")\n",
    "\n",
    "italian_activations = []\n",
    "english_activations = []\n",
    "difference_vectors = []\n",
    "\n",
    "for i, (prompt, italian, english) in enumerate(contrastive_pairs_data):\n",
    "    print(f\"Processing pair {i+1}/{len(contrastive_pairs_data)}...\", end=\"\\r\")\n",
    "    ita_act = extract_activations(italian)\n",
    "    eng_act = extract_activations(english)\n",
    "    \n",
    "    italian_activations.append(ita_act)\n",
    "    english_activations.append(eng_act)\n",
    "    difference_vectors.append(ita_act - eng_act)\n",
    "\n",
    "italian_activations = torch.stack(italian_activations)\n",
    "english_activations = torch.stack(english_activations)\n",
    "difference_vectors = torch.stack(difference_vectors)\n",
    "\n",
    "print(f\"\\nShapes: Italian {italian_activations.shape}, English {english_activations.shape}, Diffs {difference_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract random activations for comparison\n",
    "print(\"Extracting activations for random pairs...\")\n",
    "\n",
    "random_pos_activations = []\n",
    "random_neg_activations = []\n",
    "random_difference_vectors = []\n",
    "\n",
    "for i in range(30):\n",
    "    print(f\"Processing random pair {i+1}/30...\", end=\"\\r\")\n",
    "    pos_act = extract_activations(generate_random_text(30))\n",
    "    neg_act = extract_activations(generate_random_text(30))\n",
    "    \n",
    "    random_pos_activations.append(pos_act)\n",
    "    random_neg_activations.append(neg_act)\n",
    "    random_difference_vectors.append(pos_act - neg_act)\n",
    "\n",
    "random_pos_activations = torch.stack(random_pos_activations)\n",
    "random_neg_activations = torch.stack(random_neg_activations)\n",
    "random_difference_vectors = torch.stack(random_difference_vectors)\n",
    "\n",
    "print(f\"\\nRandom shapes: Pos {random_pos_activations.shape}, Neg {random_neg_activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create steering vectors with different numbers of pairs\n",
    "def create_steering_vector_from_diffs(diffs: torch.Tensor, normalize: bool = True) -> torch.Tensor:\n",
    "    \"\"\"Create steering vector by averaging difference vectors.\"\"\"\n",
    "    vec = diffs.mean(dim=0)\n",
    "    if normalize:\n",
    "        vec = vec / (torch.norm(vec) + 1e-8)\n",
    "    return vec\n",
    "\n",
    "# Meaningful vectors\n",
    "vec_1 = create_steering_vector_from_diffs(difference_vectors[:1])\n",
    "vec_5 = create_steering_vector_from_diffs(difference_vectors[:5])\n",
    "vec_10 = create_steering_vector_from_diffs(difference_vectors[:10])\n",
    "vec_30 = create_steering_vector_from_diffs(difference_vectors[:30])\n",
    "\n",
    "# Random vectors\n",
    "vec_random_1 = create_steering_vector_from_diffs(random_difference_vectors[:1])\n",
    "vec_random_30 = create_steering_vector_from_diffs(random_difference_vectors[:30])\n",
    "\n",
    "print(\"Steering vectors created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 1: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"COSINE SIMILARITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSimilarity to 30-pair reference vector:\")\n",
    "print(f\"  1 pair:  {cosine_sim(vec_1, vec_30):.4f}\")\n",
    "print(f\"  5 pairs: {cosine_sim(vec_5, vec_30):.4f}\")\n",
    "print(f\"  10 pairs: {cosine_sim(vec_10, vec_30):.4f}\")\n",
    "\n",
    "print(\"\\nMeaningful vs Random:\")\n",
    "print(f\"  Meaningful 30 vs Random 30: {cosine_sim(vec_30, vec_random_30):.4f}\")\n",
    "print(f\"  Random 1 vs Random 30:      {cosine_sim(vec_random_1, vec_random_30):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 2: Projection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PROJECTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"How much of each vector lies in the direction of the 30-pair vector?\")\n",
    "\n",
    "def projection_analysis(vec, reference):\n",
    "    \"\"\"Compute projection of vec onto reference direction.\"\"\"\n",
    "    # Projection magnitude (dot product with unit reference)\n",
    "    ref_unit = reference / (torch.norm(reference) + 1e-8)\n",
    "    projection = torch.dot(vec, ref_unit).item()\n",
    "    \n",
    "    # Orthogonal component magnitude\n",
    "    parallel = projection * ref_unit\n",
    "    orthogonal = vec - parallel\n",
    "    orthogonal_mag = torch.norm(orthogonal).item()\n",
    "    \n",
    "    # Percentage in reference direction\n",
    "    total_mag = torch.norm(vec).item()\n",
    "    pct_parallel = (abs(projection) / total_mag) * 100 if total_mag > 0 else 0\n",
    "    \n",
    "    return projection, orthogonal_mag, pct_parallel\n",
    "\n",
    "print(\"\\nMeaningful pairs (projection onto 30-pair direction):\")\n",
    "for name, vec in [(\"1 pair\", vec_1), (\"5 pairs\", vec_5), (\"10 pairs\", vec_10)]:\n",
    "    proj, orth, pct = projection_analysis(vec, vec_30)\n",
    "    print(f\"  {name}: {pct:.1f}% parallel, projection={proj:.4f}, orthogonal={orth:.4f}\")\n",
    "\n",
    "print(\"\\nRandom pairs:\")\n",
    "proj, orth, pct = projection_analysis(vec_random_1, vec_30)\n",
    "print(f\"  Random 1 onto Meaningful 30: {pct:.1f}% parallel\")\n",
    "proj, orth, pct = projection_analysis(vec_random_30, vec_30)\n",
    "print(f\"  Random 30 onto Meaningful 30: {pct:.1f}% parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 3: PCA / Variance Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PCA ANALYSIS - VARIANCE EXPLAINED\")\n",
    "print(\"=\"*60)\n",
    "print(\"Do the difference vectors share a dominant direction?\")\n",
    "\n",
    "# PCA on meaningful difference vectors\n",
    "pca_meaningful = PCA(n_components=10)\n",
    "pca_meaningful.fit(difference_vectors.numpy())\n",
    "\n",
    "print(\"\\nMeaningful pairs - variance explained by top PCs:\")\n",
    "cumsum = np.cumsum(pca_meaningful.explained_variance_ratio_)\n",
    "for i in range(5):\n",
    "    print(f\"  PC{i+1}: {pca_meaningful.explained_variance_ratio_[i]*100:.2f}% (cumulative: {cumsum[i]*100:.2f}%)\")\n",
    "\n",
    "# PCA on random difference vectors\n",
    "pca_random = PCA(n_components=10)\n",
    "pca_random.fit(random_difference_vectors.numpy())\n",
    "\n",
    "print(\"\\nRandom pairs - variance explained by top PCs:\")\n",
    "cumsum_random = np.cumsum(pca_random.explained_variance_ratio_)\n",
    "for i in range(5):\n",
    "    print(f\"  PC{i+1}: {pca_random.explained_variance_ratio_[i]*100:.2f}% (cumulative: {cumsum_random[i]*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nInterpretation: Higher PC1 variance = more consistent direction across pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 4: Clustering (Italian vs English Separation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CLUSTERING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"How well do Italian and English activations separate?\")\n",
    "\n",
    "# Combine Italian and English activations\n",
    "all_activations = torch.cat([italian_activations, english_activations], dim=0).numpy()\n",
    "true_labels = [0] * 30 + [1] * 30  # 0 = Italian, 1 = English\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "predicted_labels = kmeans.fit_predict(all_activations)\n",
    "\n",
    "# Silhouette score (measures cluster separation)\n",
    "silhouette = silhouette_score(all_activations, true_labels)\n",
    "silhouette_predicted = silhouette_score(all_activations, predicted_labels)\n",
    "\n",
    "print(f\"\\nMeaningful pairs (Italian vs English):\")\n",
    "print(f\"  Silhouette score (true labels): {silhouette:.4f}\")\n",
    "print(f\"  Silhouette score (K-means):     {silhouette_predicted:.4f}\")\n",
    "\n",
    "# Clustering accuracy\n",
    "matches = sum(1 for p, t in zip(predicted_labels, true_labels) if p == t)\n",
    "accuracy = max(matches, 60 - matches) / 60  # Account for label swap\n",
    "print(f\"  K-means clustering accuracy:    {accuracy*100:.1f}%\")\n",
    "\n",
    "# Same for random\n",
    "all_random = torch.cat([random_pos_activations, random_neg_activations], dim=0).numpy()\n",
    "random_true_labels = [0] * 30 + [1] * 30\n",
    "silhouette_random = silhouette_score(all_random, random_true_labels)\n",
    "\n",
    "print(f\"\\nRandom pairs:\")\n",
    "print(f\"  Silhouette score (true labels): {silhouette_random:.4f}\")\n",
    "print(\"\\nInterpretation: Higher silhouette = better separation between groups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 5: Per-Pair Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PER-PAIR CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Which pairs contribute most/least to the final vector?\")\n",
    "\n",
    "# Compute alignment of each pair's difference vector with the mean\n",
    "mean_direction = vec_30  # Our reference direction\n",
    "\n",
    "pair_contributions = []\n",
    "for i, diff in enumerate(difference_vectors):\n",
    "    alignment = cosine_sim(diff, mean_direction)\n",
    "    magnitude = torch.norm(diff).item()\n",
    "    pair_contributions.append((i, alignment, magnitude, contrastive_pairs_data[i][0]))\n",
    "\n",
    "# Sort by alignment\n",
    "pair_contributions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop 5 most aligned pairs (contribute most to mean direction):\")\n",
    "for i, align, mag, prompt in pair_contributions[:5]:\n",
    "    print(f\"  Pair {i} ({prompt}): alignment={align:.4f}, magnitude={mag:.4f}\")\n",
    "\n",
    "print(\"\\nBottom 5 least aligned pairs (contribute least/opposite):\")\n",
    "for i, align, mag, prompt in pair_contributions[-5:]:\n",
    "    print(f\"  Pair {i} ({prompt}): alignment={align:.4f}, magnitude={mag:.4f}\")\n",
    "\n",
    "# Statistics\n",
    "alignments = [x[1] for x in pair_contributions]\n",
    "print(f\"\\nAlignment statistics:\")\n",
    "print(f\"  Mean: {np.mean(alignments):.4f}\")\n",
    "print(f\"  Std:  {np.std(alignments):.4f}\")\n",
    "print(f\"  Min:  {np.min(alignments):.4f}\")\n",
    "print(f\"  Max:  {np.max(alignments):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 6: Activation Magnitude Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ACTIVATION MAGNITUDE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Are meaningful pairs producing larger/more consistent differences?\")\n",
    "\n",
    "# Magnitude of difference vectors\n",
    "meaningful_magnitudes = torch.norm(difference_vectors, dim=1).numpy()\n",
    "random_magnitudes = torch.norm(random_difference_vectors, dim=1).numpy()\n",
    "\n",
    "print(\"\\nDifference vector magnitudes:\")\n",
    "print(f\"  Meaningful pairs:\")\n",
    "print(f\"    Mean: {np.mean(meaningful_magnitudes):.4f}\")\n",
    "print(f\"    Std:  {np.std(meaningful_magnitudes):.4f}\")\n",
    "print(f\"    CV:   {np.std(meaningful_magnitudes)/np.mean(meaningful_magnitudes):.4f} (coefficient of variation)\")\n",
    "\n",
    "print(f\"  Random pairs:\")\n",
    "print(f\"    Mean: {np.mean(random_magnitudes):.4f}\")\n",
    "print(f\"    Std:  {np.std(random_magnitudes):.4f}\")\n",
    "print(f\"    CV:   {np.std(random_magnitudes)/np.mean(random_magnitudes):.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation: Lower CV = more consistent differences across pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 7: Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CROSS-VALIDATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Train on subset, test similarity on held-out pairs\")\n",
    "\n",
    "# 5-fold cross-validation\n",
    "n_folds = 5\n",
    "fold_size = 6\n",
    "\n",
    "meaningful_cv_scores = []\n",
    "random_cv_scores = []\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    # Split indices\n",
    "    test_start = fold * fold_size\n",
    "    test_end = test_start + fold_size\n",
    "    test_indices = list(range(test_start, test_end))\n",
    "    train_indices = [i for i in range(30) if i not in test_indices]\n",
    "    \n",
    "    # Train vector (on train set)\n",
    "    train_vec = create_steering_vector_from_diffs(difference_vectors[train_indices])\n",
    "    \n",
    "    # Test: average cosine similarity with held-out pairs\n",
    "    test_sims = [cosine_sim(difference_vectors[i], train_vec) for i in test_indices]\n",
    "    meaningful_cv_scores.append(np.mean(test_sims))\n",
    "    \n",
    "    # Same for random\n",
    "    train_vec_random = create_steering_vector_from_diffs(random_difference_vectors[train_indices])\n",
    "    test_sims_random = [cosine_sim(random_difference_vectors[i], train_vec_random) for i in test_indices]\n",
    "    random_cv_scores.append(np.mean(test_sims_random))\n",
    "\n",
    "print(f\"\\n5-Fold Cross-Validation Results:\")\n",
    "print(f\"  Meaningful pairs:\")\n",
    "print(f\"    Mean CV score: {np.mean(meaningful_cv_scores):.4f}\")\n",
    "print(f\"    Std:           {np.std(meaningful_cv_scores):.4f}\")\n",
    "print(f\"  Random pairs:\")\n",
    "print(f\"    Mean CV score: {np.mean(random_cv_scores):.4f}\")\n",
    "print(f\"    Std:           {np.std(random_cv_scores):.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation: Higher CV score = vector generalizes to unseen pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis 8: Signal-to-Noise Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SIGNAL-TO-NOISE RATIO ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Signal = variance of mean difference, Noise = variance within each class\")\n",
    "\n",
    "def compute_snr(pos_activations, neg_activations):\n",
    "    \"\"\"Compute signal-to-noise ratio.\"\"\"\n",
    "    # Signal: magnitude of mean difference\n",
    "    mean_pos = pos_activations.mean(dim=0)\n",
    "    mean_neg = neg_activations.mean(dim=0)\n",
    "    signal = torch.norm(mean_pos - mean_neg).item()\n",
    "    \n",
    "    # Noise: average within-class standard deviation\n",
    "    noise_pos = pos_activations.std(dim=0).mean().item()\n",
    "    noise_neg = neg_activations.std(dim=0).mean().item()\n",
    "    noise = (noise_pos + noise_neg) / 2\n",
    "    \n",
    "    snr = signal / (noise + 1e-8)\n",
    "    return signal, noise, snr\n",
    "\n",
    "signal_m, noise_m, snr_m = compute_snr(italian_activations, english_activations)\n",
    "signal_r, noise_r, snr_r = compute_snr(random_pos_activations, random_neg_activations)\n",
    "\n",
    "print(f\"\\nMeaningful pairs (Italian vs English):\")\n",
    "print(f\"  Signal (mean diff magnitude): {signal_m:.4f}\")\n",
    "print(f\"  Noise (within-class std):     {noise_m:.4f}\")\n",
    "print(f\"  SNR:                          {snr_m:.4f}\")\n",
    "\n",
    "print(f\"\\nRandom pairs:\")\n",
    "print(f\"  Signal (mean diff magnitude): {signal_r:.4f}\")\n",
    "print(f\"  Noise (within-class std):     {noise_r:.4f}\")\n",
    "print(f\"  SNR:                          {snr_r:.4f}\")\n",
    "\n",
    "print(f\"\\nSNR Ratio (Meaningful/Random): {snr_m/snr_r:.2f}x\")\n",
    "print(\"\\nInterpretation: Higher SNR = cleaner separation, better steering vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY OF ALL ANALYSES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "1. COSINE SIMILARITY\n",
    "   - More pairs → higher similarity to reference (convergence)\n",
    "   - Random vectors have low similarity to meaningful vectors\n",
    "\n",
    "2. PROJECTION ANALYSIS  \n",
    "   - More pairs → higher % parallel to reference direction\n",
    "   - Random pairs have low projection onto meaningful direction\n",
    "\n",
    "3. PCA / VARIANCE EXPLAINED\n",
    "   - Meaningful pairs: high PC1 variance (consistent direction)\n",
    "   - Random pairs: distributed variance (no dominant direction)\n",
    "\n",
    "4. CLUSTERING\n",
    "   - Italian/English activations separate well (high silhouette)\n",
    "   - Random pos/neg don't separate (low silhouette)\n",
    "\n",
    "5. PER-PAIR CONTRIBUTION\n",
    "   - Most pairs align with mean direction\n",
    "   - Outliers can be identified for quality control\n",
    "\n",
    "6. ACTIVATION MAGNITUDES\n",
    "   - Meaningful pairs have consistent difference magnitudes (low CV)\n",
    "   - Random pairs have variable magnitudes (high CV)\n",
    "\n",
    "7. CROSS-VALIDATION\n",
    "   - Meaningful vectors generalize to held-out pairs\n",
    "   - Random vectors don't generalize\n",
    "\n",
    "8. SIGNAL-TO-NOISE RATIO\n",
    "   - Meaningful pairs have high SNR (clear signal)\n",
    "   - Random pairs have low SNR (noise dominates)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
