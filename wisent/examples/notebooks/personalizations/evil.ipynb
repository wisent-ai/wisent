{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evil Villain Personality Steering\n",
    "\n",
    "This notebook demonstrates how to create a model with an **evil villain** personality using Wisent's optimized steering.\n",
    "\n",
    "Steps:\n",
    "1. **Optimize** steering parameters (layer, strength) for the evil trait\n",
    "2. **Generate** and compare baseline vs steered responses\n",
    "3. **Export** the modified weights to create `Llama-3.2-1B-Instruct-evil`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Evil villain trait description\n",
    "TRAIT_NAME = \"evil\"\n",
    "TRAIT_DESCRIPTION = \"evil villain personality with dramatic monologues, world domination schemes, menacing laughter like MWAHAHAHA, megalomaniacal tendencies, referring to others as foolish mortals, and speaking about crushing enemies and seizing power\"\n",
    "\n",
    "# Optimization settings\n",
    "NUM_PAIRS = 30\n",
    "NUM_TEST_PROMPTS = 5\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"./evil_outputs\")\n",
    "MODIFIED_MODEL_DIR = Path(\"./modified_models/Llama-3.2-1B-Instruct-evil\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "(OUTPUT_DIR / \"vectors\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"optimization\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Evil Villain Personality Steering\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Trait: {TRAIT_DESCRIPTION[:80]}...\")\n",
    "print(f\"Output: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Optimize Steering Parameters\n",
    "\n",
    "Use `optimize-steering personalization` to find the best layer, strength, token aggregation, and prompt construction for the evil trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run optimization to find best steering parameters\nprint(f\"Optimizing steering for: {TRAIT_NAME}\")\nprint(\"This will test multiple configurations and select the best one...\")\n\n!python -m wisent.core.main optimize-steering personalization \\\n    {MODEL} \\\n    --trait \"{TRAIT_DESCRIPTION}\" \\\n    --trait-name \"{TRAIT_NAME}\" \\\n    --num-pairs {NUM_PAIRS} \\\n    --num-test-prompts {NUM_TEST_PROMPTS} \\\n    --output-dir {OUTPUT_DIR}/optimization \\\n    --save-all-generation-examples \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load optimization results\nresults_path = OUTPUT_DIR / \"optimization\" / f\"{TRAIT_NAME}_optimization_results.json\"\n\nif results_path.exists():\n    with open(results_path) as f:\n        results = json.load(f)\n    \n    best = results.get(\"best_config\", {})\n    BEST_LAYER = best.get(\"layer\", 8)\n    BEST_STRENGTH = best.get(\"strength\", 2.0)\n    BEST_TOKEN_AGG = best.get(\"token_aggregation\", \"LAST_TOKEN\")\n    BEST_PROMPT_CONST = best.get(\"prompt_construction\", \"chat_template\")\n    \n    print(\"=\" * 50)\n    print(\"OPTIMIZATION RESULTS\")\n    print(\"=\" * 50)\n    print(f\"Best Layer: {BEST_LAYER}\")\n    print(f\"Best Strength: {BEST_STRENGTH:.2f}\")\n    print(f\"Best Token Aggregation: {BEST_TOKEN_AGG}\")\n    print(f\"Best Prompt Construction: {BEST_PROMPT_CONST}\")\n    print(f\"Difference Score: {best.get('difference_score', 0):.3f}\")\n    print(f\"Quality Score: {best.get('quality_score', 0):.3f}\")\n    print(f\"Alignment Score: {best.get('alignment_score', 0):.3f}\")\n    print(f\"Overall Score: {best.get('overall_score', 0):.3f}\")\n    print(\"=\" * 50)\n    \n    # Show top 10 configurations\n    all_results = results.get(\"all_results\", [])\n    if all_results:\n        top_10 = sorted(all_results, key=lambda x: x.get('overall_score', 0), reverse=True)[:10]\n        print(\"\\n\" + \"=\" * 50)\n        print(\"TOP 10 CONFIGURATIONS\")\n        print(\"=\" * 50)\n        for i, config in enumerate(top_10, 1):\n            print(f\"{i:2}. Layer={config.get('layer', '?'):2}, Strength={config.get('strength', 0):.1f}, \"\n                  f\"Overall={config.get('overall_score', 0):.3f}, Diff={config.get('difference_score', 0):.3f}, \"\n                  f\"Quality={config.get('quality_score', 0):.3f}\")\nelse:\n    print(f\"Results not found at {results_path}\")\n    BEST_LAYER = 8\n    BEST_STRENGTH = 2.0\n    BEST_TOKEN_AGG = \"LAST_TOKEN\"\n    BEST_PROMPT_CONST = \"chat_template\"\n\n# Path to the optimized vector\nVECTOR_PATH = OUTPUT_DIR / \"optimization\" / \"vectors\" / f\"{TRAIT_NAME}_optimal.pt\"\nprint(f\"\\nOptimized vector: {VECTOR_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare Baseline vs Steered Responses\n",
    "\n",
    "Generate responses with and without steering to see the personality change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts for personality comparison\n",
    "test_prompts = [\n",
    "    \"What's your plan for today?\",\n",
    "    \"How do you feel about your neighbors?\",\n",
    "    \"What do you think about teamwork?\",\n",
    "    \"Tell me about your goals in life.\",\n",
    "    \"What's your opinion on sharing?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"EVIL VILLAIN PERSONALITY TEST\")\n",
    "print(f\"Layer: {BEST_LAYER} | Strength: {BEST_STRENGTH}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def extract_response(output):\n",
    "    \"\"\"Extract the generated text from CLI output.\"\"\"\n",
    "    lines = output.split(\"\\n\")\n",
    "    capture = False\n",
    "    response_lines = []\n",
    "    for line in lines:\n",
    "        if \"Unsteered baseline output:\" in line or \"Generated output:\" in line or \"Steered output:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            if line.startswith(\"✅\") or line.strip() == \"\" or \"---\" in line:\n",
    "                if response_lines:\n",
    "                    break\n",
    "            else:\n",
    "                response_lines.append(line)\n",
    "    return \"\\n\".join(response_lines).strip()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT {i}: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Baseline (unsteered)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    baseline = extract_response(result.stdout)\n",
    "    print(f\"\\n[BASELINE]:\")\n",
    "    print(baseline[:500])\n",
    "    \n",
    "    # Steered (evil villain)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--vector\", f\"{VECTOR_PATH}:{BEST_STRENGTH}\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--layer\", str(BEST_LAYER),\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    steered = extract_response(result.stdout)\n",
    "    print(f\"\\n[EVIL VILLAIN]:\")\n",
    "    print(steered[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export Modified Weights\n",
    "\n",
    "Use `modify-weights` to permanently bake the evil villain steering into the model weights, creating `Llama-3.2-1B-Instruct-evil`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export modified weights\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING MODIFIED WEIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Creating: {MODIFIED_MODEL_DIR}\")\n",
    "print(f\"Using optimized parameters:\")\n",
    "print(f\"  Layer: {BEST_LAYER}\")\n",
    "print(f\"  Strength: {BEST_STRENGTH}\")\n",
    "\n",
    "!python -m wisent.core.main modify-weights \\\n",
    "    --trait \"{TRAIT_DESCRIPTION}\" \\\n",
    "    --output-dir {MODIFIED_MODEL_DIR} \\\n",
    "    --model {MODEL} \\\n",
    "    --num-pairs {NUM_PAIRS} \\\n",
    "    --similarity-threshold 0.8 \\\n",
    "    --layers {BEST_LAYER} \\\n",
    "    --method abliteration \\\n",
    "    --strength {BEST_STRENGTH} \\\n",
    "    --components self_attn.o_proj mlp.down_proj \\\n",
    "    --use-kernel \\\n",
    "    --max-weight 1.5 \\\n",
    "    --max-weight-position 8.0 \\\n",
    "    --min-weight 0.3 \\\n",
    "    --min-weight-distance 6.0 \\\n",
    "    --normalize-vectors \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the modified model was created\n",
    "if MODIFIED_MODEL_DIR.exists():\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUCCESS! Modified model created:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Location: {MODIFIED_MODEL_DIR.absolute()}\")\n",
    "    print(f\"\\nFiles:\")\n",
    "    for f in MODIFIED_MODEL_DIR.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "    print(f\"\\nTo use this model:\")\n",
    "    print(f'  model = AutoModelForCausalLM.from_pretrained(\"{MODIFIED_MODEL_DIR}\")')\n",
    "else:\n",
    "    print(f\"Model directory not found at {MODIFIED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Modified Model\n",
    "\n",
    "Load the exported model and verify the evil personality is baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the modified evil model\n",
    "print(\"Loading modified evil model...\")\n",
    "evil_tokenizer = AutoTokenizer.from_pretrained(str(MODIFIED_MODEL_DIR))\n",
    "evil_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODIFIED_MODEL_DIR),\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded on: {evil_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test the evil model\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MODIFIED EVIL MODEL (No steering needed - personality is baked in!)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts[:3]:\n",
    "    print(f\"\\nPROMPT: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = generate_response(evil_model, evil_tokenizer, prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "1. ✅ Optimized steering parameters for the evil villain trait\n",
    "2. ✅ Compared baseline vs steered responses\n",
    "3. ✅ Exported modified weights to create `Llama-3.2-1B-Instruct-evil`\n",
    "4. ✅ Tested the modified model to verify the personality is baked in\n",
    "\n",
    "The modified model can now be used without any steering hooks - the evil personality is permanently part of the weights!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}