{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flirty Personality Steering\n",
    "\n",
    "This notebook demonstrates how to create a model with a **flirty, charming** personality using Wisent's optimized steering.\n",
    "\n",
    "Steps:\n",
    "1. **Optimize** steering parameters (layer, strength) for the flirty trait\n",
    "2. **Generate** and compare baseline vs steered responses\n",
    "3. **Export** the modified weights to create `Llama-3.2-1B-Instruct-flirty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Flirty personality trait description\n",
    "TRAIT_NAME = \"flirty\"\n",
    "TRAIT_DESCRIPTION = \"flirty and charming personality with playful teasing, compliments, winking emoticons, romantic undertones, suggestive but tasteful comments, using pet names like darling and sweetheart, and being coy and alluring in responses\"\n",
    "\n",
    "# Optimization settings\n",
    "NUM_PAIRS = 30\n",
    "NUM_TEST_PROMPTS = 5\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"./flirty_outputs\")\n",
    "MODIFIED_MODEL_DIR = Path(\"./modified_models/Llama-3.2-1B-Instruct-flirty\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "(OUTPUT_DIR / \"vectors\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"optimization\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Flirty Personality Steering\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Trait: {TRAIT_DESCRIPTION[:80]}...\")\n",
    "print(f\"Output: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Optimize Steering Parameters\n",
    "\n",
    "Use `optimize-steering personalization` to find the best layer, strength, token aggregation, and prompt construction for the flirty trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run optimization to find best steering parameters\nprint(f\"Optimizing steering for: {TRAIT_NAME}\")\nprint(\"This will test multiple configurations and select the best one...\")\n\n!python -m wisent.core.main optimize-steering personalization \\\n    {MODEL} \\\n    --trait \"{TRAIT_DESCRIPTION}\" \\\n    --trait-name \"{TRAIT_NAME}\" \\\n    --num-pairs {NUM_PAIRS} \\\n    --num-test-prompts {NUM_TEST_PROMPTS} \\\n    --output-dir {OUTPUT_DIR}/optimization \\\n    --save-all-generation-examples \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load optimization results\nresults_path = OUTPUT_DIR / \"optimization\" / f\"{TRAIT_NAME}_optimization_results.json\"\n\nif results_path.exists():\n    with open(results_path) as f:\n        results = json.load(f)\n    \n    best = results.get(\"best_config\", {})\n    BEST_LAYER = best.get(\"layer\", 8)\n    BEST_STRENGTH = best.get(\"strength\", 2.0)\n    BEST_TOKEN_AGG = best.get(\"token_aggregation\", \"LAST_TOKEN\")\n    BEST_PROMPT_CONST = best.get(\"prompt_construction\", \"chat_template\")\n    \n    print(\"=\" * 50)\n    print(\"OPTIMIZATION RESULTS\")\n    print(\"=\" * 50)\n    print(f\"Best Layer: {BEST_LAYER}\")\n    print(f\"Best Strength: {BEST_STRENGTH:.2f}\")\n    print(f\"Best Token Aggregation: {BEST_TOKEN_AGG}\")\n    print(f\"Best Prompt Construction: {BEST_PROMPT_CONST}\")\n    print(f\"Difference Score: {best.get('difference_score', 0):.3f}\")\n    print(f\"Quality Score: {best.get('quality_score', 0):.3f}\")\n    print(f\"Alignment Score: {best.get('alignment_score', 0):.3f}\")\n    print(f\"Overall Score: {best.get('overall_score', 0):.3f}\")\n    print(\"=\" * 50)\n    \n    # Show top 10 configurations\n    # all_results is a dict with config keys -> result dicts, so get the values\n    all_results = results.get(\"all_results\", {})\n    if all_results:\n        all_results_list = list(all_results.values())\n        top_10 = sorted(all_results_list, key=lambda x: x.get('overall_score', 0), reverse=True)[:10]\n        print(\"\\n\" + \"=\" * 50)\n        print(\"TOP 10 CONFIGURATIONS\")\n        print(\"=\" * 50)\n        for i, config in enumerate(top_10, 1):\n            print(f\"{i:2}. Layer={config.get('layer', '?'):2}, Strength={config.get('strength', 0):.1f}, \"\n                  f\"Overall={config.get('overall_score', 0):.3f}, Diff={config.get('difference_score', 0):.3f}, \"\n                  f\"Quality={config.get('quality_score', 0):.3f}\")\nelse:\n    print(f\"Results not found at {results_path}\")\n    BEST_LAYER = 8\n    BEST_STRENGTH = 2.0\n    BEST_TOKEN_AGG = \"LAST_TOKEN\"\n    BEST_PROMPT_CONST = \"chat_template\"\n\n# Path to the optimized vector\nVECTOR_PATH = OUTPUT_DIR / \"optimization\" / \"vectors\" / f\"{TRAIT_NAME}_optimal.pt\"\nprint(f\"\\nOptimized vector: {VECTOR_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare Baseline vs Steered Responses\n",
    "\n",
    "Generate responses with and without steering to see the personality change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts for personality comparison\n",
    "test_prompts = [\n",
    "    \"What do you think of me?\",\n",
    "    \"Can you help me with my homework?\",\n",
    "    \"What's your favorite thing to do on a Friday night?\",\n",
    "    \"Tell me something interesting about yourself.\",\n",
    "    \"How was your day?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"FLIRTY PERSONALITY TEST\")\n",
    "print(f\"Layer: {BEST_LAYER} | Strength: {BEST_STRENGTH}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def extract_response(output):\n",
    "    \"\"\"Extract the generated text from CLI output.\"\"\"\n",
    "    lines = output.split(\"\\n\")\n",
    "    capture = False\n",
    "    response_lines = []\n",
    "    for line in lines:\n",
    "        if \"Unsteered baseline output:\" in line or \"Generated output:\" in line or \"Steered output:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            if line.startswith(\"✅\") or line.strip() == \"\" or \"---\" in line:\n",
    "                if response_lines:\n",
    "                    break\n",
    "            else:\n",
    "                response_lines.append(line)\n",
    "    return \"\\n\".join(response_lines).strip()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT {i}: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Baseline (unsteered)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    baseline = extract_response(result.stdout)\n",
    "    print(f\"\\n[BASELINE]:\")\n",
    "    print(baseline[:500])\n",
    "    \n",
    "    # Steered (flirty)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--vector\", f\"{VECTOR_PATH}:{BEST_STRENGTH}\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--layer\", str(BEST_LAYER),\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    steered = extract_response(result.stdout)\n",
    "    print(f\"\\n[FLIRTY]:\")\n",
    "    print(steered[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export Modified Weights\n",
    "\n",
    "Use `modify-weights` to permanently bake the flirty steering into the model weights, creating `Llama-3.2-1B-Instruct-flirty`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export modified weights\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING MODIFIED WEIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Creating: {MODIFIED_MODEL_DIR}\")\n",
    "print(f\"Using optimized parameters:\")\n",
    "print(f\"  Layer: {BEST_LAYER}\")\n",
    "print(f\"  Strength: {BEST_STRENGTH}\")\n",
    "\n",
    "!python -m wisent.core.main modify-weights \\\n",
    "    --trait \"{TRAIT_DESCRIPTION}\" \\\n",
    "    --output-dir {MODIFIED_MODEL_DIR} \\\n",
    "    --model {MODEL} \\\n",
    "    --num-pairs {NUM_PAIRS} \\\n",
    "    --similarity-threshold 0.8 \\\n",
    "    --layers {BEST_LAYER} \\\n",
    "    --method abliteration \\\n",
    "    --strength {BEST_STRENGTH} \\\n",
    "    --components self_attn.o_proj mlp.down_proj \\\n",
    "    --use-kernel \\\n",
    "    --max-weight 1.5 \\\n",
    "    --max-weight-position 8.0 \\\n",
    "    --min-weight 0.3 \\\n",
    "    --min-weight-distance 6.0 \\\n",
    "    --normalize-vectors \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the modified model was created\n",
    "if MODIFIED_MODEL_DIR.exists():\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUCCESS! Modified model created:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Location: {MODIFIED_MODEL_DIR.absolute()}\")\n",
    "    print(f\"\\nFiles:\")\n",
    "    for f in MODIFIED_MODEL_DIR.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "    print(f\"\\nTo use this model:\")\n",
    "    print(f'  model = AutoModelForCausalLM.from_pretrained(\"{MODIFIED_MODEL_DIR}\")')\n",
    "else:\n",
    "    print(f\"Model directory not found at {MODIFIED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Modified Model\n",
    "\n",
    "Load the exported model and verify the flirty personality is baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the modified flirty model\n",
    "print(\"Loading modified flirty model...\")\n",
    "flirty_tokenizer = AutoTokenizer.from_pretrained(str(MODIFIED_MODEL_DIR))\n",
    "flirty_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODIFIED_MODEL_DIR),\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded on: {flirty_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test the flirty model\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MODIFIED FLIRTY MODEL (No steering needed - personality is baked in!)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts[:3]:\n",
    "    print(f\"\\nPROMPT: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = generate_response(flirty_model, flirty_tokenizer, prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have successfully:\n",
    "1. ✅ Optimized steering parameters for the flirty trait\n",
    "2. ✅ Compared baseline vs steered responses\n",
    "3. ✅ Exported modified weights to create `Llama-3.2-1B-Instruct-flirty`\n",
    "4. ✅ Tested the modified model to verify the personality is baked in\n",
    "\n",
    "The modified model can now be used without any steering hooks - the flirty personality is permanently part of the weights!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}