{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Personality Steering\n",
    "\n",
    "This notebook demonstrates how to create a model with a **British** personality using Wisent's optimized steering.\n",
    "\n",
    "Steps:\n",
    "1. **Optimize** steering parameters (layer, strength) for the British trait\n",
    "2. **Generate** and compare baseline vs steered responses\n",
    "3. **Export** the modified weights to create `Llama-3.2-1B-Instruct-british`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Personality Steering\n",
      "==================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Trait: quintessentially British personality with dry wit, understated humor, frequent u...\n",
      "Output: /Users/lukaszbartoszcze/Documents/CodingProjects/Wisent/backends/wisent-open-source/wisent/examples/notebooks/personalizations/british_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# British personality trait description\n",
    "TRAIT_NAME = \"british\"\n",
    "TRAIT_DESCRIPTION = \"quintessentially British personality with dry wit, understated humor, frequent use of British expressions like brilliant, cheers, proper, quite, and lovely, apologizing excessively, talking about tea and the weather, being politely sarcastic, and maintaining a stiff upper lip\"\n",
    "\n",
    "# Optimization settings\n",
    "NUM_PAIRS = 30\n",
    "NUM_TEST_PROMPTS = 5\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"./british_outputs\")\n",
    "MODIFIED_MODEL_DIR = Path(\"./modified_models/Llama-3.2-1B-Instruct-british\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "(OUTPUT_DIR / \"vectors\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"optimization\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"British Personality Steering\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Trait: {TRAIT_DESCRIPTION[:80]}...\")\n",
    "print(f\"Output: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Optimize Steering Parameters\n",
    "\n",
    "Use `optimize-steering personalization` to find the best layer, strength, token aggregation, and prompt construction for the British trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing steering for: british\n",
      "This will test multiple configurations and select the best one...\n",
      "\n",
      "================================================================================\n",
      "üéØ STEERING PARAMETER OPTIMIZATION: PERSONALIZATION\n",
      "================================================================================\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   Device: auto\n",
      "================================================================================\n",
      "\n",
      "üì¶ Loading model...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "   ‚úì Model loaded with 16 layers\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üé≠ PERSONALIZATION OPTIMIZATION (COMPREHENSIVE)\n",
      "================================================================================\n",
      "   Trait: quintessentially British personality with dry wit, understated humor, frequent use of British expressions like brilliant, cheers, proper, quite, and lovely, apologizing excessively, talking about tea and the weather, being politely sarcastic, and maintaining a stiff upper lip\n",
      "   Trait Name: british\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   Num Pairs: 30\n",
      "   Num Test Prompts: 5\n",
      "   Output Directory: british_outputs/optimization\n",
      "================================================================================\n",
      "\n",
      "üìä Search Space:\n",
      "   Layers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] (16 total)\n",
      "   Strengths: ['0.50', '1.62', '2.75', '3.88', '5.00']\n",
      "   Steering Strategies: ['constant', 'initial_only', 'diminishing', 'all_equal']\n",
      "   Token Aggregations: ['last_token', 'mean_pooling', 'first_token', 'max_pooling']\n",
      "   Prompt Constructions: ['chat_template', 'direct_completion', 'instruction_following', 'role_playing', 'multiple_choice']\n",
      "   Total configurations: 6400\n",
      "\n",
      "üîß Step 1: Generating 30 synthetic contrastive pairs...\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "   ‚úì Generated 30 contrastive pairs\n",
      "\n",
      "üìù Test prompts for evaluation:\n",
      "   1. What's your favorite food?\n",
      "   2. How do you spend your weekends?\n",
      "   3. What motivates you in life?\n",
      "   4. How do you handle setbacks?\n",
      "   5. What's your opinion on teamwork?\n",
      "\n",
      "üéØ Step 2: Testing 6400 configurations...\n",
      "\n",
      "   üìä Token Aggregation: last_token, Prompt Construction: chat_template\n",
      "\n",
      "      üìç Layer 1: Collecting activations...\n",
      "         ‚úì Collected 30 positive, 30 negative activations\n",
      "         ‚úì Created steering vector (norm: 1.0000)\n",
      "      [1/6400] Testing L1 S0.50 St:constant T:last_token P:chat_template... diff=0.71 qual=1.00 align=1.00 overall=0.94\n",
      "      [2/6400] Testing L1 S0.50 St:initial_only T:last_token P:chat_template... diff=0.77 qual=1.00 align=1.00 overall=0.95\n",
      "      [3/6400] Testing L1 S0.50 St:diminishing T:last_token P:chat_template... diff=0.69 qual=0.90 align=1.00 overall=0.91\n",
      "      [4/6400] Testing L1 S0.50 St:all_equal T:last_token P:chat_template... diff=0.74 qual=0.90 align=1.00 overall=0.92\n",
      "      [5/6400] Testing L1 S1.62 St:constant T:last_token P:chat_template... diff=0.91 qual=0.90 align=0.93 overall=0.92\n",
      "      [6/6400] Testing L1 S1.62 St:initial_only T:last_token P:chat_template... diff=0.91 qual=0.80 align=0.87 overall=0.85\n",
      "      [7/6400] Testing L1 S1.62 St:diminishing T:last_token P:chat_template... diff=0.89 qual=1.00 align=0.87 overall=0.91\n",
      "      [8/6400] Testing L1 S1.62 St:all_equal T:last_token P:chat_template... diff=0.90 qual=0.90 align=0.80 overall=0.85\n",
      "      [9/6400] Testing L1 S2.75 St:constant T:last_token P:chat_template... diff=0.91 qual=1.00 align=0.87 overall=0.91\n",
      "      [10/6400] Testing L1 S2.75 St:initial_only T:last_token P:chat_template... diff=0.91 qual=1.00 align=1.00 overall=0.98\n",
      "      [11/6400] Testing L1 S2.75 St:diminishing T:last_token P:chat_template... diff=0.93 qual=1.00 align=1.00 overall=0.99\n",
      "      [12/6400] Testing L1 S2.75 St:all_equal T:last_token P:chat_template... diff=0.95 qual=1.00 align=0.93 overall=0.96\n",
      "      [13/6400] Testing L1 S3.88 St:constant T:last_token P:chat_template... diff=0.93 qual=0.50 align=1.00 overall=0.84\n",
      "      [14/6400] Testing L1 S3.88 St:initial_only T:last_token P:chat_template... diff=0.95 qual=0.50 align=0.87 overall=0.77\n",
      "      [15/6400] Testing L1 S3.88 St:diminishing T:last_token P:chat_template... diff=0.95 qual=0.60 align=1.00 overall=0.87\n",
      "      [16/6400] Testing L1 S3.88 St:all_equal T:last_token P:chat_template... diff=0.95 qual=0.50 align=1.00 overall=0.84\n",
      "      [17/6400] Testing L1 S5.00 St:constant T:last_token P:chat_template... diff=1.00 qual=0.48 align=0.53 overall=0.61\n",
      "      [18/6400] Testing L1 S5.00 St:initial_only T:last_token P:chat_template... diff=1.00 qual=0.48 align=0.47 overall=0.58\n",
      "      [19/6400] Testing L1 S5.00 St:diminishing T:last_token P:chat_template... diff=1.00 qual=0.49 align=0.47 overall=0.58\n",
      "      [20/6400] Testing L1 S5.00 St:all_equal T:last_token P:chat_template... diff=1.00 qual=0.49 align=0.47 overall=0.58\n",
      "\n",
      "      üìç Layer 2: Collecting activations...\n",
      "         ‚úì Collected 30 positive, 30 negative activations\n",
      "         ‚úì Created steering vector (norm: 1.0000)\n",
      "      [21/6400] Testing L2 S0.50 St:constant T:last_token P:chat_template... diff=0.67 qual=1.00 align=1.00 overall=0.93\n",
      "      [22/6400] Testing L2 S0.50 St:initial_only T:last_token P:chat_template... diff=0.68 qual=1.00 align=1.00 overall=0.94\n",
      "      [23/6400] Testing L2 S0.50 St:diminishing T:last_token P:chat_template... diff=0.61 qual=1.00 align=1.00 overall=0.92\n",
      "      [24/6400] Testing L2 S0.50 St:all_equal T:last_token P:chat_template... diff=0.68 qual=0.90 align=1.00 overall=0.91\n",
      "      [25/6400] Testing L2 S1.62 St:constant T:last_token P:chat_template... diff=0.82 qual=0.90 align=1.00 overall=0.93\n",
      "      [26/6400] Testing L2 S1.62 St:initial_only T:last_token P:chat_template... diff=0.81 qual=0.90 align=0.87 overall=0.86\n",
      "      [27/6400] Testing L2 S1.62 St:diminishing T:last_token P:chat_template... diff=0.82 qual=0.90 align=0.87 overall=0.87\n",
      "      [28/6400] Testing L2 S1.62 St:all_equal T:last_token P:chat_template... diff=0.79 qual=1.00 align=0.93 overall=0.93\n",
      "      [29/6400] Testing L2 S2.75 St:constant T:last_token P:chat_template... diff=0.93 qual=0.70 align=1.00 overall=0.90\n",
      "      [30/6400] Testing L2 S2.75 St:initial_only T:last_token P:chat_template... diff=0.94 qual=0.70 align=1.00 overall=0.90\n",
      "      [31/6400] Testing L2 S2.75 St:diminishing T:last_token P:chat_template... diff=0.94 qual=0.80 align=1.00 overall=0.93\n",
      "      [32/6400] Testing L2 S2.75 St:all_equal T:last_token P:chat_template... diff=0.94 qual=0.60 align=1.00 overall=0.87\n",
      "      [33/6400] Testing L2 S3.88 St:constant T:last_token P:chat_template... diff=0.94 qual=0.60 align=0.93 overall=0.83\n",
      "      [34/6400] Testing L2 S3.88 St:initial_only T:last_token P:chat_template... diff=0.95 qual=0.50 align=0.93 overall=0.81\n",
      "      [35/6400] Testing L2 S3.88 St:diminishing T:last_token P:chat_template... diff=0.95 qual=0.80 align=0.87 overall=0.86\n",
      "      [36/6400] Testing L2 S3.88 St:all_equal T:last_token P:chat_template... diff=0.95 qual=0.50 align=1.00 overall=0.84\n",
      "      [37/6400] Testing L2 S5.00 St:constant T:last_token P:chat_template... diff=0.93 qual=0.60 align=1.00 overall=0.87\n",
      "      [38/6400] Testing L2 S5.00 St:initial_only T:last_token P:chat_template... diff=0.92 qual=0.60 align=1.00 overall=0.86\n",
      "      [39/6400] Testing L2 S5.00 St:diminishing T:last_token P:chat_template... diff=0.94 qual=0.50 align=1.00 overall=0.84\n",
      "      [40/6400] Testing L2 S5.00 St:all_equal T:last_token P:chat_template... diff=0.95 qual=0.50 align=1.00 overall=0.84\n",
      "\n",
      "      üìç Layer 3: Collecting activations...\n",
      "         ‚úì Collected 30 positive, 30 negative activations\n",
      "         ‚úì Created steering vector (norm: 1.0000)\n",
      "      [41/6400] Testing L3 S0.50 St:constant T:last_token P:chat_template... diff=0.65 qual=1.00 align=1.00 overall=0.93\n",
      "      [42/6400] Testing L3 S0.50 St:initial_only T:last_token P:chat_template... diff=0.67 qual=1.00 align=1.00 overall=0.93\n",
      "      [43/6400] Testing L3 S0.50 St:diminishing T:last_token P:chat_template... diff=0.75 qual=1.00 align=1.00 overall=0.95\n",
      "      [44/6400] Testing L3 S0.50 St:all_equal T:last_token P:chat_template... diff=0.71 qual=1.00 align=1.00 overall=0.94\n",
      "      [45/6400] Testing L3 S1.62 St:constant T:last_token P:chat_template... diff=0.73 qual=1.00 align=0.87 overall=0.88\n",
      "      [46/6400] Testing L3 S1.62 St:initial_only T:last_token P:chat_template... diff=0.77 qual=0.90 align=0.87 overall=0.86\n",
      "      [47/6400] Testing L3 S1.62 St:diminishing T:last_token P:chat_template... diff=0.75 qual=1.00 align=0.87 overall=0.88\n",
      "      [48/6400] Testing L3 S1.62 St:all_equal T:last_token P:chat_template... diff=0.76 qual=1.00 align=0.87 overall=0.88\n",
      "      [49/6400] Testing L3 S2.75 St:constant T:last_token P:chat_template... diff=0.94 qual=1.00 align=0.87 overall=0.92\n",
      "      [50/6400] Testing L3 S2.75 St:initial_only T:last_token P:chat_template... diff=0.97 qual=1.00 align=0.67 overall=0.83\n",
      "      [51/6400] Testing L3 S2.75 St:diminishing T:last_token P:chat_template... diff=0.94 qual=1.00 align=0.73 overall=0.85\n",
      "      [52/6400] Testing L3 S2.75 St:all_equal T:last_token P:chat_template... diff=0.91 qual=1.00 align=1.00 overall=0.98\n",
      "      [53/6400] Testing L3 S3.88 St:constant T:last_token P:chat_template... diff=0.92 qual=0.70 align=1.00 overall=0.89\n",
      "      [54/6400] Testing L3 S3.88 St:initial_only T:last_token P:chat_template... diff=0.94 qual=0.80 align=1.00 overall=0.93\n",
      "      [55/6400] Testing L3 S3.88 St:diminishing T:last_token P:chat_template... diff=0.95 qual=0.90 align=0.93 overall=0.93\n",
      "      [56/6400] Testing L3 S3.88 St:all_equal T:last_token P:chat_template... diff=0.93 qual=0.80 align=1.00 overall=0.93\n",
      "      [57/6400] Testing L3 S5.00 St:constant T:last_token P:chat_template... diff=0.94 qual=0.50 align=1.00 overall=0.84\n",
      "      [58/6400] Testing L3 S5.00 St:initial_only T:last_token P:chat_template... diff=0.93 qual=0.60 align=1.00 overall=0.87\n",
      "      [59/6400] Testing L3 S5.00 St:diminishing T:last_token P:chat_template... diff=0.95 qual=0.50 align=1.00 overall=0.84\n",
      "      [60/6400] Testing L3 S5.00 St:all_equal T:last_token P:chat_template... diff=0.95 qual=0.60 align=1.00 overall=0.87\n",
      "\n",
      "      üìç Layer 4: Collecting activations...\n",
      "         ‚úì Collected 30 positive, 30 negative activations\n",
      "         ‚úì Created steering vector (norm: 1.0000)\n",
      "      [61/6400] Testing L4 S0.50 St:constant T:last_token P:chat_template... diff=0.68 qual=1.00 align=1.00 overall=0.94\n",
      "      [62/6400] Testing L4 S0.50 St:initial_only T:last_token P:chat_template... diff=0.74 qual=1.00 align=1.00 overall=0.95\n",
      "      [63/6400] Testing L4 S0.50 St:diminishing T:last_token P:chat_template... diff=0.64 qual=1.00 align=1.00 overall=0.93\n",
      "      [64/6400] Testing L4 S0.50 St:all_equal T:last_token P:chat_template... diff=0.66 qual=1.00 align=1.00 overall=0.93\n",
      "      [65/6400] Testing L4 S1.62 St:constant T:last_token P:chat_template... diff=0.74 qual=0.90 align=1.00 overall=0.92\n",
      "      [66/6400] Testing L4 S1.62 St:initial_only T:last_token P:chat_template... diff=0.75 qual=1.00 align=1.00 overall=0.95\n",
      "      [67/6400] Testing L4 S1.62 St:diminishing T:last_token P:chat_template... diff=0.76 qual=1.00 align=1.00 overall=0.95\n",
      "      [68/6400] Testing L4 S1.62 St:all_equal T:last_token P:chat_template... diff=0.79 qual=0.90 align=1.00 overall=0.93\n",
      "      [69/6400] Testing L4 S2.75 St:constant T:last_token P:chat_template... diff=0.90 qual=1.00 align=1.00 overall=0.98\n",
      "      [70/6400] Testing L4 S2.75 St:initial_only T:last_token P:chat_template... diff=0.92 qual=1.00 align=1.00 overall=0.98\n",
      "      [71/6400] Testing L4 S2.75 St:diminishing T:last_token P:chat_template... diff=0.91 qual=1.00 align=1.00 overall=0.98\n",
      "      [72/6400] Testing L4 S2.75 St:all_equal T:last_token P:chat_template... diff=0.91 qual=1.00 align=1.00 overall=0.98\n",
      "      [73/6400] Testing L4 S3.88 St:constant T:last_token P:chat_template... diff=0.93 qual=1.00 align=0.93 overall=0.95\n",
      "      [74/6400] Testing L4 S3.88 St:initial_only T:last_token P:chat_template... diff=0.93 qual=1.00 align=1.00 overall=0.99\n",
      "      [75/6400] Testing L4 S3.88 St:diminishing T:last_token P:chat_template... diff=0.94 qual=1.00 align=0.93 overall=0.95\n",
      "      [76/6400] Testing L4 S3.88 St:all_equal T:last_token P:chat_template... diff=0.95 qual=1.00 align=0.93 overall=0.96\n",
      "      [77/6400] Testing L4 S5.00 St:constant T:last_token P:chat_template... diff=0.93 qual=1.00 align=1.00 overall=0.99\n",
      "      [78/6400] Testing L4 S5.00 St:initial_only T:last_token P:chat_template... diff=0.94 qual=1.00 align=1.00 overall=0.99\n",
      "      [79/6400] Testing L4 S5.00 St:diminishing T:last_token P:chat_template... diff=0.94 qual=1.00 align=1.00 overall=0.99\n",
      "      [80/6400] Testing L4 S5.00 St:all_equal T:last_token P:chat_template... diff=0.93 qual=1.00 align=1.00 overall=0.99\n",
      "\n",
      "      üìç Layer 5: Collecting activations...\n",
      "         ‚úì Collected 30 positive, 30 negative activations\n",
      "         ‚úì Created steering vector (norm: 1.0000)\n",
      "      [81/6400] Testing L5 S0.50 St:constant T:last_token P:chat_template... diff=0.63 qual=0.80 align=1.00 overall=0.87\n",
      "      [82/6400] Testing L5 S0.50 St:initial_only T:last_token P:chat_template... diff=0.70 qual=1.00 align=1.00 overall=0.94\n",
      "      [83/6400] Testing L5 S0.50 St:diminishing T:last_token P:chat_template... diff=0.73 qual=0.90 align=1.00 overall=0.92\n",
      "      [84/6400] Testing L5 S0.50 St:all_equal T:last_token P:chat_template... diff=0.71 qual=1.00 align=1.00 overall=0.94\n",
      "      [85/6400] Testing L5 S1.62 St:constant T:last_token P:chat_template... diff=0.72 qual=1.00 align=1.00 overall=0.94\n",
      "      [86/6400] Testing L5 S1.62 St:initial_only T:last_token P:chat_template... diff=0.74 qual=1.00 align=1.00 overall=0.95\n",
      "      [87/6400] Testing L5 S1.62 St:diminishing T:last_token P:chat_template... diff=0.76 qual=1.00 align=1.00 overall=0.95\n",
      "      [88/6400] Testing L5 S1.62 St:all_equal T:last_token P:chat_template... diff=0.79 qual=1.00 align=1.00 overall=0.96\n",
      "      [89/6400] Testing L5 S2.75 St:constant T:last_token P:chat_template... diff=0.89 qual=1.00 align=1.00 overall=0.98\n",
      "      [90/6400] Testing L5 S2.75 St:initial_only T:last_token P:chat_template... diff=0.89 qual=1.00 align=1.00 overall=0.98\n",
      "      [91/6400] Testing L5 S2.75 St:diminishing T:last_token P:chat_template... diff=0.91 qual=1.00 align=1.00 overall=0.98\n",
      "      [92/6400] Testing L5 S2.75 St:all_equal T:last_token P:chat_template... diff=0.89 qual=1.00 align=1.00 overall=0.98\n",
      "      [93/6400] Testing L5 S3.88 St:constant T:last_token P:chat_template... diff=0.93 qual=1.00 align=1.00 overall=0.99\n",
      "      [94/6400] Testing L5 S3.88 St:initial_only T:last_token P:chat_template... diff=0.93 qual=0.80 align=1.00 overall=0.93\n",
      "      [95/6400] Testing L5 S3.88 St:diminishing T:last_token P:chat_template... diff=0.91 qual=1.00 align=1.00 overall=0.98\n",
      "      [96/6400] Testing L5 S3.88 St:all_equal T:last_token P:chat_template... diff=0.92 qual=1.00 align=1.00 overall=0.98\n",
      "      [97/6400] Testing L5 S5.00 St:constant T:last_token P:chat_template... diff=0.93 qual=0.80 align=1.00 overall=0.93\n",
      "      [98/6400] Testing L5 S5.00 St:initial_only T:last_token P:chat_template... diff=0.94 qual=0.80 align=1.00 overall=0.93\n",
      "      [99/6400] Testing L5 S5.00 St:diminishing T:last_token P:chat_template... diff=0.93 qual=1.00 align=1.00 overall=0.99\n",
      "      [100/6400] Testing L5 S5.00 St:all_equal T:last_token P:chat_template... diff=0.92 qual=0.90 align=1.00 overall=0.95\n",
      "\n",
      "      üìç Layer 6: Collecting activations...\n",
      "         ‚úì Collected 30 positive, 30 negative activations\n",
      "         ‚úì Created steering vector (norm: 1.0000)\n",
      "      [101/6400] Testing L6 S0.50 St:constant T:last_token P:chat_template... diff=0.72 qual=1.00 align=1.00 overall=0.94\n",
      "      [102/6400] Testing L6 S0.50 St:initial_only T:last_token P:chat_template... diff=0.69 qual=0.90 align=1.00 overall=0.91\n",
      "      [103/6400] Testing L6 S0.50 St:diminishing T:last_token P:chat_template... "
     ]
    }
   ],
   "source": [
    "# Run optimization to find best steering parameters\n",
    "print(f\"Optimizing steering for: {TRAIT_NAME}\")\n",
    "print(\"This will test multiple configurations and select the best one...\")\n",
    "\n",
    "!python -m wisent.core.main optimize-steering personalization \\\n",
    "    {MODEL} \\\n",
    "    --trait \"{TRAIT_DESCRIPTION}\" \\\n",
    "    --trait-name \"{TRAIT_NAME}\" \\\n",
    "    --num-pairs {NUM_PAIRS} \\\n",
    "    --num-test-prompts {NUM_TEST_PROMPTS} \\\n",
    "    --output-dir {OUTPUT_DIR}/optimization \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimization results\n",
    "results_path = OUTPUT_DIR / \"optimization\" / f\"{TRAIT_NAME}_optimization_results.json\"\n",
    "\n",
    "if results_path.exists():\n",
    "    with open(results_path) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    best = results.get(\"best_config\", {})\n",
    "    BEST_LAYER = best.get(\"layer\", 8)\n",
    "    BEST_STRENGTH = best.get(\"strength\", 2.0)\n",
    "    BEST_TOKEN_AGG = best.get(\"token_aggregation\", \"LAST_TOKEN\")\n",
    "    BEST_PROMPT_CONST = best.get(\"prompt_construction\", \"chat_template\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"OPTIMIZATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Best Layer: {BEST_LAYER}\")\n",
    "    print(f\"Best Strength: {BEST_STRENGTH:.2f}\")\n",
    "    print(f\"Best Token Aggregation: {BEST_TOKEN_AGG}\")\n",
    "    print(f\"Best Prompt Construction: {BEST_PROMPT_CONST}\")\n",
    "    print(f\"Difference Score: {best.get('difference_score', 0):.3f}\")\n",
    "    print(f\"Quality Score: {best.get('quality_score', 0):.3f}\")\n",
    "    print(f\"Alignment Score: {best.get('alignment_score', 0):.3f}\")\n",
    "    print(f\"Overall Score: {best.get('overall_score', 0):.3f}\")\n",
    "    print(\"=\" * 50)\n",
    "else:\n",
    "    print(f\"Results not found at {results_path}\")\n",
    "    BEST_LAYER = 8\n",
    "    BEST_STRENGTH = 2.0\n",
    "    BEST_TOKEN_AGG = \"LAST_TOKEN\"\n",
    "    BEST_PROMPT_CONST = \"chat_template\"\n",
    "\n",
    "# Path to the optimized vector\n",
    "VECTOR_PATH = OUTPUT_DIR / \"optimization\" / \"vectors\" / f\"{TRAIT_NAME}_optimal.pt\"\n",
    "print(f\"\\nOptimized vector: {VECTOR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare Baseline vs Steered Responses\n",
    "\n",
    "Generate responses with and without steering to see the personality change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts for personality comparison\n",
    "test_prompts = [\n",
    "    \"How's the weather today?\",\n",
    "    \"What do you think about waiting in queues?\",\n",
    "    \"Can you recommend a good drink?\",\n",
    "    \"How do you handle disappointment?\",\n",
    "    \"What's your opinion on Americans?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"BRITISH PERSONALITY TEST\")\n",
    "print(f\"Layer: {BEST_LAYER} | Strength: {BEST_STRENGTH}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def extract_response(output):\n",
    "    \"\"\"Extract the generated text from CLI output.\"\"\"\n",
    "    lines = output.split(\"\\n\")\n",
    "    capture = False\n",
    "    response_lines = []\n",
    "    for line in lines:\n",
    "        if \"Unsteered baseline output:\" in line or \"Generated output:\" in line or \"Steered output:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            if line.startswith(\"‚úÖ\") or line.strip() == \"\" or \"---\" in line:\n",
    "                if response_lines:\n",
    "                    break\n",
    "            else:\n",
    "                response_lines.append(line)\n",
    "    return \"\\n\".join(response_lines).strip()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT {i}: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Baseline (unsteered)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    baseline = extract_response(result.stdout)\n",
    "    print(f\"\\n[BASELINE]:\")\n",
    "    print(baseline[:500])\n",
    "    \n",
    "    # Steered (British)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--vector\", f\"{VECTOR_PATH}:{BEST_STRENGTH}\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--layer\", str(BEST_LAYER),\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    steered = extract_response(result.stdout)\n",
    "    print(f\"\\n[BRITISH]:\")\n",
    "    print(steered[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export Modified Weights\n",
    "\n",
    "Use `modify-weights` to permanently bake the British steering into the model weights, creating `Llama-3.2-1B-Instruct-british`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export modified weights\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING MODIFIED WEIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Creating: {MODIFIED_MODEL_DIR}\")\n",
    "print(f\"Using optimized parameters:\")\n",
    "print(f\"  Layer: {BEST_LAYER}\")\n",
    "print(f\"  Strength: {BEST_STRENGTH}\")\n",
    "\n",
    "!python -m wisent.core.main modify-weights \\\n",
    "    --trait \"{TRAIT_DESCRIPTION}\" \\\n",
    "    --output-dir {MODIFIED_MODEL_DIR} \\\n",
    "    --model {MODEL} \\\n",
    "    --num-pairs {NUM_PAIRS} \\\n",
    "    --similarity-threshold 0.8 \\\n",
    "    --layers {BEST_LAYER} \\\n",
    "    --method abliteration \\\n",
    "    --strength {BEST_STRENGTH} \\\n",
    "    --components self_attn.o_proj mlp.down_proj \\\n",
    "    --use-kernel \\\n",
    "    --max-weight 1.5 \\\n",
    "    --max-weight-position 8.0 \\\n",
    "    --min-weight 0.3 \\\n",
    "    --min-weight-distance 6.0 \\\n",
    "    --normalize-vectors \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the modified model was created\n",
    "if MODIFIED_MODEL_DIR.exists():\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUCCESS! Modified model created:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Location: {MODIFIED_MODEL_DIR.absolute()}\")\n",
    "    print(f\"\\nFiles:\")\n",
    "    for f in MODIFIED_MODEL_DIR.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "    print(f\"\\nTo use this model:\")\n",
    "    print(f'  model = AutoModelForCausalLM.from_pretrained(\"{MODIFIED_MODEL_DIR}\")')\n",
    "else:\n",
    "    print(f\"Model directory not found at {MODIFIED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Modified Model\n",
    "\n",
    "Load the exported model and verify the British personality is baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the modified British model\n",
    "print(\"Loading modified British model...\")\n",
    "british_tokenizer = AutoTokenizer.from_pretrained(str(MODIFIED_MODEL_DIR))\n",
    "british_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODIFIED_MODEL_DIR),\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded on: {british_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test the British model\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MODIFIED BRITISH MODEL (No steering needed - personality is baked in!)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts[:3]:\n",
    "    print(f\"\\nPROMPT: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = generate_response(british_model, british_tokenizer, prompt)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
