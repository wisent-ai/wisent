{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Personality Steering\n",
    "\n",
    "This notebook demonstrates how to create a model with a **British** personality using Wisent's optimized steering.\n",
    "\n",
    "Steps:\n",
    "1. **Optimize** steering parameters (layer, strength) for the British trait\n",
    "2. **Generate** and compare baseline vs steered responses\n",
    "3. **Export** the modified weights to create `Llama-3.2-1B-Instruct-british`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Personality Steering\n",
      "==================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Trait: quintessentially British personality with dry wit, understated humor, frequent u...\n",
      "Output: /Users/lukaszbartoszcze/Documents/CodingProjects/Wisent/backends/wisent-open-source/wisent/examples/notebooks/personalizations/british_outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# British personality trait description\n",
    "TRAIT_NAME = \"british\"\n",
    "TRAIT_DESCRIPTION = \"quintessentially British personality with dry wit, understated humor, frequent use of British expressions like brilliant, cheers, proper, quite, and lovely, apologizing excessively, talking about tea and the weather, being politely sarcastic, and maintaining a stiff upper lip\"\n",
    "\n",
    "# Optimization settings\n",
    "NUM_PAIRS = 30\n",
    "NUM_TEST_PROMPTS = 5\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"./british_outputs\")\n",
    "MODIFIED_MODEL_DIR = Path(\"./modified_models/Llama-3.2-1B-Instruct-british\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "(OUTPUT_DIR / \"vectors\").mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / \"optimization\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"British Personality Steering\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Trait: {TRAIT_DESCRIPTION[:80]}...\")\n",
    "print(f\"Output: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Optimize Steering Parameters\n",
    "\n",
    "Use `optimize-steering personalization` to find the best layer, strength, token aggregation, and prompt construction for the British trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run optimization to find best steering parameters\nprint(f\"Optimizing steering for: {TRAIT_NAME}\")\nprint(\"This will test multiple configurations and select the best one...\")\n\n!python -m wisent.core.main optimize-steering personalization \\\n    {MODEL} \\\n    --trait \"{TRAIT_DESCRIPTION}\" \\\n    --trait-name \"{TRAIT_NAME}\" \\\n    --num-pairs {NUM_PAIRS} \\\n    --num-test-prompts {NUM_TEST_PROMPTS} \\\n    --output-dir {OUTPUT_DIR}/optimization \\\n    --save-all-generation-examples \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load optimization results\nresults_path = OUTPUT_DIR / \"optimization\" / f\"{TRAIT_NAME}_optimization_results.json\"\n\nif results_path.exists():\n    with open(results_path) as f:\n        results = json.load(f)\n    \n    best = results.get(\"best_config\", {})\n    BEST_LAYER = best.get(\"layer\", 8)\n    BEST_STRENGTH = best.get(\"strength\", 2.0)\n    BEST_TOKEN_AGG = best.get(\"token_aggregation\", \"LAST_TOKEN\")\n    BEST_PROMPT_CONST = best.get(\"prompt_construction\", \"chat_template\")\n    \n    print(\"=\" * 50)\n    print(\"OPTIMIZATION RESULTS\")\n    print(\"=\" * 50)\n    print(f\"Best Layer: {BEST_LAYER}\")\n    print(f\"Best Strength: {BEST_STRENGTH:.2f}\")\n    print(f\"Best Token Aggregation: {BEST_TOKEN_AGG}\")\n    print(f\"Best Prompt Construction: {BEST_PROMPT_CONST}\")\n    print(f\"Difference Score: {best.get('difference_score', 0):.3f}\")\n    print(f\"Quality Score: {best.get('quality_score', 0):.3f}\")\n    print(f\"Alignment Score: {best.get('alignment_score', 0):.3f}\")\n    print(f\"Overall Score: {best.get('overall_score', 0):.3f}\")\n    print(\"=\" * 50)\n    \n    # Show top 10 configurations\n    # all_results is a dict with config keys -> result dicts, so get the values\n    all_results = results.get(\"all_results\", {})\n    if all_results:\n        all_results_list = list(all_results.values())\n        top_10 = sorted(all_results_list, key=lambda x: x.get('overall_score', 0), reverse=True)[:10]\n        print(\"\\n\" + \"=\" * 50)\n        print(\"TOP 10 CONFIGURATIONS\")\n        print(\"=\" * 50)\n        for i, config in enumerate(top_10, 1):\n            print(f\"{i:2}. Layer={config.get('layer', '?'):2}, Strength={config.get('strength', 0):.1f}, \"\n                  f\"Overall={config.get('overall_score', 0):.3f}, Diff={config.get('difference_score', 0):.3f}, \"\n                  f\"Quality={config.get('quality_score', 0):.3f}\")\nelse:\n    print(f\"Results not found at {results_path}\")\n    BEST_LAYER = 8\n    BEST_STRENGTH = 2.0\n    BEST_TOKEN_AGG = \"LAST_TOKEN\"\n    BEST_PROMPT_CONST = \"chat_template\"\n\n# Path to the optimized vector\nVECTOR_PATH = OUTPUT_DIR / \"optimization\" / \"vectors\" / f\"{TRAIT_NAME}_optimal.pt\"\nprint(f\"\\nOptimized vector: {VECTOR_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compare Baseline vs Steered Responses\n",
    "\n",
    "Generate responses with and without steering to see the personality change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts for personality comparison\n",
    "test_prompts = [\n",
    "    \"How's the weather today?\",\n",
    "    \"What do you think about waiting in queues?\",\n",
    "    \"Can you recommend a good drink?\",\n",
    "    \"How do you handle disappointment?\",\n",
    "    \"What's your opinion on Americans?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"BRITISH PERSONALITY TEST\")\n",
    "print(f\"Layer: {BEST_LAYER} | Strength: {BEST_STRENGTH}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def extract_response(output):\n",
    "    \"\"\"Extract the generated text from CLI output.\"\"\"\n",
    "    lines = output.split(\"\\n\")\n",
    "    capture = False\n",
    "    response_lines = []\n",
    "    for line in lines:\n",
    "        if \"Unsteered baseline output:\" in line or \"Generated output:\" in line or \"Steered output:\" in line:\n",
    "            capture = True\n",
    "            continue\n",
    "        if capture:\n",
    "            if line.startswith(\"âœ…\") or line.strip() == \"\" or \"---\" in line:\n",
    "                if response_lines:\n",
    "                    break\n",
    "            else:\n",
    "                response_lines.append(line)\n",
    "    return \"\\n\".join(response_lines).strip()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT {i}: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Baseline (unsteered)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    baseline = extract_response(result.stdout)\n",
    "    print(f\"\\n[BASELINE]:\")\n",
    "    print(baseline[:500])\n",
    "    \n",
    "    # Steered (British)\n",
    "    result = subprocess.run([\n",
    "        \"python\", \"-m\", \"wisent.core.main\", \"multi-steer\",\n",
    "        \"--vector\", f\"{VECTOR_PATH}:{BEST_STRENGTH}\",\n",
    "        \"--model\", MODEL,\n",
    "        \"--layer\", str(BEST_LAYER),\n",
    "        \"--prompt\", prompt,\n",
    "        \"--max-new-tokens\", str(MAX_NEW_TOKENS)\n",
    "    ], capture_output=True, text=True)\n",
    "    steered = extract_response(result.stdout)\n",
    "    print(f\"\\n[BRITISH]:\")\n",
    "    print(steered[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export Modified Weights\n",
    "\n",
    "Use `modify-weights` to permanently bake the British steering into the model weights, creating `Llama-3.2-1B-Instruct-british`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export modified weights\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPORTING MODIFIED WEIGHTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Creating: {MODIFIED_MODEL_DIR}\")\n",
    "print(f\"Using optimized parameters:\")\n",
    "print(f\"  Layer: {BEST_LAYER}\")\n",
    "print(f\"  Strength: {BEST_STRENGTH}\")\n",
    "\n",
    "!python -m wisent.core.main modify-weights \\\n",
    "    --trait \"{TRAIT_DESCRIPTION}\" \\\n",
    "    --output-dir {MODIFIED_MODEL_DIR} \\\n",
    "    --model {MODEL} \\\n",
    "    --num-pairs {NUM_PAIRS} \\\n",
    "    --similarity-threshold 0.8 \\\n",
    "    --layers {BEST_LAYER} \\\n",
    "    --method abliteration \\\n",
    "    --strength {BEST_STRENGTH} \\\n",
    "    --components self_attn.o_proj mlp.down_proj \\\n",
    "    --use-kernel \\\n",
    "    --max-weight 1.5 \\\n",
    "    --max-weight-position 8.0 \\\n",
    "    --min-weight 0.3 \\\n",
    "    --min-weight-distance 6.0 \\\n",
    "    --normalize-vectors \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the modified model was created\n",
    "if MODIFIED_MODEL_DIR.exists():\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUCCESS! Modified model created:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Location: {MODIFIED_MODEL_DIR.absolute()}\")\n",
    "    print(f\"\\nFiles:\")\n",
    "    for f in MODIFIED_MODEL_DIR.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {f.name}: {size_mb:.1f} MB\")\n",
    "    print(f\"\\nTo use this model:\")\n",
    "    print(f'  model = AutoModelForCausalLM.from_pretrained(\"{MODIFIED_MODEL_DIR}\")')\n",
    "else:\n",
    "    print(f\"Model directory not found at {MODIFIED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Modified Model\n",
    "\n",
    "Load the exported model and verify the British personality is baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the modified British model\n",
    "print(\"Loading modified British model...\")\n",
    "british_tokenizer = AutoTokenizer.from_pretrained(str(MODIFIED_MODEL_DIR))\n",
    "british_model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODIFIED_MODEL_DIR),\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded on: {british_model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompt, max_new_tokens=200):\n",
    "    \"\"\"Generate a response from the model.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# Test the British model\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTING MODIFIED BRITISH MODEL (No steering needed - personality is baked in!)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in test_prompts[:3]:\n",
    "    print(f\"\\nPROMPT: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    response = generate_response(british_model, british_tokenizer, prompt)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}