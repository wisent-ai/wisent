{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Guard: Representation Reading for Truthfulness Detection\n",
    "\n",
    "This notebook demonstrates **representation reading** - training classifiers on a model's internal activations to detect hallucinations and untruthful responses.\n",
    "\n",
    "**Approach:** Instead of modifying the model or using steering, we train classifiers that \"read\" the model's internal representations to detect when it's likely generating untruthful content. This is a non-invasive monitoring approach.\n",
    "\n",
    "**Key Concept:** The model's hidden states contain information about whether it's generating truthful vs untruthful content. By training a classifier on these activations, we can detect hallucinations in real-time.\n",
    "\n",
    "## CLI Commands Used:\n",
    "- `generate-pairs-from-task`: Extract truthful vs untruthful pairs from TruthfulQA\n",
    "- `tasks`: Train an activation-based classifier (representation reading)\n",
    "- `generate-responses`: Generate responses for testing\n",
    "- `evaluate-responses`: Evaluate generated responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "TASK = \"truthfulqa_gen\"  # TruthfulQA generation task\n",
    "OUTPUT_DIR = \"./hallucination_guard_outputs\"\n",
    "LAYER = 8  # Layer for activation collection (middle layers often work best)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/pairs\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/classifiers\", exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/responses\", exist_ok=True)\n",
    "\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Layer for classification: {LAYER}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Contrastive Pairs from TruthfulQA\n",
    "\n",
    "TruthfulQA provides questions with:\n",
    "- **Truthful answers**: Factually correct, honest responses\n",
    "- **Untruthful answers**: Common misconceptions, false beliefs, hallucinations\n",
    "\n",
    "We use these to train our representation reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract contrastive pairs from TruthfulQA\n!python -m wisent.core.main generate-pairs-from-task \\\n    truthfulqa_gen \\\n    --output {OUTPUT_DIR}/pairs/truthfulqa_pairs.json \\\n    --limit 150 \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the extracted pairs\n",
    "with open(f\"{OUTPUT_DIR}/pairs/truthfulqa_pairs.json\", 'r') as f:\n",
    "    pairs_data = json.load(f)\n",
    "\n",
    "print(f\"Extracted {pairs_data['num_pairs']} contrastive pairs from {pairs_data['task_name']}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Show examples\n",
    "for i, pair in enumerate(pairs_data['pairs'][:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {pair['prompt'][:100]}...\")\n",
    "    print(f\"TRUTHFUL: {pair['positive_response']['model_response'][:80]}...\")\n",
    "    print(f\"UNTRUTHFUL: {pair['negative_response']['model_response'][:80]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Representation Reader (Classifier)\n",
    "\n",
    "The `tasks` command trains a classifier on the model's internal activations. This classifier learns to distinguish truthful from untruthful responses by reading the model's hidden states.\n",
    "\n",
    "**How it works:**\n",
    "1. Feed truthful and untruthful responses through the model\n",
    "2. Extract activations from a specific layer\n",
    "3. Train a classifier (logistic regression or MLP) on these activations\n",
    "4. The classifier learns to predict truthfulness from internal representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train logistic regression classifier on layer 8\n!python -m wisent.core.main tasks \\\n    truthfulqa_gen \\\n    --model {MODEL} \\\n    --layer {LAYER} \\\n    --classifier-type logistic \\\n    --token-aggregation average \\\n    --detection-threshold 0.5 \\\n    --split-ratio 0.8 \\\n    --limit 100 \\\n    --save-classifier {OUTPUT_DIR}/classifiers/truthfulness_classifier_layer8.pt \\\n    --output {OUTPUT_DIR}/classifiers \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "import glob\n",
    "\n",
    "# Find the training report\n",
    "report_files = glob.glob(f\"{OUTPUT_DIR}/classifiers/*report*.json\")\n",
    "if report_files:\n",
    "    with open(report_files[0], 'r') as f:\n",
    "        report = json.load(f)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CLASSIFIER TRAINING REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Task: {report.get('task', 'N/A')}\")\n",
    "    print(f\"  Layer: {report.get('layer', 'N/A')}\")\n",
    "    print(f\"  Classifier type: {report.get('classifier_type', 'N/A')}\")\n",
    "    \n",
    "    if 'metrics' in report:\n",
    "        metrics = report['metrics']\n",
    "        print(f\"\\nPerformance Metrics:\")\n",
    "        print(f\"  Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "        print(f\"  F1 Score: {metrics.get('f1_score', 0):.4f}\")\n",
    "        print(f\"  Precision: {metrics.get('precision', 0):.4f}\")\n",
    "        print(f\"  Recall: {metrics.get('recall', 0):.4f}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Training report not found. Check classifier output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Classifiers on Multiple Layers\n",
    "\n",
    "Different layers may capture different aspects of truthfulness. Let's train classifiers on multiple layers and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train classifiers on layers 4, 8, 12 (early, middle, late)\nLAYERS_TO_TEST = [4, 8, 12]\n\nfor layer in LAYERS_TO_TEST:\n    print(f\"\\n{'='*60}\")\n    print(f\"Training classifier for Layer {layer}\")\n    print(\"=\"*60)\n    \n    !python -m wisent.core.main tasks \\\n        truthfulqa_gen \\\n        --model {MODEL} \\\n        --layer {layer} \\\n        --classifier-type logistic \\\n        --token-aggregation average \\\n        --detection-threshold 0.5 \\\n        --split-ratio 0.8 \\\n        --limit 100 \\\n        --save-classifier {OUTPUT_DIR}/classifiers/truthfulness_classifier_layer{layer}.pt \\\n        --output {OUTPUT_DIR}/classifiers/layer{layer}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train MLP Classifier (More Expressive)\n",
    "\n",
    "For potentially better performance, we can use an MLP classifier instead of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train MLP classifier\n!python -m wisent.core.main tasks \\\n    truthfulqa_gen \\\n    --model {MODEL} \\\n    --layer {LAYER} \\\n    --classifier-type mlp \\\n    --token-aggregation average \\\n    --detection-threshold 0.5 \\\n    --split-ratio 0.8 \\\n    --limit 100 \\\n    --save-classifier {OUTPUT_DIR}/classifiers/truthfulness_classifier_mlp.pt \\\n    --output {OUTPUT_DIR}/classifiers/mlp \\\n    --verbose"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Test Responses\n",
    "\n",
    "Generate responses from the model to test our hallucination detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate responses on TruthfulQA test questions\n!python -m wisent.core.main generate-responses \\\n    {MODEL} \\\n    --task truthfulqa_gen \\\n    --output {OUTPUT_DIR}/responses/generated_responses.json \\\n    --num-questions 20 \\\n    --max-new-tokens 100 \\\n    --temperature 0.7 \\\n    --verbose"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated responses\n",
    "with open(f\"{OUTPUT_DIR}/responses/generated_responses.json\", 'r') as f:\n",
    "    responses = json.load(f)\n",
    "\n",
    "print(f\"Generated {len(responses['responses'])} responses\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "for i, resp in enumerate(responses['responses'][:3]):\n",
    "    print(f\"\\nQuestion {i+1}:\")\n",
    "    print(f\"Prompt: {resp['prompt'][:80]}...\")\n",
    "    print(f\"Generated: {resp['generated_response'][:100]}...\")\n",
    "    print(f\"Reference (truthful): {resp['positive_reference'][:80]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Responses\n",
    "\n",
    "Use TruthfulQA's evaluator to assess the truthfulness of generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate generated responses\n",
    "!python -m wisent.core.main evaluate-responses \\\n",
    "    --input {OUTPUT_DIR}/responses/generated_responses.json \\\n",
    "    --output {OUTPUT_DIR}/responses/evaluation_results.json \\\n",
    "    --task truthfulqa_gen \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "try:\n",
    "    with open(f\"{OUTPUT_DIR}/responses/evaluation_results.json\", 'r') as f:\n",
    "        eval_results = json.load(f)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Task: {eval_results.get('task', 'N/A')}\")\n",
    "    print(f\"  Evaluator: {eval_results.get('evaluator_used', 'N/A')}\")\n",
    "    print(f\"  Total evaluated: {eval_results.get('num_evaluated', 0)}\")\n",
    "    \n",
    "    metrics = eval_results.get('aggregated_metrics', {})\n",
    "    print(f\"\\nAggregated Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {value}\")\n",
    "    print(\"=\"*60)\n",
    "except FileNotFoundError:\n",
    "    print(\"Evaluation results not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Visualize Token-Level Hallucination Scores\n\nThe classifier outputs a hallucination probability for **each token** in the generated response. We can visualize this as a heatmap where:\n- **Green** = Likely truthful (low hallucination score)\n- **Yellow** = Uncertain\n- **Red** = Likely hallucinating (high hallucination score)\n\nThis helps identify exactly **where** in a response the model starts hallucinating.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom transformers import AutoTokenizer\nfrom IPython.display import Image as IPImage\n\n# Load tokenizer for the model\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# Path to generation details (created by the tasks command)\ngeneration_details_path = f\"{OUTPUT_DIR}/classifiers/generation_details.json\"\n\nprint(f\"Tokenizer loaded: {MODEL}\")\nprint(f\"Generation details path: {generation_details_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def save_hallucination_heatmap_png(text, scores, output_path, tokenizer, \n                                    title=\"Hallucination Heatmap\", words_per_row=12):\n    \"\"\"Save word-level hallucination scores as a clean PNG heatmap.\n    \n    Args:\n        text: The generated response text\n        scores: List of per-token hallucination scores (0-1)\n        output_path: Path to save the PNG file\n        tokenizer: The tokenizer used for the model\n        title: Title for the plot\n        words_per_row: Number of words per row in the visualization\n    \"\"\"\n    # Split into words\n    words = text.split()\n    \n    # Map token scores to words by averaging scores for tokens in each word\n    tokens = tokenizer.tokenize(text)\n    word_scores = []\n    token_idx = 0\n    \n    for word in words:\n        word_tokens = tokenizer.tokenize(word)\n        num_tokens = len(word_tokens)\n        \n        if token_idx + num_tokens <= len(scores):\n            word_score = np.mean(scores[token_idx:token_idx + num_tokens])\n        else:\n            word_score = scores[token_idx] if token_idx < len(scores) else 0.5\n        \n        word_scores.append(word_score)\n        token_idx += num_tokens\n    \n    # Create visualization\n    num_words = len(words)\n    num_rows = (num_words + words_per_row - 1) // words_per_row\n    \n    fig_height = max(4, num_rows * 1.0 + 2)\n    fig, ax = plt.subplots(figsize=(16, fig_height))\n    \n    cmap = plt.cm.RdYlGn_r  # Red = hallucinating, Green = truthful\n    \n    y_pos = num_rows - 1\n    x_pos = 0\n    \n    for word, score in zip(words, word_scores):\n        color = cmap(score)\n        \n        rect = plt.Rectangle((x_pos, y_pos), 1, 0.7, \n                             facecolor=color, edgecolor='white', linewidth=1)\n        ax.add_patch(rect)\n        \n        display_word = word[:12] if len(word) > 12 else word\n        text_color = 'black' if score < 0.65 else 'white'\n        ax.text(x_pos + 0.5, y_pos + 0.35, display_word,\n               ha='center', va='center', fontsize=8,\n               color=text_color, fontweight='bold')\n        \n        x_pos += 1\n        if x_pos >= words_per_row:\n            x_pos = 0\n            y_pos -= 1\n    \n    ax.set_xlim(0, words_per_row)\n    ax.set_ylim(-0.5, num_rows)\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n    \n    # Add colorbar legend\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(0, 1))\n    sm.set_array([])\n    cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', \n                        fraction=0.04, pad=0.08, aspect=50)\n    cbar.set_label('Hallucination Probability', fontsize=11)\n    cbar.set_ticks([0, 0.5, 1.0])\n    cbar.set_ticklabels(['Truthful', 'Uncertain', 'Hallucinating'])\n    \n    plt.tight_layout()\n    \n    os.makedirs(os.path.dirname(os.path.abspath(output_path)) or '.', exist_ok=True)\n    plt.savefig(output_path, dpi=150, bbox_inches='tight', \n                facecolor='white', edgecolor='none')\n    plt.close()\n    \n    print(f\"Saved heatmap to: {output_path}\")\n    return output_path\n\nprint(\"PNG export function loaded!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Export Visualizations as PNG\n\nSave the token-level hallucination heatmaps as PNG images for reports or presentations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Export visualizations as PNG images\nos.makedirs(f\"{OUTPUT_DIR}/visualizations\", exist_ok=True)\n\nif os.path.exists(generation_details_path):\n    with open(generation_details_path, 'r') as f:\n        data = json.load(f)\n    \n    generations = data.get('generations', [])\n    \n    print(\"Exporting hallucination heatmaps as PNG...\")\n    print(\"=\"*70)\n    \n    # Export first 2 examples\n    for i, gen in enumerate(generations[:2]):\n        response = gen.get('response', gen.get('original_response', ''))\n        token_scores = gen.get('token_scores', [])\n        eval_result = gen.get('eval_result', 'N/A')\n        classifier_proba = gen.get('classifier_proba', 0)\n        \n        if not token_scores:\n            continue\n        \n        title = f\"Example {i+1}: {eval_result} ({classifier_proba:.1%} hallucination prob.)\"\n        output_path = f\"{OUTPUT_DIR}/visualizations/hallucination_heatmap_{i+1}.png\"\n        save_hallucination_heatmap_png(response, token_scores, output_path, tokenizer, title=title)\n    \n    print(f\"\\nPNG files saved to: {OUTPUT_DIR}/visualizations/\")\nelse:\n    print(\"Run classifier training first to generate token scores.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Display one of the saved PNG images\nfrom IPython.display import Image as IPImage\n\npng_path = f\"{OUTPUT_DIR}/visualizations/hallucination_heatmap_1.png\"\nif os.path.exists(png_path):\n    print(\"Example PNG visualization:\")\n    display(IPImage(filename=png_path))\nelse:\n    print(\"PNG not found. Run the export cell above first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Summary: CLI Commands Reference\n\n### Extract Contrastive Pairs\n```bash\npython -m wisent.core.main generate-pairs-from-task \\\n    truthfulqa_gen \\\n    --output pairs.json \\\n    --limit 100\n```\n\n### Train Representation Reader (Classifier)\n```bash\npython -m wisent.core.main tasks \\\n    truthfulqa_gen \\\n    --model meta-llama/Llama-3.2-1B-Instruct \\\n    --layer 8 \\\n    --classifier-type logistic \\\n    --token-aggregation average \\\n    --save-classifier classifier.pt \\\n    --output ./classifiers\n```\n\n### Generate Responses for Testing\n```bash\npython -m wisent.core.main generate-responses \\\n    meta-llama/Llama-3.2-1B-Instruct \\\n    --task truthfulqa_gen \\\n    --output responses.json \\\n    --num-questions 20\n```\n\n### Evaluate Responses\n```bash\npython -m wisent.core.main evaluate-responses \\\n    --input responses.json \\\n    --output evaluation.json \\\n    --task truthfulqa_gen\n```\n\n### Key Parameters:\n- **`--layer`**: Which transformer layer to read activations from (middle layers often work best)\n- **`--classifier-type`**: `logistic` (simpler, faster) or `mlp` (more expressive)\n- **`--token-aggregation`**: How to combine token activations (`average`, `last`, `first`)\n- **`--detection-threshold`**: Classification threshold (0.5 = balanced)\n\n### Output Files:\n- **`training_report.json`**: Classifier metrics (accuracy, F1, precision, recall, AUC)\n- **`generation_details.json`**: Per-response and per-token hallucination scores\n- **`classifier.pt`**: Saved classifier model for reuse\n\n### Token-Level Scores:\nThe classifier outputs a hallucination probability for each token:\n- **`classifier_proba`**: Overall response-level hallucination probability (0-1)\n- **`token_scores`**: Array of per-token hallucination probabilities\n\n### Key Concepts:\n- **Representation Reading**: Using internal activations to detect model behavior\n- **Non-invasive**: Does not modify the model, only monitors it\n- **Real-time**: Can be used during inference to flag potential hallucinations\n- **Interpretable**: Token-level scores show exactly where hallucinations begin\n- **Visualizable**: Heatmaps make it easy to identify problematic regions"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}