{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.2-1B-Instruct...\n",
      "Model loaded!\n",
      "Number of layers: 16\n",
      "Hidden dimension: 2048\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from wisent.core.models.wisent_model import WisentModel\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load model and tokenizer using WisentModel for consistent settings\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "wisent_model = WisentModel(model_name=MODEL_NAME)\n",
    "model = wisent_model.hf_model\n",
    "tokenizer = wisent_model.tokenizer\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden dimension: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Activation Shape\n",
    "\n",
    "Let's pass a simple prompt through the model and examine the activation tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: 'The capital of France is'\n",
      "Token IDs: [128000, 791, 6864, 315, 9822, 374]\n",
      "Tokens: ['<|begin_of_text|>', 'The', ' capital', ' of', ' France', ' is']\n",
      "Number of tokens: 6\n"
     ]
    }
   ],
   "source": [
    "# A simple prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(f\"Input prompt: '{prompt}'\")\n",
    "print(f\"Token IDs: {inputs.input_ids[0].tolist()}\")\n",
    "print(f\"Tokens: {[tokenizer.decode([t]) for t in inputs.input_ids[0]]}\")\n",
    "print(f\"Number of tokens: {inputs.input_ids.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hidden state tensors: 17\n",
      "(This is 1 embedding layer + 16 transformer layers)\n",
      "\n",
      "Layer 8 activations:\n",
      "  Shape: torch.Size([1, 6, 2048])\n",
      "  - Batch size: 1\n",
      "  - Sequence length: 6\n",
      "  - Hidden dimension: 2048\n"
     ]
    }
   ],
   "source": [
    "# Forward pass to get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# hidden_states is a tuple: (embedding_output, layer_1, layer_2, ..., layer_n)\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(f\"Number of hidden state tensors: {len(hidden_states)}\")\n",
    "print(f\"(This is 1 embedding layer + {len(hidden_states)-1} transformer layers)\")\n",
    "print()\n",
    "\n",
    "# Examine a specific layer's activations\n",
    "layer_idx = 8  # Middle layer\n",
    "layer_activations = hidden_states[layer_idx]\n",
    "\n",
    "print(f\"Layer {layer_idx} activations:\")\n",
    "print(f\"  Shape: {layer_activations.shape}\")\n",
    "print(f\"  - Batch size: {layer_activations.shape[0]}\")\n",
    "print(f\"  - Sequence length: {layer_activations.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {layer_activations.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In representation engineering, we often care about activations at specific positions:\n",
    "\n",
    "- **Last token**: The most common choice - represents the \"state\" of the model after processing the full context\n",
    "- **Mean pooling**: Average across all tokens for a more holistic representation\n",
    "- **Specific positions**: Sometimes we want activations at particular token positions\n",
    "\n",
    "### Getting the Last Token Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last token activation for layer 8:\n",
      "  Shape: torch.Size([2048])\n",
      "  This is a 2048-dimensional vector!\n",
      "\n",
      "  First 10 values: [0.025029459968209267, -0.08347947895526886, -0.09321831911802292, -0.001393824815750122, -0.1272227019071579, -0.0920216366648674, 0.010746791958808899, -0.12839482724666595, 0.03147208318114281, 0.0033951252698898315]\n",
      "  L2 Norm: 5.5896\n",
      "  Mean: 0.0032\n",
      "  Std: 0.1235\n"
     ]
    }
   ],
   "source": [
    "# Get activation at the last token position for layer 8\n",
    "layer_idx = 8\n",
    "last_token_idx = -1  # Last position\n",
    "\n",
    "# Extract: [batch, seq, hidden] -> [hidden] for last token\n",
    "last_token_activation = hidden_states[layer_idx][0, last_token_idx, :]\n",
    "\n",
    "print(f\"Last token activation for layer {layer_idx}:\")\n",
    "print(f\"  Shape: {last_token_activation.shape}\")\n",
    "print(f\"  This is a {last_token_activation.shape[0]}-dimensional vector!\")\n",
    "print()\n",
    "print(f\"  First 10 values: {last_token_activation[:10].tolist()}\")\n",
    "print(f\"  L2 Norm: {torch.norm(last_token_activation).item():.4f}\")\n",
    "print(f\"  Mean: {last_token_activation.mean().item():.4f}\")\n",
    "print(f\"  Std: {last_token_activation.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Pooling Across Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-pooled activation for layer 8:\n",
      "  Shape: torch.Size([2048])\n",
      "  L2 Norm: 132.4203\n",
      "\n",
      "Cosine similarity between last-token and mean-pooled: 0.0050\n"
     ]
    }
   ],
   "source": [
    "# Mean pooling: average across all token positions\n",
    "mean_activation = hidden_states[layer_idx][0].mean(dim=0)  # [seq, hidden] -> [hidden]\n",
    "\n",
    "print(f\"Mean-pooled activation for layer {layer_idx}:\")\n",
    "print(f\"  Shape: {mean_activation.shape}\")\n",
    "print(f\"  L2 Norm: {torch.norm(mean_activation).item():.4f}\")\n",
    "\n",
    "# Compare with last token\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "    last_token_activation.unsqueeze(0),\n",
    "    mean_activation.unsqueeze(0)\n",
    ").item()\n",
    "print(f\"\\nCosine similarity between last-token and mean-pooled: {cosine_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerActivations object:\n",
      "  Number of layers: 16\n",
      "  Layer names: ['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5']... (showing first 5)\n",
      "  Layer 8 shape: torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "from wisent.core.activations.core.atoms import LayerActivations, ActivationAggregationStrategy\n",
    "\n",
    "# Create a LayerActivations object manually\n",
    "# In practice, wisent's ActivationsCollector does this for you\n",
    "layer_activations_dict = {\n",
    "    f\"layer_{i}\": hidden_states[i][0, -1, :]  # Last token for each layer\n",
    "    for i in range(1, len(hidden_states))  # Skip embedding layer\n",
    "}\n",
    "\n",
    "activations = LayerActivations(layer_activations_dict)\n",
    "\n",
    "print(f\"LayerActivations object:\")\n",
    "print(f\"  Number of layers: {len(activations)}\")\n",
    "print(f\"  Layer names: {list(activations.keys())[:5]}... (showing first 5)\")\n",
    "print(f\"  Layer 8 shape: {activations['layer_8'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸŽ¯ Generating Steering Vector from Synthetic Pairs (Full Pipeline)\n",
      "============================================================\n",
      "   Trait: happy, joyful, optimistic, cheerful responses full of positivity and enthusiasm\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   Num Pairs: 10\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Step 1/3: Generating synthetic contrastive pairs...\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸŽ¨ Generating synthetic contrastive pairs\n",
      "   Trait: happy, joyful, optimistic, cheerful responses full of positivity and enthusiasm\n",
      "   Number of pairs: 10\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "\n",
      "ðŸ¤– Loading model 'meta-llama/Llama-3.2-1B-Instruct'...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "   âœ“ Model loaded with 16 layers\n",
      "\n",
      "ðŸ§¹ Setting up cleaning pipeline...\n",
      "\n",
      "âš™ï¸  Initializing generator...\n",
      "\n",
      "ðŸŽ¯ Generating 10 contrastive pairs...\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "   âœ“ Generated 10 pairs\n",
      "\n",
      "ðŸ“Š Generation Report:\n",
      "   Requested: 10\n",
      "   Kept after dedupe: 10\n",
      "   Retries for refusals: 0\n",
      "   Diversity:\n",
      "     â€¢ Unique unigrams: 0.547\n",
      "     â€¢ Unique bigrams: 0.823\n",
      "     â€¢ Avg Jaccard: 0.157\n",
      "\n",
      "ðŸ’¾ Saving pairs to '/var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmpn9innrf3_pairs.json'...\n",
      "   âœ“ Saved 10 pairs to: /var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmpn9innrf3_pairs.json\n",
      "\n",
      "âœ… Synthetic pair generation completed successfully!\n",
      "\n",
      "\n",
      "âœ“ Step 1 complete: Pairs saved to /var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmpn9innrf3_pairs.json\n",
      "============================================================\n",
      "Step 2/3: Collecting activations from pairs...\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸŽ¨ Collecting activations from contrastive pairs\n",
      "   Input file: /var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmpn9innrf3_pairs.json\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "\n",
      "ðŸ“‚ Loading contrastive pairs...\n",
      "   âœ“ Loaded 10 pairs\n",
      "\n",
      "ðŸ¤– Loading model 'meta-llama/Llama-3.2-1B-Instruct'...\n",
      "   âœ“ Model loaded with 16 layers\n",
      "\n",
      "ðŸŽ¯ Collecting activations from 1 layer(s): [8]\n",
      "   Token aggregation: average (MEAN_POOLING)\n",
      "   Prompt strategy: chat_template\n",
      "\n",
      "âš¡ Collecting activations...\n",
      "   Processing pair 1/10...\n",
      "   Processing pair 2/10...\n",
      "   Processing pair 3/10...\n",
      "   Processing pair 4/10...\n",
      "   Processing pair 5/10...\n",
      "   Processing pair 6/10...\n",
      "   Processing pair 7/10...\n",
      "   Processing pair 8/10...\n",
      "   Processing pair 9/10...\n",
      "   Processing pair 10/10...\n",
      "   âœ“ Collected activations for 10 pairs\n",
      "\n",
      "ðŸ’¾ Saving enriched pairs to '/var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmp34o0ju15_enriched.json'...\n",
      "   âœ“ Saved enriched pairs to: /var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmp34o0ju15_enriched.json\n",
      "\n",
      "âœ… Activation collection completed successfully!\n",
      "\n",
      "\n",
      "âœ“ Step 2 complete: Enriched pairs saved to /var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmp34o0ju15_enriched.json\n",
      "\n",
      "============================================================\n",
      "Step 3/3: Creating steering vector...\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸŽ¯ Creating steering vectors from enriched pairs\n",
      "   Input file: /var/folders/4m/g5zcy_y57jgfk_cg9dqt10w00000gn/T/tmp34o0ju15_enriched.json\n",
      "   Method: caa\n",
      "\n",
      "ðŸ“‚ Loading enriched pairs...\n",
      "   âœ“ Loaded 10 pairs\n",
      "   âœ“ Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   âœ“ Layers: [8]\n",
      "   âœ“ Token aggregation: average\n",
      "\n",
      "ðŸ“Š Organizing activations by layer...\n",
      "   âœ“ Found activations for 1 layers: ['8']\n",
      "\n",
      "ðŸ§  Initializing CAA steering method...\n",
      "   âœ“ Method initialized (normalize=True)\n",
      "\n",
      "âš¡ Generating steering vectors...\n",
      "   Processing layer 8: 10 positive, 10 negative\n",
      "   âœ“ Generated 1 steering vectors\n",
      "\n",
      "ðŸ“Š Vector Quality Analysis:\n",
      "   Overall quality: GOOD\n",
      "   Convergence: 0.926\n",
      "   Cross-validation: 0.478\n",
      "   Signal-to-noise: 39.92\n",
      "   PCA PC1 variance: 17.8%\n",
      "   Held-out transfer: 0.435\n",
      "   CV classification: 0.950\n",
      "   Cohen's d: 5.71\n",
      "\n",
      "âš ï¸  Quality Issues:\n",
      "   âš ï¸ [warning] Low cross-validation score: 0.478. Vector may not generalize well.\n",
      "\n",
      "ðŸ’¾ Saving steering vectors to './steering_outputs/vectors/happy_vector.pt'...\n",
      "   âœ“ Saved steering vector (layer 8) to: ./steering_outputs/vectors/happy_vector.pt\n",
      "\n",
      "ðŸ“ˆ Steering Vector Statistics:\n",
      "   Layer 8: dim=2048, norm=1.0000\n",
      "\n",
      "âœ… Steering vector creation completed successfully!\\n\n",
      "\n",
      "âœ“ Step 3 complete: Steering vector saved to ./steering_outputs/vectors/happy_vector.pt\n",
      "\n",
      "\n",
      "ðŸ§¹ Cleaning up intermediate files...\n",
      "   âœ“ Removed temporary pairs file\n",
      "   âœ“ Removed temporary enriched file\n",
      "\n",
      "============================================================\n",
      "âœ… Full Pipeline Completed Successfully!\n",
      "============================================================\n",
      "   Final steering vector: ./steering_outputs/vectors/happy_vector.pt\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = \"./steering_outputs\"\n",
    "LAYER = 8\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/vectors\", exist_ok=True)\n",
    "\n",
    "# Generate a \"happy\" steering vector using CLI\n",
    "# This will:\n",
    "# 1. Generate synthetic positive (happy) and negative (sad) response pairs\n",
    "# 2. Collect activations from both\n",
    "# 3. Compute steering_vector = mean(positive) - mean(negative)\n",
    "\n",
    "HAPPY_TRAIT = \"happy, joyful, optimistic, cheerful responses full of positivity and enthusiasm\"\n",
    "\n",
    "!python -m wisent.core.main generate-vector-from-synthetic \\\n",
    "    --trait \"{HAPPY_TRAIT}\" \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --num-pairs 10 \\\n",
    "    --layers {LAYER} \\\n",
    "    --output {OUTPUT_DIR}/vectors/happy_vector.pt \\\n",
    "    --normalize \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded steering vector:\n",
      "  Shape: torch.Size([2048])\n",
      "  L2 norm: 1.0000\n",
      "  First 10 values: [0.01185621414333582, -0.014781808480620384, 0.002184569602832198, -0.01078332681208849, -0.004495644476264715, 0.024912234395742416, 0.022735361009836197, 0.02050434984266758, 0.01592581532895565, 0.0009902934543788433]\n"
     ]
    }
   ],
   "source": [
    "# Load the generated steering vector\n",
    "steering_data = torch.load(f\"{OUTPUT_DIR}/vectors/happy_vector.pt\")\n",
    "steering_vector_normalized = steering_data[\"vector\"]\n",
    "\n",
    "print(f\"Loaded steering vector:\")\n",
    "print(f\"  Shape: {steering_vector_normalized.shape}\")\n",
    "print(f\"  L2 norm: {torch.norm(steering_vector_normalized).item():.4f}\")\n",
    "print(f\"  First 10 values: {steering_vector_normalized[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SteeringHook class defined!\n",
      "This modifies activations during the forward pass.\n"
     ]
    }
   ],
   "source": [
    "class SteeringHook:\n",
    "    \"\"\"A hook that adds a steering vector to activations.\"\"\"\n",
    "    \n",
    "    def __init__(self, steering_vector, alpha=1.0):\n",
    "        self.steering_vector = steering_vector\n",
    "        self.alpha = alpha\n",
    "        self.handle = None\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        \"\"\"Called during forward pass. Modifies the output activation.\"\"\"\n",
    "        # output is typically (hidden_states, ...)\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        \n",
    "        # Add steering vector (broadcast across batch and sequence)\n",
    "        # steering_vector: [hidden_dim] -> [1, 1, hidden_dim]\n",
    "        sv = self.steering_vector.to(hidden_states.device).to(hidden_states.dtype)\n",
    "        sv = sv.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        modified = hidden_states + self.alpha * sv\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            return (modified,) + output[1:]\n",
    "        return modified\n",
    "    \n",
    "    def attach(self, layer):\n",
    "        \"\"\"Attach the hook to a layer.\"\"\"\n",
    "        self.handle = layer.register_forward_hook(self)\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook.\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "\n",
    "print(\"SteeringHook class defined!\")\n",
    "print(\"This modifies activations during the forward pass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE (No Steering)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Unsteered Baseline Generation Mode\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   No steering vectors provided - generating baseline response\n",
      "\n",
      "ðŸ¤– Loading model 'meta-llama/Llama-3.2-1B-Instruct'...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "Unsteered baseline output:\n",
      "It seems like you started your day with a thought. You mentioned feeling something, but didn't specify what it was. Would you like to share more about how you're feeling now? Sometimes talking about our emotions can help clarify them.\n",
      "\n",
      "\n",
      "âœ… Baseline generation completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline (no steering)\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE (No Steering)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "!python -m wisent.core.main multi-steer \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --prompt \"Today I woke up and felt\" \\\n",
    "    --max-new-tokens 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POSITIVE STEERING (strength=1.5 -> happy)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Multi-Steering Mode\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   Layer: 8\n",
      "   Method: CAA\n",
      "Loading vector from ./steering_outputs/vectors/happy_vector.pt with weight 1.5\n",
      "   âœ“ Loaded vector from layer 8\n",
      "\n",
      "Using CAA method for vector combination\n",
      "Target layer: 8\n",
      "\n",
      "ðŸ”„ Combining 1 vectors\n",
      "   âœ“ Combined vector shape: torch.Size([2048])\n",
      "   âœ“ Combined vector norm: 1.0000\n",
      "   âœ“ Steering scale: 1.5000\n",
      "\n",
      "ðŸ¤– Loading model 'meta-llama/Llama-3.2-1B-Instruct'...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "ðŸŽ¯ Applying combined steering vector at layer 8\n",
      "Prompt: Today I woke up and felt\n",
      "Prompt is formatted: False\n",
      "==================================================\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "Generated output:\n",
      "I'd love to help you explore your feelings. What's been going on lately that's made you feel something new? Is it a specific experience, emotion, or thought?\n",
      "\n",
      "We can dive into this together! What do you think might be the first\n",
      "\n",
      "\n",
      "âœ… Multi-steering completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Positive steering (toward happy)\n",
    "print(\"=\" * 60)\n",
    "print(\"POSITIVE STEERING (strength=1.5 -> happy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    f'python -m wisent.core.main multi-steer '\n",
    "    f'--vector {OUTPUT_DIR}/vectors/happy_vector.pt:1.5 '\n",
    "    f'--model {MODEL_NAME} '\n",
    "    f'--layer {LAYER} '\n",
    "    f'--prompt \"Today I woke up and felt\" '\n",
    "    f'--max-new-tokens 50',\n",
    "    shell=True, capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "happy_output = result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NEGATIVE STEERING (strength=-1.5 -> sad)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Multi-Steering Mode\n",
      "   Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "   Layer: 8\n",
      "   Method: CAA\n",
      "Loading vector from ./steering_outputs/vectors/happy_vector.pt with weight -1.5\n",
      "   âœ“ Loaded vector from layer 8\n",
      "\n",
      "Using CAA method for vector combination\n",
      "Target layer: 8\n",
      "\n",
      "ðŸ”„ Combining 1 vectors\n",
      "   âœ“ Combined vector shape: torch.Size([2048])\n",
      "   âœ“ Combined vector norm: 1.0000\n",
      "   âœ“ Steering scale: -1.5000\n",
      "\n",
      "ðŸ¤– Loading model 'meta-llama/Llama-3.2-1B-Instruct'...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\n",
      "ðŸŽ¯ Applying combined steering vector at layer 8\n",
      "Prompt: Today I woke up and felt\n",
      "Prompt is formatted: False\n",
      "==================================================\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "Generated output:\n",
      "I can provide general responses to help you complete your sentence. Would you like me to continue?\n",
      "\n",
      "\n",
      "âœ… Multi-steering completed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Negative steering (toward sad)\n",
    "print(\"=\" * 60)\n",
    "print(\"NEGATIVE STEERING (strength=-1.5 -> sad)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = subprocess.run(\n",
    "    f'python -m wisent.core.main multi-steer '\n",
    "    f'--vector {OUTPUT_DIR}/vectors/happy_vector.pt:-1.5 '\n",
    "    f'--model {MODEL_NAME} '\n",
    "    f'--layer {LAYER} '\n",
    "    f'--prompt \"Today I woke up and felt\" '\n",
    "    f'--max-new-tokens 50',\n",
    "    shell=True, capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout)\n",
    "sad_output = result.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: Happy vs Sad Steering\n",
      "============================================================\n",
      "\n",
      "HAPPY (strength=1.5):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'happy_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHAPPY (strength=1.5):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(extract_generated_output(\u001b[43mhappy_output\u001b[49m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAD (strength=-1.5):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'happy_output' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_generated_output(output):\n",
    "    \"\"\"Extract just the generated text from CLI output.\"\"\"\n",
    "    match = re.search(r'Generated output:\\n(.+?)\\n\\n', output, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return output\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Happy vs Sad Steering\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"HAPPY (strength=1.5):\")\n",
    "print(\"-\" * 40)\n",
    "print(extract_generated_output(happy_output))\n",
    "print()\n",
    "print(\"SAD (strength=-1.5):\")\n",
    "print(\"-\" * 40)\n",
    "print(extract_generated_output(sad_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Activations from a Single Pair\n",
    "\n",
    "Let's see how to extract activations from one contrastive pair using the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pair to ./steering_outputs/single_pair.json\n",
      "Prompt: How are you feeling today?\n",
      "Positive: I'm feeling absolutely wonderful! Everything is going great!\n",
      "Negative: I'm feeling terrible. Everything is going wrong.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Create a single contrastive pair\n",
    "pair = {\n",
    "    \"pairs\": [\n",
    "        {\n",
    "            \"prompt\": \"How are you feeling today?\",\n",
    "            \"positive_response\": {\"model_response\": \"I'm feeling absolutely wonderful! Everything is going great!\"},\n",
    "            \"negative_response\": {\"model_response\": \"I'm feeling terrible. Everything is going wrong.\"}\n",
    "        }\n",
    "    ],\n",
    "    \"num_pairs\": 1\n",
    "}\n",
    "\n",
    "# Save the pair to a file\n",
    "pairs_file = f\"{OUTPUT_DIR}/single_pair.json\"\n",
    "with open(pairs_file, \"w\") as f:\n",
    "    json.dump(pair, f, indent=2)\n",
    "\n",
    "print(f\"Saved pair to {pairs_file}\")\n",
    "print(f\"Prompt: {pair['pairs'][0]['prompt']}\")\n",
    "print(f\"Positive: {pair['pairs'][0]['positive_response']['model_response']}\")\n",
    "print(f\"Negative: {pair['pairs'][0]['negative_response']['model_response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: main.py [-h] [--verbose]\n",
      "               {tasks,generate-pairs,diagnose-pairs,generate-pairs-from-task,get-activations,diagnose-vectors,create-steering-vector,generate-vector-from-task,generate-vector-from-synthetic,synthetic,test-nonsense,monitor,agent,model-config,configure-model,optimize-classification,optimize-steering,optimize-sample-size,optimize-all,optimize,generate-vector,multi-steer,evaluate,generate-responses,evaluate-responses,modify-weights,evaluate-refusal,inference-config,optimization-cache,optimize-weights,train-unified-goodness,check-linearity}\n",
      "               ...\n",
      "main.py: error: unrecognized arguments: --pairs-file\n"
     ]
    }
   ],
   "source": [
    "# Extract activations using CLI\n",
    "enriched_file = f\"{OUTPUT_DIR}/single_pair_enriched.json\"\n",
    "\n",
    "!python -m wisent.core.main get-activations \\\n",
    "    {pairs_file} \\\n",
    "    --model {MODEL_NAME} \\\n",
    "    --layers {LAYER} \\\n",
    "    --token-aggregation final \\\n",
    "    --output {enriched_file} \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './steering_outputs/single_pair_enriched.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load and inspect the activations\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menriched_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     enriched \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      5\u001b[0m pair_data \u001b[38;5;241m=\u001b[39m enriched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpairs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './steering_outputs/single_pair_enriched.json'"
     ]
    }
   ],
   "source": [
    "# Load and inspect the activations\n",
    "with open(enriched_file, \"r\") as f:\n",
    "    enriched = json.load(f)\n",
    "\n",
    "pair_data = enriched[\"pairs\"][0]\n",
    "pos_activations = pair_data[\"positive_response\"][\"activations\"]\n",
    "neg_activations = pair_data[\"negative_response\"][\"activations\"]\n",
    "\n",
    "print(\"Positive response activations:\")\n",
    "print(f\"  Layers: {list(pos_activations.keys())}\")\n",
    "print(f\"  Layer {LAYER} shape: {len(pos_activations[str(LAYER)])} dimensions\")\n",
    "print(f\"  First 10 values: {pos_activations[str(LAYER)][:10]}\")\n",
    "\n",
    "print(\"\\nNegative response activations:\")\n",
    "print(f\"  Layers: {list(neg_activations.keys())}\")\n",
    "print(f\"  Layer {LAYER} shape: {len(neg_activations[str(LAYER)])} dimensions\")\n",
    "print(f\"  First 10 values: {neg_activations[str(LAYER)][:10]}\")\n",
    "\n",
    "# Compute difference (this is what CAA uses)\n",
    "import numpy as np\n",
    "pos_vec = np.array(pos_activations[str(LAYER)])\n",
    "neg_vec = np.array(neg_activations[str(LAYER)])\n",
    "diff = pos_vec - neg_vec\n",
    "\n",
    "print(\"\\nDifference vector (positive - negative):\")\n",
    "print(f\"  Shape: {diff.shape}\")\n",
    "print(f\"  Norm: {np.linalg.norm(diff):.4f}\")\n",
    "print(f\"  First 10 values: {diff[:10].tolist()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
