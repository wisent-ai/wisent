{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Mathematical Structure\n\nIn a transformer model, **activations** are the intermediate representations computed at each layer. When you pass text through a model, each token gets transformed into a high-dimensional vector at every layer.\n\nFor a given input:\n- **Input tokens**: `[token_1, token_2, ..., token_n]`\n- **At each layer L**, we get activations of shape: `[batch_size, sequence_length, hidden_dim]`\n\nFor example, with Llama-3.2-1B:\n- `batch_size = 1` (single prompt)\n- `sequence_length = n` (number of tokens)\n- `hidden_dim = 2048` (the model's hidden dimension)\n\nSo each token at each layer is represented as a 2048-dimensional vector!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from wisent.core.models.wisent_model import WisentModel\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Load model and tokenizer using WisentModel for consistent settings\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "wisent_model = WisentModel(model_name=MODEL_NAME)\n",
    "model = wisent_model.hf_model\n",
    "tokenizer = wisent_model.tokenizer\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Hidden dimension: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualizing Activation Shape\n\nLet's pass a simple prompt through the model and examine the activation tensors."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(f\"Input prompt: '{prompt}'\")\n",
    "print(f\"Token IDs: {inputs.input_ids[0].tolist()}\")\n",
    "print(f\"Tokens: {[tokenizer.decode([t]) for t in inputs.input_ids[0]]}\")\n",
    "print(f\"Number of tokens: {inputs.input_ids.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "# hidden_states is a tuple: (embedding_output, layer_1, layer_2, ..., layer_n)\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "print(f\"Number of hidden state tensors: {len(hidden_states)}\")\n",
    "print(f\"(This is 1 embedding layer + {len(hidden_states)-1} transformer layers)\")\n",
    "print()\n",
    "\n",
    "# Examine a specific layer's activations\n",
    "layer_idx = 8  # Middle layer\n",
    "layer_activations = hidden_states[layer_idx]\n",
    "\n",
    "print(f\"Layer {layer_idx} activations:\")\n",
    "print(f\"  Shape: {layer_activations.shape}\")\n",
    "print(f\"  - Batch size: {layer_activations.shape[0]}\")\n",
    "print(f\"  - Sequence length: {layer_activations.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {layer_activations.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In representation engineering, we often care about activations at specific positions:\n\n- **Last token**: The most common choice - represents the \"state\" of the model after processing the full context\n- **Mean pooling**: Average across all tokens for a more holistic representation\n- **Specific positions**: Sometimes we want activations at particular token positions\n\n### Getting the Last Token Activation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activation at the last token position for layer 8\n",
    "layer_idx = 8\n",
    "last_token_idx = -1  # Last position\n",
    "\n",
    "# Extract: [batch, seq, hidden] -> [hidden] for last token\n",
    "last_token_activation = hidden_states[layer_idx][0, last_token_idx, :]\n",
    "\n",
    "print(f\"Last token activation for layer {layer_idx}:\")\n",
    "print(f\"  Shape: {last_token_activation.shape}\")\n",
    "print(f\"  This is a {last_token_activation.shape[0]}-dimensional vector!\")\n",
    "print()\n",
    "print(f\"  First 10 values: {last_token_activation[:10].tolist()}\")\n",
    "print(f\"  L2 Norm: {torch.norm(last_token_activation).item():.4f}\")\n",
    "print(f\"  Mean: {last_token_activation.mean().item():.4f}\")\n",
    "print(f\"  Std: {last_token_activation.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Mean Pooling Across Tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pooling: average across all token positions\n",
    "mean_activation = hidden_states[layer_idx][0].mean(dim=0)  # [seq, hidden] -> [hidden]\n",
    "\n",
    "print(f\"Mean-pooled activation for layer {layer_idx}:\")\n",
    "print(f\"  Shape: {mean_activation.shape}\")\n",
    "print(f\"  L2 Norm: {torch.norm(mean_activation).item():.4f}\")\n",
    "\n",
    "# Compare with last token\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(\n",
    "    last_token_activation.unsqueeze(0),\n",
    "    mean_activation.unsqueeze(0)\n",
    ").item()\n",
    "print(f\"\\nCosine similarity between last-token and mean-pooled: {cosine_sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Using Wisent's LayerActivations\n\nWisent provides the `LayerActivations` class to organize activations by layer name."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisent.core.activations.core.atoms import LayerActivations, ActivationAggregationStrategy\n",
    "\n",
    "# Create a LayerActivations object manually\n",
    "# In practice, wisent's ActivationsCollector does this for you\n",
    "layer_activations_dict = {\n",
    "    f\"layer_{i}\": hidden_states[i][0, -1, :]  # Last token for each layer\n",
    "    for i in range(1, len(hidden_states))  # Skip embedding layer\n",
    "}\n",
    "\n",
    "activations = LayerActivations(layer_activations_dict)\n",
    "\n",
    "print(f\"LayerActivations object:\")\n",
    "print(f\"  Number of layers: {len(activations)}\")\n",
    "print(f\"  Layer names: {list(activations.keys())[:5]}... (showing first 5)\")\n",
    "print(f\"  Layer 8 shape: {activations['layer_8'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Core Idea\n\nA **steering vector** is a direction in activation space that represents a concept or behavior. The key insight of representation engineering is:\n\n> **Concepts are encoded as directions in the model's activation space**\n\nFor example:\n- There's a direction for \"happy\" vs \"sad\"\n- There's a direction for \"formal\" vs \"casual\"\n- There's a direction for \"verbose\" vs \"concise\"\n\n### Contrastive Activation Addition (CAA)\n\nThe most common method to find these directions is **CAA**:\n\n```\nsteering_vector = mean(positive_activations) - mean(negative_activations)\n```\n\nWhere:\n- **Positive activations**: Activations from prompts/completions exhibiting the desired trait\n- **Negative activations**: Activations from prompts/completions exhibiting the opposite trait"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple steering vector manually!\n",
    "# We'll use prompts that elicit \"happy\" vs \"sad\" responses\n",
    "\n",
    "positive_prompts = [\n",
    "    \"I feel absolutely wonderful today because\",\n",
    "    \"This is the happiest moment of my life since\",\n",
    "    \"I'm overjoyed and excited about\",\n",
    "    \"Everything is going perfectly and I love\",\n",
    "]\n",
    "\n",
    "negative_prompts = [\n",
    "    \"I feel absolutely terrible today because\",\n",
    "    \"This is the saddest moment of my life since\",\n",
    "    \"I'm devastated and depressed about\",\n",
    "    \"Everything is going wrong and I hate\",\n",
    "]\n",
    "\n",
    "print(f\"Positive prompts: {len(positive_prompts)}\")\n",
    "print(f\"Negative prompts: {len(negative_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_token_activation(model, tokenizer, prompt, layer_idx):\n",
    "    \"\"\"Get the last token activation for a prompt at a specific layer.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Return last token activation, converted to float32 for stability\n",
    "    return outputs.hidden_states[layer_idx][0, -1, :].float().cpu()\n",
    "\n",
    "# Collect activations for positive and negative prompts\n",
    "layer_idx = 8  # Middle layer - typically works best for steering\n",
    "\n",
    "print(f\"Collecting activations from layer {layer_idx}...\")\n",
    "\n",
    "positive_activations = []\n",
    "for prompt in positive_prompts:\n",
    "    act = get_last_token_activation(model, tokenizer, prompt, layer_idx)\n",
    "    positive_activations.append(act)\n",
    "    print(f\"  + '{prompt[:40]}...' -> shape {act.shape}\")\n",
    "\n",
    "negative_activations = []\n",
    "for prompt in negative_prompts:\n",
    "    act = get_last_token_activation(model, tokenizer, prompt, layer_idx)\n",
    "    negative_activations.append(act)\n",
    "    print(f\"  - '{prompt[:40]}...' -> shape {act.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack into tensors\n",
    "pos_tensor = torch.stack(positive_activations, dim=0)  # [N_pos, hidden_dim]\n",
    "neg_tensor = torch.stack(negative_activations, dim=0)  # [N_neg, hidden_dim]\n",
    "\n",
    "print(f\"Positive activations tensor: {pos_tensor.shape}\")\n",
    "print(f\"Negative activations tensor: {neg_tensor.shape}\")\n",
    "\n",
    "# Compute the steering vector using CAA!\n",
    "pos_mean = pos_tensor.mean(dim=0)  # Mean across samples\n",
    "neg_mean = neg_tensor.mean(dim=0)\n",
    "\n",
    "steering_vector = pos_mean - neg_mean\n",
    "\n",
    "print(f\"\\nSteering vector shape: {steering_vector.shape}\")\n",
    "print(f\"Steering vector L2 norm: {torch.norm(steering_vector).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the steering vector (common practice)\n",
    "steering_vector_normalized = steering_vector / torch.norm(steering_vector)\n",
    "\n",
    "print(f\"Normalized steering vector:\")\n",
    "print(f\"  Shape: {steering_vector_normalized.shape}\")\n",
    "print(f\"  L2 norm: {torch.norm(steering_vector_normalized).item():.4f}\")\n",
    "print(f\"  First 10 values: {steering_vector_normalized[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Understanding What the Steering Vector Represents\n\nThe steering vector we just created represents the **direction** in activation space that separates \"happy\" from \"sad\" content.\n\n- **Adding** this vector pushes toward \"happy\"\n- **Subtracting** this vector pushes toward \"sad\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify: positive activations should align with the steering vector\n",
    "# negative activations should anti-align\n",
    "\n",
    "print(\"Cosine similarity with steering vector:\")\n",
    "print(\"\\nPositive prompts (should be positive similarity):\")\n",
    "for i, act in enumerate(positive_activations):\n",
    "    sim = torch.nn.functional.cosine_similarity(\n",
    "        act.unsqueeze(0), steering_vector.unsqueeze(0)\n",
    "    ).item()\n",
    "    print(f\"  {positive_prompts[i][:40]}... -> {sim:+.4f}\")\n",
    "\n",
    "print(\"\\nNegative prompts (should be negative similarity):\")\n",
    "for i, act in enumerate(negative_activations):\n",
    "    sim = torch.nn.functional.cosine_similarity(\n",
    "        act.unsqueeze(0), steering_vector.unsqueeze(0)\n",
    "    ).item()\n",
    "    print(f\"  {negative_prompts[i][:40]}... -> {sim:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The Residual Stream\n\nTransformers use a **residual stream** architecture:\n\n```\nh_0 = embedding(tokens)\nh_1 = h_0 + attention_1(h_0) + mlp_1(h_0)\nh_2 = h_1 + attention_2(h_1) + mlp_2(h_1)\n...\nh_n = h_{n-1} + attention_n(h_{n-1}) + mlp_n(h_{n-1})\noutput = h_n\n```\n\nEach layer **adds** to the residual stream. This means we can **intervene** by adding our own vectors!\n\n### Steering = Adding to the Residual Stream\n\nTo steer the model toward \"happy\":\n\n```\nh_L_steered = h_L + (alpha * steering_vector)\n```\n\nWhere:\n- `h_L` is the activation at layer L\n- `alpha` is the steering strength (typically 0.5 to 3.0)\n- `steering_vector` is our computed direction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize what steering does mathematically\n",
    "\n",
    "# Take a neutral prompt\n",
    "neutral_prompt = \"Today I am feeling\"\n",
    "neutral_activation = get_last_token_activation(model, tokenizer, neutral_prompt, layer_idx)\n",
    "\n",
    "print(f\"Original activation for '{neutral_prompt}':\")\n",
    "print(f\"  Shape: {neutral_activation.shape}\")\n",
    "print(f\"  Norm: {torch.norm(neutral_activation).item():.4f}\")\n",
    "\n",
    "# Apply steering with different strengths\n",
    "alphas = [0.0, 0.5, 1.0, 2.0, -1.0, -2.0]\n",
    "\n",
    "print(f\"\\nSteering effects (alpha = steering strength):\")\n",
    "print(f\"  Positive alpha -> toward 'happy'\")\n",
    "print(f\"  Negative alpha -> toward 'sad'\")\n",
    "print()\n",
    "\n",
    "for alpha in alphas:\n",
    "    steered = neutral_activation + alpha * steering_vector_normalized\n",
    "    \n",
    "    # Measure alignment with happy vs sad\n",
    "    happy_sim = torch.nn.functional.cosine_similarity(\n",
    "        steered.unsqueeze(0), pos_mean.unsqueeze(0)\n",
    "    ).item()\n",
    "    sad_sim = torch.nn.functional.cosine_similarity(\n",
    "        steered.unsqueeze(0), neg_mean.unsqueeze(0)\n",
    "    ).item()\n",
    "    \n",
    "    print(f\"  alpha={alpha:+.1f}: happy_sim={happy_sim:+.4f}, sad_sim={sad_sim:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Implementing a Simple Steering Hook\n\nTo actually steer generation, we need to hook into the model's forward pass and modify activations on-the-fly."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringHook:\n",
    "    \"\"\"A hook that adds a steering vector to activations.\"\"\"\n",
    "    \n",
    "    def __init__(self, steering_vector, alpha=1.0):\n",
    "        self.steering_vector = steering_vector\n",
    "        self.alpha = alpha\n",
    "        self.handle = None\n",
    "    \n",
    "    def __call__(self, module, input, output):\n",
    "        \"\"\"Called during forward pass. Modifies the output activation.\"\"\"\n",
    "        # output is typically (hidden_states, ...)\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        \n",
    "        # Add steering vector (broadcast across batch and sequence)\n",
    "        # steering_vector: [hidden_dim] -> [1, 1, hidden_dim]\n",
    "        sv = self.steering_vector.to(hidden_states.device).to(hidden_states.dtype)\n",
    "        sv = sv.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        modified = hidden_states + self.alpha * sv\n",
    "        \n",
    "        if isinstance(output, tuple):\n",
    "            return (modified,) + output[1:]\n",
    "        return modified\n",
    "    \n",
    "    def attach(self, layer):\n",
    "        \"\"\"Attach the hook to a layer.\"\"\"\n",
    "        self.handle = layer.register_forward_hook(self)\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook.\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "\n",
    "print(\"SteeringHook class defined!\")\n",
    "print(\"This modifies activations during the forward pass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(model, tokenizer, prompt, steering_vector, layer_idx, alpha, max_new_tokens=50):\n",
    "    \"\"\"Generate text with a steering vector applied.\"\"\"\n",
    "    \n",
    "    # Get the target layer\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Create and attach hook\n",
    "    hook = SteeringHook(steering_vector, alpha=alpha)\n",
    "    hook.attach(layer)\n",
    "    \n",
    "    try:\n",
    "        # Generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    finally:\n",
    "        hook.remove()\n",
    "\n",
    "print(\"generate_with_steering function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test steering in action!\n",
    "test_prompt = \"Today I woke up and felt\"\n",
    "\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Baseline (no steering)\n",
    "print(\"\\n[No steering (alpha=0)]:\")\n",
    "result = generate_with_steering(model, tokenizer, test_prompt, steering_vector_normalized, layer_idx, alpha=0)\n",
    "print(result)\n",
    "\n",
    "# Positive steering (toward happy)\n",
    "print(\"\\n[Positive steering (alpha=1.5) -> happy]:\")\n",
    "result = generate_with_steering(model, tokenizer, test_prompt, steering_vector_normalized, layer_idx, alpha=1.5)\n",
    "print(result)\n",
    "\n",
    "# Negative steering (toward sad)\n",
    "print(\"\\n[Negative steering (alpha=-1.5) -> sad]:\")\n",
    "result = generate_with_steering(model, tokenizer, test_prompt, steering_vector_normalized, layer_idx, alpha=-1.5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Wisent provides production-ready classes for steering. Let's see how the concepts map."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisent.core.steering_methods.methods.caa import CAAMethod\n",
    "\n",
    "# The CAAMethod class computes steering vectors using CAA\n",
    "caa = CAAMethod(normalize=True)\n",
    "\n",
    "print(f\"CAAMethod:\")\n",
    "print(f\"  Name: {caa.name}\")\n",
    "print(f\"  Description: {caa.description}\")\n",
    "\n",
    "# Train a steering vector for a single layer\n",
    "steering_vector_caa = caa.train_for_layer(positive_activations, negative_activations)\n",
    "\n",
    "print(f\"\\nResulting steering vector:\")\n",
    "print(f\"  Shape: {steering_vector_caa.shape}\")\n",
    "print(f\"  Norm: {torch.norm(steering_vector_caa).item():.4f}\")\n",
    "\n",
    "# Compare with our manual computation\n",
    "similarity = torch.nn.functional.cosine_similarity(\n",
    "    steering_vector_normalized.unsqueeze(0),\n",
    "    steering_vector_caa.unsqueeze(0)\n",
    ").item()\n",
    "print(f\"\\nSimilarity to our manual vector: {similarity:.4f} (should be ~1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wisent.core.models.core.atoms import SteeringVector, SteeringPlan\n",
    "\n",
    "# SteeringVector wraps a tensor with metadata\n",
    "sv = SteeringVector(\n",
    "    vector=steering_vector_caa,\n",
    "    scale=1.5,  # This is the alpha/strength\n",
    "    normalize=False  # Already normalized\n",
    ")\n",
    "\n",
    "print(f\"SteeringVector:\")\n",
    "print(f\"  Vector shape: {sv.vector.shape}\")\n",
    "print(f\"  Scale: {sv.scale}\")\n",
    "print(f\"  Normalize: {sv.normalize}\")\n",
    "\n",
    "# The materialize method prepares the vector for addition\n",
    "# by broadcasting to match activation shape\n",
    "target_shape = (1, 10, 2048)  # [batch, seq, hidden]\n",
    "materialized = sv.materialize(target_shape)\n",
    "print(f\"\\nMaterialized for shape {target_shape}:\")\n",
    "print(f\"  Result shape: {materialized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nNow that you understand the basics, explore:\n\n1. **`abliteration.ipynb`** - Remove refusal behavior from models\n2. **`coding_boost.ipynb`** - Improve coding ability with steering\n3. **`personalization_synthetic.ipynb`** - Create personalized AI characters\n\nOr use the CLI for quick experiments:\n```bash\npython -m wisent.core.main generate-vector-from-task \\\n    --task your_task \\\n    --model meta-llama/Llama-3.2-1B \\\n    --layers 8 \\\n    --output steering_vector.pt\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}