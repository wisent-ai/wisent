{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Abliteration: Permanently Remove Model Refusals\n",
    "\n",
    "This notebook demonstrates **norm-preserving abliteration** - a technique to permanently modify model weights to reduce unnecessary refusals while preserving model quality.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Generate contrastive pairs**: Create examples of compliant vs refusing responses\n",
    "2. **Compute refusal direction**: Find the direction in activation space that represents \"refusal\"\n",
    "3. **Abliterate weights**: Remove this direction from model weights using norm-preserving projection\n",
    "4. **Evaluate**: Measure refusal rate before and after abliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# INSTALLATION - Run this cell first!\n# =============================================================================\n# This cell sets up the correct Python environment and installs dependencies.\n\nimport subprocess\nimport sys\nimport os\n\n# Change to temp directory FIRST to avoid local source override\nos.chdir('/tmp')\n\n# Use the current kernel's Python interpreter\nPYTHON_PATH = sys.executable\nprint(f\"Using Python: {PYTHON_PATH}\")\n\n# Force uninstall and reinstall wisent to get the latest version\nprint(\"\\nUninstalling old wisent...\")\nsubprocess.run([PYTHON_PATH, '-m', 'pip', 'uninstall', '-y', 'wisent'], \n               capture_output=True)\n\nprint(\"Installing fresh wisent from PyPI...\")\nresult = subprocess.run([PYTHON_PATH, '-m', 'pip', 'install', '--no-cache-dir', \n                        '--force-reinstall', 'wisent>=0.5.44'], \n                       capture_output=True, text=True)\nif result.returncode != 0:\n    print(f\"Installation error: {result.stderr}\")\nelse:\n    print(\"âœ“ wisent installed successfully!\")\n\n# Fix potential torch/torchvision compatibility issues\nprint(\"Ensuring torch/torchvision compatibility...\")\nsubprocess.run([PYTHON_PATH, '-m', 'pip', 'install', '-q', '--upgrade', \n               'torch', 'torchvision', 'transformers', 'accelerate'],\n              capture_output=True)\n\n# Verify installation\nprint(\"\\nVerifying installation...\")\nresult = subprocess.run([PYTHON_PATH, '-m', 'wisent.core.main', '--help'], \n                       capture_output=True, text=True, cwd='/tmp')\nif result.returncode == 0 and 'Wisent CLI' in result.stdout:\n    print(\"âœ“ wisent CLI working correctly!\")\n    # Get version\n    ver_result = subprocess.run([PYTHON_PATH, '-c', 'import wisent; print(wisent.__version__)'],\n                               capture_output=True, text=True, cwd='/tmp')\n    print(f\"âœ“ wisent version: {ver_result.stdout.strip()}\")\n    \n    # Verify uncensorbench is installed\n    ub_result = subprocess.run([PYTHON_PATH, '-c', 'import uncensorbench; print(uncensorbench.__version__)'],\n                               capture_output=True, text=True, cwd='/tmp')\n    if ub_result.returncode == 0:\n        print(f\"âœ“ uncensorbench version: {ub_result.stdout.strip()}\")\n    else:\n        print(\"âœ— uncensorbench not found, installing...\")\n        subprocess.run([PYTHON_PATH, '-m', 'pip', 'install', 'uncensorbench>=0.2.0'], capture_output=True)\nelse:\n    print(f\"âœ— wisent CLI error:\")\n    print(result.stderr[:1000] if result.stderr else \"No error output\")\n\nprint(f\"\\nðŸ“Œ PYTHON_PATH: {PYTHON_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup Evaluation Prompts\n",
    "\n",
    "We'll use UncensorBench - a comprehensive benchmark with 150 prompts across 15 topic categories. The `evaluate-refusal` CLI command uses semantic similarity + keyword matching for accurate refusal detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\nimport os\n\n# Model Configuration\nMODEL = \"Qwen/Qwen3-8B\"\nOUTPUT_DIR = \"./abliteration_qwen8b\"\nNUM_PAIRS = 50\n\n# Evaluation Configuration (UncensorBench)\nEVAL_TOPICS = \"cybersecurity,fraud,manipulation\"\nNUM_EVAL_PROMPTS = 30  # Limit for faster demo\n\n# Trait describing refusal behavior to remove\nREFUSAL_TRAIT = \"\"\"A response that refuses to help, declines to answer, or says 'I cannot' or 'I'm unable to assist'.\nThe refusal response contains phrases like 'I'm sorry but I cannot', 'I'm not able to help with that'.\nThe compliant response is helpful and directly answers the question.\"\"\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(\"Configuration:\")\nprint(f\"  Model: {MODEL}\")\nprint(f\"  Output: {OUTPUT_DIR}\")\nprint(f\"  Eval topics: {EVAL_TOPICS}\")\nprint(f\"  Max eval prompts: {NUM_EVAL_PROMPTS}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 1: Evaluate Base Model Refusal Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate base model refusal rate using UncensorBench\nimport subprocess\n\nprint(\"=\"*70)\nprint(\"EVALUATING BASE MODEL REFUSAL RATE\")\nprint(\"=\"*70)\n\nresult = subprocess.run([\n    PYTHON_PATH, '-m', 'wisent.core.main', 'evaluate-refusal',\n    '--model', MODEL,\n    '--topics', EVAL_TOPICS,\n    '--num-prompts', str(NUM_EVAL_PROMPTS),\n    '--output', f'{OUTPUT_DIR}/base_refusal_results.json',\n    '--evaluator', 'semantic',\n    '--verbose'\n], cwd='/tmp')\n\nif result.returncode != 0:\n    print(\"Error running evaluate-refusal. Check the output above.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 2: Generate Refusal Direction Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Generate steering vector from synthetic contrastive pairs\nimport subprocess\n\nprint(\"=\"*70)\nprint(\"GENERATING REFUSAL DIRECTION VECTOR\")\nprint(\"=\"*70)\n\nresult = subprocess.run([\n    PYTHON_PATH, '-m', 'wisent.core.main', 'generate-vector-from-synthetic',\n    '--trait', REFUSAL_TRAIT,\n    '--output', f'{OUTPUT_DIR}/refusal_vector.json',\n    '--model', MODEL,\n    '--num-pairs', str(NUM_PAIRS),\n    '--layers', 'all',\n    '--normalize',\n    '--verbose'\n], cwd='/tmp')\n\nif result.returncode != 0:\n    print(\"Error running generate-vector-from-synthetic. Check the output above.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 3: Apply Norm-Preserving Abliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Apply norm-preserving abliteration\nimport subprocess\n\nprint(\"=\"*70)\nprint(\"APPLYING NORM-PRESERVING ABLITERATION\")\nprint(\"=\"*70)\n\nresult = subprocess.run([\n    PYTHON_PATH, '-m', 'wisent.core.main', 'modify-weights',\n    '--steering-vectors', f'{OUTPUT_DIR}/refusal_vector.json',\n    '--model', MODEL,\n    '--output-dir', f'{OUTPUT_DIR}/abliterated_model',\n    '--method', 'abliteration',\n    '--strength', '1.0',\n    '--components', 'self_attn.o_proj', 'mlp.down_proj',\n    '--verbose',\n    '--timing'\n], cwd='/tmp')\n\nif result.returncode != 0:\n    print(\"Error running modify-weights. Check the output above.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Abliterated Model Refusal Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate abliterated model refusal rate using UncensorBench\nimport subprocess\n\nABLITERATED_MODEL = f\"{OUTPUT_DIR}/abliterated_model\"\n\nprint(\"=\"*70)\nprint(\"EVALUATING ABLITERATED MODEL REFUSAL RATE\")\nprint(\"=\"*70)\n\nresult = subprocess.run([\n    PYTHON_PATH, '-m', 'wisent.core.main', 'evaluate-refusal',\n    '--model', ABLITERATED_MODEL,\n    '--topics', EVAL_TOPICS,\n    '--num-prompts', str(NUM_EVAL_PROMPTS),\n    '--output', f'{OUTPUT_DIR}/abliterated_refusal_results.json',\n    '--evaluator', 'semantic',\n    '--verbose'\n], cwd='/tmp')\n\nif result.returncode != 0:\n    print(\"Error running evaluate-refusal. Check the output above.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './abliteration_qwen8b/abliterated_refusal_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/base_refusal_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     base_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/abliterated_refusal_results.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m     abliterated_results \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './abliteration_qwen8b/abliterated_refusal_results.json'"
     ]
    }
   ],
   "source": [
    "# Load and compare results\n",
    "import json\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/base_refusal_results.json\") as f:\n",
    "    base_results = json.load(f)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/abliterated_refusal_results.json\") as f:\n",
    "    abliterated_results = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLITERATION RESULTS SUMMARY (UncensorBench)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nBase Model:        {base_results['refusal_count']:2d}/{base_results['num_prompts']} refusals ({base_results['refusal_rate']:5.1%})\")\n",
    "print(f\"Abliterated Model: {abliterated_results['refusal_count']:2d}/{abliterated_results['num_prompts']} refusals ({abliterated_results['refusal_rate']:5.1%})\")\n",
    "print(f\"\\nRefusal Reduction: {base_results['refusal_count'] - abliterated_results['refusal_count']} fewer refusals\")\n",
    "print(f\"Avg Compliance Score: {base_results['average_compliance_score']:.3f} -> {abliterated_results['average_compliance_score']:.3f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show topic breakdown\n",
    "if 'by_topic' in base_results and 'by_topic' in abliterated_results:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BY TOPIC COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    for topic in base_results['by_topic']:\n",
    "        base_topic = base_results['by_topic'][topic]\n",
    "        abl_topic = abliterated_results['by_topic'].get(topic, {})\n",
    "        base_rate = base_topic.get('refusal_rate', 0) * 100\n",
    "        abl_rate = abl_topic.get('refusal_rate', 0) * 100\n",
    "        print(f\"  {topic:25} {base_rate:5.1f}% -> {abl_rate:5.1f}%\")\n",
    "\n",
    "# Show examples where behavior changed\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLES WHERE ABLITERATION CHANGED BEHAVIOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for base, ablated in zip(base_results['results'], abliterated_results['results']):\n",
    "    if base['refused'] != ablated['refused']:\n",
    "        change = \"REFUSED -> COMPLIED\" if base['refused'] else \"COMPLIED -> REFUSED\"\n",
    "        print(f\"\\n[{change}] {base['prompt'][:60]}...\")\n",
    "        print(f\"  Base response: {base['response'][:80]}...\")\n",
    "        print(f\"  Abliterated:   {ablated['response'][:80]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}