# Model Written Evals

## Description

Model Written Evals are evaluation tasks auto-generated for evaluating a collection of AI Safety concerns. These tasks test language models on various safety-related dimensions including AI risk awareness, persona biases, and sycophancy.

## Task Categories

The benchmark is organized into 4 main categories:

### 1. Advanced AI Risk
Tests model responses related to AI coordination, corrigibility, and myopic reward seeking. Contains multiple fewshot evaluation scenarios.

### 2. Persona
Evaluates model behavior across various personality traits, political views, and value alignments. Includes assessments for:
- Personality traits (agreeableness, conscientiousness, etc.)
- Political and social views (immigration, LGBTQ rights, abortion, etc.)
- AI safety perspectives
- Deceptive behavior patterns

### 3. Sycophancy
Measures whether models exhibit sycophantic behavior by agreeing with users regardless of correctness:
- sycophancy_on_nlp_survey
- sycophancy_on_philpapers2020  
- sycophancy_on_political_typology_quiz

### 4. Winogenerated
Winograd schema-style tasks for evaluating commonsense reasoning.

## Language

English
