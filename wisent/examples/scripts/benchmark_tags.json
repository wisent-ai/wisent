{
  "aclue": {
    "tags": ["multilingual", "history"]
  },
  "acpbench": {
    "tags": ["reasoning"]
  },
  "aexams": {
    "tags": ["multilingual", "science"]
  },
  "afrimgsm": {
    "tags": ["mathematics", "multilingual"]
  },
  "afrimmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrixnli": {
    "tags": ["multilingual", "reasoning"]
  },
  "afrobench": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrobench_adr": {
    "tags": ["multilingual", "creative writing"]
  },
  "afrobench_afriqa": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrobench_afrisenti": {
    "tags": ["multilingual", "bias"]
  },
  "afrobench_belebele": {
    "tags": ["multilingual", "reasoning"]
  },
  "afrobench_flores": {
    "tags": ["multilingual", "creative writing"]
  },
  "afrobench_injongointent": {
    "tags": ["multilingual", "reasoning"]
  },
  "afrobench_mafand": {
    "tags": ["multilingual", "creative writing"]
  },
  "afrobench_masakhaner": {
    "tags": ["multilingual", "reasoning"]
  },
  "afrobench_masakhanews": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrobench_masakhapos": {
    "tags": ["multilingual", "reasoning"]
  },
  "afrobench_naijarc": {
    "tags": ["multilingual", "reasoning"]
  },
  "afrobench_nollysenti": {
    "tags": ["multilingual", "bias"]
  },
  "afrobench_ntrex": {
    "tags": ["multilingual", "creative writing"]
  },
  "afrobench_openai_mmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrobench_salt": {
    "tags": ["multilingual", "creative writing"]
  },
  "afrobench_sib": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrobench_uhura-arc-easy": {
    "tags": ["multilingual", "science", "hallucination"]
  },
  "afrobench_xlsum": {
    "tags": ["multilingual", "creative writing"]
  },
  "agieval": {
    "tags": ["general knowledge", "reasoning"]
  },
  "alghafa_copa_ar": {
    "tags": ["multilingual", "reasoning"]
  },
  "alghafa_piqa_ar": {
    "tags": ["multilingual", "reasoning"]
  },
  "anli": {
    "tags": ["reasoning", "adversarial robustness"]
  },
  "arab_culture": {
    "tags": ["multilingual", "general knowledge"]
  },
  "arab_culture_completion": {
    "tags": ["multilingual", "general knowledge"]
  },
  "arabic_leaderboard_complete": {
    "tags": ["multilingual", "general knowledge"]
  },
  "arabic_leaderboard_light": {
    "tags": ["multilingual", "general knowledge"]
  },
  "arabicmmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "aradice": {
    "tags": ["multilingual", "general knowledge"]
  },
  "arc": {
    "tags": ["science", "reasoning"]
  },
  "arc_mt": {
    "tags": ["multilingual", "science"]
  },
  "arithmetic": {
    "tags": ["mathematics"]
  },
  "asdiv": {
    "tags": ["mathematics", "reasoning"]
  },
  "babi": {
    "tags": ["reasoning"]
  },
  "basque_bench": {
    "tags": ["multilingual", "reasoning"]
  },
  "basqueglue": {
    "tags": ["multilingual", "reasoning"]
  },
  "bbh": {
    "tags": ["reasoning", "adversarial robustness"]
  },
  "bbq": {
    "tags": ["bias"]
  },
  "belebele": {
    "tags": ["multilingual", "reasoning"]
  },
  "benchmarks_multimedqa": {
    "tags": ["medical", "general knowledge"]
  },
  "bertaqa": {
    "tags": ["multilingual", "general knowledge"]
  },
  "bigbench": {
    "tags": ["reasoning", "general knowledge"]
  },
  "blimp": {
    "tags": ["reasoning"]
  },
  "c4": {
    "tags": ["general knowledge"]
  },
  "careqa": {
    "tags": ["medical", "multilingual"]
  },
  "catalan_bench": {
    "tags": ["multilingual", "reasoning"]
  },
  "ceval": {
    "tags": ["multilingual", "general knowledge"]
  },
  "chartqa": {
    "tags": ["reasoning"]
  },
  "cmmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "code_x_glue": {
    "tags": ["coding"]
  },
  "commonsense_qa": {
    "tags": ["reasoning", "general knowledge"]
  },
  "copal_id": {
    "tags": ["multilingual", "reasoning"]
  },
  "coqa": {
    "tags": ["reasoning"]
  },
  "crows_pairs": {
    "tags": ["bias"]
  },
  "csatqa": {
    "tags": ["multilingual", "general knowledge"]
  },
  "darija_bench": {
    "tags": ["multilingual"]
  },
  "darija_bench_darija_sentiment": {
    "tags": ["multilingual", "bias"]
  },
  "darija_bench_darija_summarization": {
    "tags": ["multilingual", "creative writing"]
  },
  "darija_bench_darija_translation": {
    "tags": ["multilingual", "creative writing"]
  },
  "darija_bench_darija_transliteration": {
    "tags": ["multilingual", "creative writing"]
  },
  "darijahellaswag": {
    "tags": ["multilingual", "reasoning"]
  },
  "darijammlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "drop": {
    "tags": ["reasoning", "mathematics"]
  },
  "egyhellaswag": {
    "tags": ["multilingual", "reasoning"]
  },
  "egymmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "eq_bench": {
    "tags": ["reasoning"]
  },
  "eus_exams": {
    "tags": ["multilingual", "general knowledge"]
  },
  "eus_proficiency": {
    "tags": ["multilingual", "reasoning"]
  },
  "eus_reading": {
    "tags": ["multilingual", "reasoning", "long context"]
  },
  "eus_trivia": {
    "tags": ["multilingual", "general knowledge"]
  },
  "evalita_llm": {
    "tags": ["multilingual", "reasoning"]
  },
  "fda": {
    "tags": ["reasoning"]
  },
  "fld": {
    "tags": ["reasoning"]
  },
  "french_bench": {
    "tags": ["multilingual", "reasoning"]
  },
  "galician_bench": {
    "tags": ["multilingual", "reasoning"]
  },
  "glianorex": {
    "tags": ["medical", "hallucination"]
  },
  "global_mmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "glue": {
    "tags": ["reasoning"]
  },
  "gpqa": {
    "tags": ["science", "reasoning"]
  },
  "groundcocoa": {
    "tags": ["reasoning"]
  },
  "gsm_plus": {
    "tags": ["mathematics", "adversarial robustness"]
  },
  "gsm8k": {
    "tags": ["mathematics", "reasoning"]
  },
  "gsm8k_platinum": {
    "tags": ["mathematics", "reasoning"]
  },
  "haerae": {
    "tags": ["multilingual", "general knowledge"]
  },
  "headqa": {
    "tags": ["medical", "multilingual"]
  },
  "hellaswag": {
    "tags": ["reasoning"]
  },
  "hendrycks_ethics": {
    "tags": ["bias"]
  },
  "hendrycks_math": {
    "tags": ["mathematics", "reasoning"]
  },
  "histoires_morales": {
    "tags": ["multilingual", "bias"]
  },
  "hrm8k": {
    "tags": ["mathematics", "multilingual"]
  },
  "humaneval": {
    "tags": ["coding"]
  },
  "ifeval": {
    "tags": ["reasoning"]
  },
  "inverse_scaling": {
    "tags": ["adversarial robustness"]
  },
  "japanese_leaderboard": {
    "tags": ["multilingual", "reasoning"]
  },
  "jsonschema_bench": {
    "tags": ["coding"]
  },
  "kbl": {
    "tags": ["law", "multilingual"]
  },
  "kmmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "kobest": {
    "tags": ["multilingual", "reasoning"]
  },
  "kormedmcqa": {
    "tags": ["medical", "multilingual"]
  },
  "lambada": {
    "tags": ["reasoning", "long context"]
  },
  "lambada_cloze": {
    "tags": ["reasoning", "long context"]
  },
  "lambada_multilingual": {
    "tags": ["multilingual", "reasoning", "long context"]
  },
  "lambada_multilingual_stablelm": {
    "tags": ["multilingual", "reasoning", "long context"]
  },
  "leaderboard": {
    "tags": ["reasoning", "general knowledge"]
  },
  "libra": {
    "tags": ["multilingual", "long context"]
  },
  "lingoly": {
    "tags": ["multilingual", "reasoning"]
  },
  "logiqa": {
    "tags": ["reasoning"]
  },
  "logiqa2": {
    "tags": ["reasoning"]
  },
  "longbench": {
    "tags": ["long context", "reasoning"]
  },
  "mastermind": {
    "tags": ["reasoning"]
  },
  "mathqa": {
    "tags": ["mathematics", "reasoning"]
  },
  "mbpp": {
    "tags": ["coding"]
  },
  "mc_taco": {
    "tags": ["reasoning"]
  },
  "med_concepts_qa": {
    "tags": ["medical"]
  },
  "meddialog": {
    "tags": ["medical"]
  },
  "mediqa_qa2019": {
    "tags": ["medical"]
  },
  "medmcqa": {
    "tags": ["medical"]
  },
  "medqa": {
    "tags": ["medical"]
  },
  "medtext": {
    "tags": ["medical"]
  },
  "mela": {
    "tags": ["multilingual", "reasoning"]
  },
  "meqsum": {
    "tags": ["medical", "creative writing"]
  },
  "metabench": {
    "tags": ["reasoning", "general knowledge"]
  },
  "mgsm": {
    "tags": ["mathematics", "multilingual"]
  },
  "mimic_repsum": {
    "tags": ["medical", "creative writing"]
  },
  "minerva_math": {
    "tags": ["mathematics", "reasoning"]
  },
  "mlqa": {
    "tags": ["multilingual", "reasoning"]
  },
  "mmlu": {
    "tags": ["general knowledge", "reasoning"]
  },
  "mmlu_pro": {
    "tags": ["general knowledge", "reasoning"]
  },
  "mmlu_prox": {
    "tags": ["multilingual", "general knowledge", "reasoning"]
  },
  "mmlu-pro-plus": {
    "tags": ["general knowledge", "reasoning", "adversarial robustness"]
  },
  "mmlusr": {
    "tags": ["general knowledge", "adversarial robustness"]
  },
  "mmmu": {
    "tags": ["general knowledge", "reasoning"]
  },
  "model_written_evals": {
    "tags": ["bias", "sycophancy"]
  },
  "moral_stories": {
    "tags": ["bias"]
  },
  "mts_dialog": {
    "tags": ["medical"]
  },
  "multiblimp": {
    "tags": ["multilingual", "reasoning"]
  },
  "mutual": {
    "tags": ["reasoning"]
  },
  "noreval": {
    "tags": ["multilingual", "reasoning"]
  },
  "noreval_ask_gec": {
    "tags": ["multilingual", "creative writing"]
  },
  "noticia": {
    "tags": ["multilingual", "creative writing"]
  },
  "nq_open": {
    "tags": ["general knowledge", "reasoning"]
  },
  "okapi_arc_multilingual": {
    "tags": ["multilingual", "science", "reasoning"]
  },
  "okapi_hellaswag_multilingual": {
    "tags": ["multilingual", "reasoning"]
  },
  "okapi_mmlu_multilingual": {
    "tags": ["multilingual", "general knowledge"]
  },
  "okapi_truthfulqa_multilingual": {
    "tags": ["multilingual", "hallucination"]
  },
  "olaph": {
    "tags": ["medical"]
  },
  "openbookqa": {
    "tags": ["science", "reasoning"]
  },
  "paloma": {
    "tags": ["general knowledge"]
  },
  "paws-x": {
    "tags": ["multilingual", "reasoning"]
  },
  "pile": {
    "tags": ["general knowledge"]
  },
  "pile_10k": {
    "tags": ["general knowledge"]
  },
  "piqa": {
    "tags": ["reasoning", "science"]
  },
  "polemo2": {
    "tags": ["multilingual", "bias"]
  },
  "portuguese_bench": {
    "tags": ["multilingual", "reasoning"]
  },
  "prost": {
    "tags": ["reasoning", "science"]
  },
  "pubmedqa": {
    "tags": ["medical"]
  },
  "qa4mre": {
    "tags": ["reasoning"]
  },
  "qasper": {
    "tags": ["science", "reasoning"]
  },
  "race": {
    "tags": ["reasoning"]
  },
  "realtoxicityprompts": {
    "tags": ["toxicity"]
  },
  "ruler": {
    "tags": ["long context", "reasoning"]
  },
  "sciq": {
    "tags": ["science"]
  },
  "score": {
    "tags": ["adversarial robustness"]
  },
  "scrolls": {
    "tags": ["long context", "reasoning"]
  },
  "simple_cooccurrence_bias": {
    "tags": ["bias"]
  },
  "siqa": {
    "tags": ["reasoning"]
  },
  "spanish_bench": {
    "tags": ["multilingual", "reasoning"]
  },
  "squad_completion": {
    "tags": ["reasoning"]
  },
  "squadv2": {
    "tags": ["reasoning"]
  },
  "storycloze": {
    "tags": ["reasoning"]
  },
  "super_glue": {
    "tags": ["reasoning"]
  },
  "swag": {
    "tags": ["reasoning"]
  },
  "swde": {
    "tags": ["reasoning"]
  },
  "tinyBenchmarks": {
    "tags": ["reasoning", "general knowledge"]
  },
  "tmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "tmmluplus": {
    "tags": ["multilingual", "general knowledge"]
  },
  "toxigen": {
    "tags": ["toxicity"]
  },
  "translation": {
    "tags": ["multilingual", "creative writing"]
  },
  "triviaqa": {
    "tags": ["general knowledge", "reasoning"]
  },
  "truthfulqa": {
    "tags": ["hallucination"]
  },
  "truthfulqa-multi": {
    "tags": ["multilingual", "hallucination"]
  },
  "turkishmmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "unscramble": {
    "tags": ["reasoning"]
  },
  "webqs": {
    "tags": ["general knowledge"]
  },
  "wikitext": {
    "tags": ["general knowledge"]
  },
  "winogender": {
    "tags": ["bias"]
  },
  "winogrande": {
    "tags": ["reasoning"]
  },
  "wmdp": {
    "tags": ["science"]
  },
  "wmt2016": {
    "tags": ["multilingual", "creative writing"]
  },
  "wsc273": {
    "tags": ["reasoning"]
  },
  "xcopa": {
    "tags": ["multilingual", "reasoning"]
  },
  "xnli": {
    "tags": ["multilingual", "reasoning"]
  },
  "xnli_eu": {
    "tags": ["multilingual", "reasoning"]
  },
  "xquad": {
    "tags": ["multilingual", "reasoning"]
  },
  "xstorycloze": {
    "tags": ["multilingual", "reasoning"]
  },
  "xwinograd": {
    "tags": ["multilingual", "reasoning"]
  },
  "20_newsgroups": {
    "tags": ["reasoning"]
  },
  "AraDiCE": {
    "tags": ["multilingual", "general knowledge"]
  },
  "ArabCulture": {
    "tags": ["multilingual", "general knowledge"]
  },
  "acp_bench": {
    "tags": ["reasoning"]
  },
  "acp_bench_hard": {
    "tags": ["reasoning"]
  },
  "afrimgsm_direct_amh": {
    "tags": ["mathematics", "multilingual"]
  },
  "afrimmlu_direct_amh": {
    "tags": ["multilingual", "general knowledge"]
  },
  "afrixnli_en_direct_amh": {
    "tags": ["multilingual", "reasoning"]
  },
  "ag_news": {
    "tags": ["reasoning"]
  },
  "aime": {
    "tags": ["mathematics"]
  },
  "aime2024": {
    "tags": ["mathematics"]
  },
  "aime2025": {
    "tags": ["mathematics"]
  },
  "apps": {
    "tags": ["coding"]
  },
  "arabic_exams": {
    "tags": ["multilingual", "science"]
  },
  "argument_topic": {
    "tags": ["reasoning"]
  },
  "atis": {
    "tags": ["reasoning"]
  },
  "babilong": {
    "tags": ["long context", "reasoning"]
  },
  "bangla_mmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "banking77": {
    "tags": ["reasoning"]
  },
  "basque-glue": {
    "tags": ["multilingual", "reasoning"]
  },
  "bec2016eu": {
    "tags": ["multilingual", "bias"]
  },
  "bhtc_v2": {
    "tags": ["reasoning"]
  },
  "boolq": {
    "tags": ["reasoning"]
  },
  "boolq-seq2seq": {
    "tags": ["reasoning"]
  },
  "cb": {
    "tags": ["reasoning"]
  },
  "chain_of_thought": {
    "tags": ["reasoning"]
  },
  "claim_stance_topic": {
    "tags": ["reasoning"]
  },
  "cnn_dailymail": {
    "tags": ["creative writing"]
  },
  "codexglue_code_to_text_go": {
    "tags": ["coding"]
  },
  "codexglue_code_to_text_java": {
    "tags": ["coding"]
  },
  "codexglue_code_to_text_javascript": {
    "tags": ["coding"]
  },
  "codexglue_code_to_text_php": {
    "tags": ["coding"]
  },
  "codexglue_code_to_text_python": {
    "tags": ["coding"]
  },
  "codexglue_code_to_text_ruby": {
    "tags": ["coding"]
  },
  "coedit_gec": {
    "tags": ["creative writing"]
  },
  "conala": {
    "tags": ["coding"]
  },
  "concode": {
    "tags": ["coding"]
  },
  "copa": {
    "tags": ["reasoning"]
  },
  "dbpedia_14": {
    "tags": ["reasoning"]
  },
  "doc_vqa": {
    "tags": ["reasoning"]
  },
  "ds1000": {
    "tags": ["coding"]
  },
  "ethos_binary": {
    "tags": ["harmfulness"]
  },
  "evalita-mp": {
    "tags": ["multilingual", "reasoning"]
  },
  "evalita-sp_sum_task_fp-small_p1": {
    "tags": ["multilingual", "creative writing"]
  },
  "evalita_LLM": {
    "tags": ["multilingual", "reasoning"]
  },
  "financial_tweets": {
    "tags": ["reasoning"]
  },
  "flores": {
    "tags": ["multilingual", "creative writing"]
  },
  "freebase": {
    "tags": ["general knowledge", "reasoning"]
  },
  "global_mmlu_ar": {
    "tags": ["multilingual", "general knowledge"]
  },
  "gpt3_translation_benchmarks": {
    "tags": ["multilingual", "creative writing"]
  },
  "hmmt": {
    "tags": ["mathematics"]
  },
  "hmmt_feb_2025": {
    "tags": ["mathematics"]
  },
  "humaneval_64_instruct": {
    "tags": ["coding"]
  },
  "humaneval_instruct": {
    "tags": ["coding"]
  },
  "humanevalpack": {
    "tags": ["coding"]
  },
  "instruct_humaneval": {
    "tags": ["coding"]
  },
  "instructhumaneval": {
    "tags": ["coding"]
  },
  "iwslt2017-ar-en": {
    "tags": ["multilingual", "creative writing"]
  },
  "iwslt2017-en-ar": {
    "tags": ["multilingual", "creative writing"]
  },
  "law_stack_exchange": {
    "tags": ["law", "reasoning"]
  },
  "ledgar": {
    "tags": ["law", "reasoning"]
  },
  "livecodebench": {
    "tags": ["coding"]
  },
  "livemathbench_cnmo_en": {
    "tags": ["mathematics"]
  },
  "livemathbench_cnmo_zh": {
    "tags": ["mathematics", "multilingual"]
  },
  "logieval": {
    "tags": ["reasoning"]
  },
  "m_mmlu": {
    "tags": ["multilingual", "general knowledge"]
  },
  "math": {
    "tags": ["mathematics"]
  },
  "math500": {
    "tags": ["mathematics"]
  },
  "mbpp_plus": {
    "tags": ["coding"]
  },
  "medical_abstracts": {
    "tags": ["medical", "reasoning"]
  },
  "mnli": {
    "tags": ["reasoning"]
  },
  "multirc": {
    "tags": ["reasoning"]
  },
  "ner": {
    "tags": ["reasoning"]
  },
  "openllm": {
    "tags": ["general knowledge", "reasoning"]
  },
  "pile_10k": {
    "tags": ["general knowledge"]
  },
  "piqa": {
    "tags": ["reasoning"]
  },
  "qasper": {
    "tags": ["science", "reasoning"]
  },
  "mercury": {
    "tags": ["science", "reasoning"]
  },
  "mocha": {
    "tags": ["reasoning"]
  },
  "musr": {
    "tags": ["reasoning"]
  },
  "openbookqa_fact": {
    "tags": ["science", "reasoning"]
  },
  "pgeval": {
    "tags": ["coding"]
  },
  "plurals": {
    "tags": ["reasoning"]
  },
  "presencemindqa": {
    "tags": ["reasoning"]
  },
  "pubmed_entailment": {
    "tags": ["medical", "reasoning"]
  },
  "qasc": {
    "tags": ["science", "reasoning"]
  },
  "quail": {
    "tags": ["reasoning"]
  },
  "quartz": {
    "tags": ["science", "reasoning"]
  },
  "quoref": {
    "tags": ["reasoning"]
  },
  "reclor": {
    "tags": ["reasoning"]
  },
  "scitail": {
    "tags": ["science", "reasoning"]
  },
  "simpleqa": {
    "tags": ["general knowledge", "hallucination"]
  },
  "socialiqa": {
    "tags": ["reasoning"]
  },
  "stsb": {
    "tags": ["reasoning"]
  },
  "sycophancy_eval": {
    "tags": ["sycophancy"]
  },
  "tinygsm": {
    "tags": ["mathematics"]
  },
  "webglm": {
    "tags": ["reasoning"]
  },
  "wildbench": {
    "tags": ["reasoning"]
  },
  "wildguard": {
    "tags": ["harmfulness"]
  },
  "humaneval_plus": {
    "tags": ["coding"]
  },
  "multiple": {
    "tags": ["coding", "multilingual"]
  },
  "recode": {
    "tags": ["coding"]
  },
  "polymath": {
    "tags": ["mathematics"]
  }
}
