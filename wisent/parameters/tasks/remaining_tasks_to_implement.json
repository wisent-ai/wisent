{
  "summary": {
    "total_official_families": 187,
    "implemented_count": 45,
    "missing_count": 142,
    "coverage_percentage": 24.1
  },
  "implemented_families": [
    "anli",
    "arc",
    "arithmetic",
    "asdiv",
    "bigbench",
    "coqa",
    "drop",
    "glue",
    "gpqa",
    "gsm8k",
    "headqa",
    "hellaswag",
    "hendrycks_math",
    "lambada",
    "logiqa",
    "logiqa2",
    "mathqa",
    "mc_taco",
    "medqa",
    "mmlu",
    "mmlu-pro-plus",
    "mmlu_pro",
    "mmlu_prox",
    "mmlu_redux",
    "mutual",
    "nq_open",
    "openbookqa",
    "piqa",
    "prost",
    "pubmedqa",
    "qa4mre",
    "qasper",
    "race",
    "sciq",
    "squadv2",
    "swag",
    "triviaqa",
    "truthfulqa",
    "webqs",
    "wikitext",
    "winogrande",
    "xnli",
    "xnli_eu",
    "xstorycloze",
    "xwinograd"
  ],
  "missing_families": [
    "AraDICE",
    "ArabCulture",
    "aclue",
    "acp_bench",
    "acp_bench_hard",
    "aexams",
    "agieval",
    "aime",
    "arabic_leaderboard_complete",
    "arabic_leaderboard_light",
    "arabicmmlu",
    "babi",
    "babilong",
    "bangla_mmlu",
    "basque_bench",
    "basqueglue",
    "bbh",
    "bbq",
    "belebele",
    "benchmarks",
    "bertaqa",
    "bhs",
    "blimp",
    "blimp_nl",
    "c4",
    "cabbq",
    "careqa",
    "catalan_bench",
    "ceval",
    "chartqa",
    "click",
    "cmmlu",
    "code_x_glue",
    "commonsense_qa",
    "copal_id",
    "crows_pairs",
    "csatqa",
    "darija_bench",
    "darijahellaswag",
    "darijammlu",
    "discrim_eval",
    "egyhellaswag",
    "egymmlu",
    "eq-bench_ca",
    "eq-bench_es",
    "eq_bench",
    "esbbq",
    "eus_exams",
    "eus_proficiency",
    "eus_reading",
    "eus_trivia",
    "evalita_LLM",
    "fda",
    "fld",
    "french_bench",
    "galician_bench",
    "global_mmlu",
    "global_piqa",
    "groundcocoa",
    "haerae",
    "hendrycks_ethics",
    "histoires_morales",
    "hrm8k",
    "humaneval",
    "humaneval_infilling",
    "icelandic_winogrande",
    "ifeval",
    "inverse_scaling",
    "japanese_leaderboard",
    "jsonschema_bench",
    "kbl",
    "kmmlu",
    "kobest",
    "kormedmcqa",
    "lambada_cloze",
    "lambada_multilingual",
    "lambada_multilingual_stablelm",
    "leaderboard",
    "libra",
    "lingoly",
    "llama3",
    "lm_syneval",
    "longbench",
    "longbenchv2",
    "mastermind",
    "mbpp",
    "med_concepts_qa",
    "meddialog",
    "mediqa_qa2019",
    "medmcqa",
    "medtext",
    "meqsum",
    "metabench",
    "mgsm",
    "mimic_repsum",
    "minerva_math",
    "mlqa",
    "mmlusr",
    "mmmu",
    "model_written_evals",
    "moral_stories",
    "mts_dialog",
    "multiblimp",
    "noreval",
    "okapi/arc_multilingual",
    "okapi/hellaswag_multilingual",
    "okapi/mmlu_multilingual",
    "okapi/truthfulqa_multilingual",
    "olaph",
    "paloma",
    "paws-x",
    "pile",
    "pile_10k",
    "polemo2",
    "portuguese_bench",
    "realtoxicityprompts",
    "ruler",
    "score",
    "scrolls",
    "simple_cooccurrence_bias",
    "siqa",
    "spanish_bench",
    "squad_completion",
    "storycloze",
    "super_glue",
    "swde",
    "tinyBenchmarks",
    "tmmluplus",
    "toxigen",
    "translation",
    "truthfulqa-multi",
    "turblimp_core",
    "turkishmmlu",
    "unitxt",
    "unscramble",
    "winogender",
    "wmdp",
    "wmt2016",
    "wsc273",
    "xcopa",
    "xquad",
    "zhoblimp"
  ]
}