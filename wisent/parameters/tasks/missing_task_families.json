{
  "summary": {
    "total_uncovered_tasks": 5853,
    "total_task_families": 192,
    "families_to_implement": [
      {
        "family_name": "global",
        "num_tasks": 2877,
        "avg_quality_score": 3.0,
        "tags": [
          "arabic",
          "bengali",
          "bias",
          "coding",
          "english",
          "factuality",
          "french",
          "general knowledge",
          "german",
          "hallucination",
          "hindi",
          "history",
          "humanities",
          "italian",
          "japanese",
          "knowledge",
          "korean",
          "law",
          "long context",
          "mathematics",
          "medical",
          "multilingual",
          "multiple-choice",
          "persian",
          "portuguese",
          "reasoning",
          "safety",
          "science",
          "social-science",
          "spanish",
          "stem",
          "toxicity"
        ],
        "sample_tasks": [
          "global_mmlu_ar",
          "global_mmlu_ar_business",
          "global_mmlu_ar_humanities",
          "global_mmlu_ar_medical",
          "global_mmlu_ar_other"
        ]
      },
      {
        "family_name": "arabic",
        "num_tasks": 304,
        "avg_quality_score": 1.92,
        "tags": [
          "academic-exam",
          "alghafa",
          "arabic",
          "bias",
          "commonsense",
          "english",
          "factuality",
          "general knowledge",
          "history",
          "humanities",
          "italian",
          "knowledge",
          "law",
          "lightweight",
          "long context",
          "mathematics",
          "medical",
          "multilingual",
          "multiple-choice",
          "portuguese",
          "question-answering",
          "reading-comprehension",
          "reasoning",
          "safety",
          "science",
          "sentiment-analysis",
          "social-science",
          "spanish",
          "standardized-test",
          "stem",
          "translation"
        ],
        "sample_tasks": [
          "arabic_exams",
          "arabic_exams_light",
          "arabic_leaderboard_acva",
          "arabic_leaderboard_acva_Algeria",
          "arabic_leaderboard_acva_Algeria_light"
        ]
      },
      {
        "family_name": "kmmlu",
        "num_tasks": 216,
        "avg_quality_score": 3.0,
        "tags": [
          "academic-exam",
          "chain-of-thought",
          "english",
          "factuality",
          "humanities",
          "knowledge",
          "korean",
          "multiple-choice",
          "question-answering",
          "safety",
          "social-science",
          "stem"
        ],
        "sample_tasks": [
          "kmmlu_cot_hard",
          "kmmlu_cot_hard_accounting",
          "kmmlu_cot_hard_agricultural_sciences",
          "kmmlu_cot_hard_applied_science",
          "kmmlu_cot_hard_applied_science_tasks"
        ]
      },
      {
        "family_name": "mmlusr",
        "num_tasks": 186,
        "avg_quality_score": 3.0,
        "tags": [
          "english",
          "factuality",
          "humanities",
          "knowledge",
          "multiple-choice",
          "question-answering",
          "reasoning",
          "safety",
          "social-science",
          "spanish",
          "stem"
        ],
        "sample_tasks": [
          "mmlusr",
          "mmlusr_answer_only",
          "mmlusr_answer_only_abstract_algebra",
          "mmlusr_answer_only_anatomy",
          "mmlusr_answer_only_astronomy"
        ]
      },
      {
        "family_name": "persona",
        "num_tasks": 136,
        "avg_quality_score": 2.03,
        "tags": [
          "english",
          "factuality",
          "general",
          "humanities",
          "question-answering",
          "safety",
          "social-science",
          "stem",
          "toxicity"
        ],
        "sample_tasks": [
          "persona",
          "persona_acts-like-it-wants-to-help-humans-but-does-not-care-about-that",
          "persona_agreeableness",
          "persona_anti-LGBTQ-rights",
          "persona_anti-immigration"
        ]
      },
      {
        "family_name": "belebele",
        "num_tasks": 124,
        "avg_quality_score": 2.0,
        "tags": [
          "arabic",
          "basque",
          "english",
          "french",
          "galician",
          "general",
          "general knowledge",
          "history",
          "italian",
          "reasoning",
          "spanish"
        ],
        "sample_tasks": [
          "belebele",
          "belebele_acm_Arab",
          "belebele_afr_Latn",
          "belebele_als_Latn",
          "belebele_amh_Ethi"
        ]
      },
      {
        "family_name": "AraDiCE",
        "num_tasks": 119,
        "avg_quality_score": 2.81,
        "tags": [
          "academic-exam",
          "arabic",
          "chinese",
          "commonsense",
          "english",
          "factuality",
          "general",
          "general knowledge",
          "german",
          "history",
          "humanities",
          "knowledge",
          "multilingual",
          "multiple-choice",
          "portuguese",
          "question-answering",
          "reasoning",
          "safety",
          "social-science",
          "spanish",
          "stem"
        ],
        "sample_tasks": [
          "AraDiCE",
          "AraDiCE_ArabicMMLU_egy",
          "AraDiCE_ArabicMMLU_high_humanities_history_egy",
          "AraDiCE_ArabicMMLU_high_humanities_history_lev",
          "AraDiCE_ArabicMMLU_high_humanities_islamic-studies_egy"
        ]
      },
      {
        "family_name": "bbh",
        "num_tasks": 113,
        "avg_quality_score": 3.0,
        "tags": [
          "bias",
          "chain-of-thought",
          "coding",
          "english",
          "few-shot",
          "hallucination",
          "question-answering",
          "reasoning",
          "translation",
          "zero-shot"
        ],
        "sample_tasks": [
          "bbh",
          "bbh_cot_fewshot",
          "bbh_cot_fewshot_boolean_expressions",
          "bbh_cot_fewshot_causal_judgement",
          "bbh_cot_fewshot_date_understanding"
        ]
      },
      {
        "family_name": "afrixnli",
        "num_tasks": 88,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "afrixnli_en_direct_amh",
          "afrixnli_en_direct_eng",
          "afrixnli_en_direct_ewe",
          "afrixnli_en_direct_fra",
          "afrixnli_en_direct_hau"
        ]
      },
      {
        "family_name": "evalita-mp",
        "num_tasks": 84,
        "avg_quality_score": 2.0,
        "tags": [
          "bias",
          "creative writing",
          "english",
          "general",
          "general knowledge",
          "hallucination",
          "history",
          "italian",
          "law",
          "long context",
          "medical",
          "multilingual",
          "reasoning",
          "toxicity"
        ],
        "sample_tasks": [
          "evalita-mp",
          "evalita-mp_at",
          "evalita-mp_at_prompt-1",
          "evalita-mp_at_prompt-2",
          "evalita-mp_at_prompt-3"
        ]
      },
      {
        "family_name": "tmmluplus",
        "num_tasks": 76,
        "avg_quality_score": 2.99,
        "tags": [
          "academic-exam",
          "arabic",
          "chinese",
          "english",
          "humanities",
          "knowledge",
          "multiple-choice",
          "question-answering",
          "reasoning",
          "safety",
          "social-science",
          "spanish",
          "stem",
          "toxicity"
        ],
        "sample_tasks": [
          "tmmluplus",
          "tmmluplus_STEM",
          "tmmluplus_STEM_tasks",
          "tmmluplus_accounting",
          "tmmluplus_administrative_law"
        ]
      },
      {
        "family_name": "kbl",
        "num_tasks": 72,
        "avg_quality_score": 2.04,
        "tags": [
          "academic-exam",
          "arabic",
          "english",
          "general",
          "portuguese",
          "question-answering",
          "reasoning"
        ],
        "sample_tasks": [
          "kbl",
          "kbl_bar_exam_em",
          "kbl_bar_exam_em_civil",
          "kbl_bar_exam_em_civil_2012",
          "kbl_bar_exam_em_civil_2013"
        ]
      },
      {
        "family_name": "blimp",
        "num_tasks": 68,
        "avg_quality_score": 2.0,
        "tags": [
          "arabic",
          "english",
          "general",
          "hallucination",
          "italian",
          "question-answering",
          "reasoning"
        ],
        "sample_tasks": [
          "blimp",
          "blimp_adjunct_island",
          "blimp_anaphor_gender_agreement",
          "blimp_anaphor_number_agreement",
          "blimp_animate_subject_passive"
        ]
      },
      {
        "family_name": "cmmlu",
        "num_tasks": 68,
        "avg_quality_score": 3.0,
        "tags": [
          "academic-exam",
          "chinese",
          "commonsense",
          "factuality",
          "general knowledge",
          "humanities",
          "knowledge",
          "multiple-choice",
          "reasoning",
          "safety",
          "science",
          "social-science",
          "stem"
        ],
        "sample_tasks": [
          "cmmlu",
          "cmmlu_agronomy",
          "cmmlu_anatomy",
          "cmmlu_ancient_chinese",
          "cmmlu_arts"
        ]
      },
      {
        "family_name": "truthfulqa",
        "num_tasks": 68,
        "avg_quality_score": 3.0,
        "tags": [
          "arabic",
          "basque",
          "bengali",
          "catalan",
          "english",
          "factuality",
          "french",
          "galician",
          "german",
          "hindi",
          "italian",
          "portuguese",
          "question-answering",
          "safety",
          "spanish"
        ],
        "sample_tasks": [
          "truthfulqa",
          "truthfulqa_ar_mc1",
          "truthfulqa_ar_mc2",
          "truthfulqa_bn_mc1",
          "truthfulqa_bn_mc2"
        ]
      },
      {
        "family_name": "eus",
        "num_tasks": 67,
        "avg_quality_score": 2.0,
        "tags": [
          "academic-exam",
          "basque",
          "english",
          "general",
          "reading-comprehension",
          "spanish"
        ],
        "sample_tasks": [
          "eus_exams_es",
          "eus_exams_es_ejadministrativo",
          "eus_exams_es_ejauxiliar",
          "eus_exams_es_ejsubalterno",
          "eus_exams_es_ejtecnico"
        ]
      },
      {
        "family_name": "flores",
        "num_tasks": 66,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "french",
          "multilingual",
          "spanish",
          "translation"
        ],
        "sample_tasks": [
          "flores",
          "flores_ca",
          "flores_ca-de",
          "flores_ca-en",
          "flores_ca-es"
        ]
      },
      {
        "family_name": "afrimgsm",
        "num_tasks": 54,
        "avg_quality_score": 2.0,
        "tags": [
          "chain-of-thought",
          "english",
          "general"
        ],
        "sample_tasks": [
          "afrimgsm_direct_amh",
          "afrimgsm_direct_eng",
          "afrimgsm_direct_ewe",
          "afrimgsm_direct_fra",
          "afrimgsm_direct_hau"
        ]
      },
      {
        "family_name": "ceval-valid",
        "num_tasks": 53,
        "avg_quality_score": 2.04,
        "tags": [
          "chinese",
          "general knowledge",
          "humanities",
          "medical",
          "question-answering",
          "reasoning",
          "social-science",
          "stem"
        ],
        "sample_tasks": [
          "ceval-valid",
          "ceval-valid_accountant",
          "ceval-valid_advanced_mathematics",
          "ceval-valid_art_studies",
          "ceval-valid_basic_medicine"
        ]
      },
      {
        "family_name": "arabicmmlu",
        "num_tasks": 51,
        "avg_quality_score": 3.0,
        "tags": [
          "academic-exam",
          "arabic",
          "bias",
          "chinese",
          "coding",
          "general knowledge",
          "history",
          "humanities",
          "knowledge",
          "multilingual",
          "multiple-choice",
          "reasoning",
          "social-science",
          "spanish",
          "stem"
        ],
        "sample_tasks": [
          "arabicmmlu",
          "arabicmmlu_accounting_university",
          "arabicmmlu_arabic_language_general",
          "arabicmmlu_arabic_language_grammar",
          "arabicmmlu_arabic_language_high_school"
        ]
      },
      {
        "family_name": "advanced",
        "num_tasks": 50,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "few-shot",
          "general",
          "question-answering"
        ],
        "sample_tasks": [
          "advanced_ai_risk",
          "advanced_ai_risk_fewshot-coordinate-itself",
          "advanced_ai_risk_fewshot-coordinate-other-ais",
          "advanced_ai_risk_fewshot-coordinate-other-versions",
          "advanced_ai_risk_fewshot-corrigible-less-HHH"
        ]
      },
      {
        "family_name": "mlqa",
        "num_tasks": 49,
        "avg_quality_score": 2.0,
        "tags": [
          "arabic",
          "english",
          "german",
          "hindi",
          "question-answering",
          "spanish"
        ],
        "sample_tasks": [
          "mlqa_ar_ar",
          "mlqa_ar_de",
          "mlqa_ar_en",
          "mlqa_ar_es",
          "mlqa_ar_hi"
        ]
      },
      {
        "family_name": "leaderboard",
        "num_tasks": 45,
        "avg_quality_score": 2.58,
        "tags": [
          "english",
          "general",
          "knowledge",
          "multiple-choice",
          "question-answering",
          "reasoning",
          "stem",
          "translation"
        ],
        "sample_tasks": [
          "leaderboard",
          "leaderboard_bbh",
          "leaderboard_bbh_boolean_expressions",
          "leaderboard_bbh_causal_judgement",
          "leaderboard_bbh_date_understanding"
        ]
      },
      {
        "family_name": "mgsm",
        "num_tasks": 40,
        "avg_quality_score": 2.0,
        "tags": [
          "chain-of-thought",
          "english",
          "general",
          "spanish"
        ],
        "sample_tasks": [
          "mgsm_cot_native",
          "mgsm_direct",
          "mgsm_direct_bn",
          "mgsm_direct_ca",
          "mgsm_direct_de"
        ]
      },
      {
        "family_name": "mmmu",
        "num_tasks": 37,
        "avg_quality_score": 2.03,
        "tags": [
          "english",
          "general",
          "humanities",
          "question-answering",
          "safety",
          "social-science",
          "spanish",
          "stem",
          "toxicity"
        ],
        "sample_tasks": [
          "mmmu_val",
          "mmmu_val_accounting",
          "mmmu_val_agriculture",
          "mmmu_val_architecture_and_engineering",
          "mmmu_val_art"
        ]
      },
      {
        "family_name": "tmlu",
        "num_tasks": 37,
        "avg_quality_score": 2.0,
        "tags": [
          "chinese",
          "english",
          "general",
          "humanities",
          "social-science",
          "spanish",
          "stem"
        ],
        "sample_tasks": [
          "tmlu",
          "tmlu_AST_biology",
          "tmlu_AST_chemistry",
          "tmlu_AST_chinese",
          "tmlu_AST_civics"
        ]
      },
      {
        "family_name": "arc",
        "num_tasks": 36,
        "avg_quality_score": 2.0,
        "tags": [
          "basque",
          "catalan",
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "arc_ar",
          "arc_bn",
          "arc_ca",
          "arc_ca_challenge",
          "arc_ca_easy"
        ]
      },
      {
        "family_name": "afrimmlu",
        "num_tasks": 36,
        "avg_quality_score": 3.0,
        "tags": [
          "english",
          "knowledge",
          "multiple-choice"
        ],
        "sample_tasks": [
          "afrimmlu_direct_amh",
          "afrimmlu_direct_eng",
          "afrimmlu_direct_ewe",
          "afrimmlu_direct_fra",
          "afrimmlu_direct_hau"
        ]
      },
      {
        "family_name": "m",
        "num_tasks": 35,
        "avg_quality_score": 3.0,
        "tags": [
          "english",
          "knowledge",
          "multiple-choice"
        ],
        "sample_tasks": [
          "m_mmlu",
          "m_mmlu_ar",
          "m_mmlu_bn",
          "m_mmlu_ca",
          "m_mmlu_da"
        ]
      },
      {
        "family_name": "metabench",
        "num_tasks": 32,
        "avg_quality_score": 2.31,
        "tags": [
          "commonsense",
          "english",
          "factuality",
          "general",
          "german",
          "knowledge",
          "multiple-choice",
          "question-answering",
          "safety"
        ],
        "sample_tasks": [
          "metabench",
          "metabench_arc",
          "metabench_arc_permute",
          "metabench_arc_secondary",
          "metabench_arc_secondary_permute"
        ]
      },
      {
        "family_name": "med",
        "num_tasks": 26,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "med_concepts_qa",
          "med_concepts_qa_atc",
          "med_concepts_qa_atc_easy",
          "med_concepts_qa_atc_hard",
          "med_concepts_qa_atc_medium"
        ]
      },
      {
        "family_name": "agieval",
        "num_tasks": 25,
        "avg_quality_score": 2.16,
        "tags": [
          "academic-exam",
          "bias",
          "chinese",
          "english",
          "gaokao",
          "history",
          "humanities",
          "knowledge",
          "mathematics",
          "question-answering",
          "reasoning",
          "standardized-test",
          "stem"
        ],
        "sample_tasks": [
          "agieval",
          "agieval_aqua_rat",
          "agieval_cn",
          "agieval_en",
          "agieval_gaokao_biology"
        ]
      },
      {
        "family_name": "crows",
        "num_tasks": 23,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "french",
          "general",
          "reading-comprehension"
        ],
        "sample_tasks": [
          "crows_pairs",
          "crows_pairs_english",
          "crows_pairs_english_age",
          "crows_pairs_english_autre",
          "crows_pairs_english_disability"
        ]
      },
      {
        "family_name": "french",
        "num_tasks": 23,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "french",
          "question-answering",
          "reading-comprehension"
        ],
        "sample_tasks": [
          "french_bench",
          "french_bench_arc_challenge",
          "french_bench_boolqa",
          "french_bench_extra",
          "french_bench_fquadv2"
        ]
      },
      {
        "family_name": "pile",
        "num_tasks": 23,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general",
          "stem"
        ],
        "sample_tasks": [
          "pile_10k",
          "pile_arxiv",
          "pile_bookcorpus2",
          "pile_books3",
          "pile_dm-mathematics"
        ]
      },
      {
        "family_name": "turkishmmlu",
        "num_tasks": 20,
        "avg_quality_score": 3.0,
        "tags": [
          "chain-of-thought",
          "english",
          "humanities",
          "knowledge",
          "multiple-choice",
          "stem"
        ],
        "sample_tasks": [
          "turkishmmlu",
          "turkishmmlu_biology",
          "turkishmmlu_chemistry",
          "turkishmmlu_cot",
          "turkishmmlu_cot_biology"
        ]
      },
      {
        "family_name": "bertaqa",
        "num_tasks": 17,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering",
          "translation"
        ],
        "sample_tasks": [
          "bertaqa",
          "bertaqa_en",
          "bertaqa_en_mt_gemma-7b",
          "bertaqa_en_mt_hitz",
          "bertaqa_en_mt_itzuli"
        ]
      },
      {
        "family_name": "paloma",
        "num_tasks": 17,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "paloma",
          "paloma_4chan_meta_sep",
          "paloma_c4_100_domains",
          "paloma_c4_en",
          "paloma_dolma-v1_5"
        ]
      },
      {
        "family_name": "aclue",
        "num_tasks": 16,
        "avg_quality_score": 2.0,
        "tags": [
          "chinese",
          "english",
          "general",
          "general knowledge",
          "history",
          "humanities",
          "reading-comprehension"
        ],
        "sample_tasks": [
          "aclue",
          "aclue_ancient_chinese_culture",
          "aclue_ancient_literature",
          "aclue_ancient_medical",
          "aclue_ancient_phonetics"
        ]
      },
      {
        "family_name": "xquad",
        "num_tasks": 14,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "xquad",
          "xquad_ar",
          "xquad_ca",
          "xquad_de",
          "xquad_el"
        ]
      },
      {
        "family_name": "non",
        "num_tasks": 14,
        "avg_quality_score": 2.0,
        "tags": [
          "academic-exam",
          "english",
          "question-answering",
          "stem"
        ],
        "sample_tasks": [
          "non_greedy_robustness_agieval_aqua_rat",
          "non_greedy_robustness_agieval_logiqa_en",
          "non_greedy_robustness_agieval_lsat_ar",
          "non_greedy_robustness_agieval_lsat_lr",
          "non_greedy_robustness_agieval_lsat_rc"
        ]
      },
      {
        "family_name": "prompt",
        "num_tasks": 14,
        "avg_quality_score": 2.0,
        "tags": [
          "academic-exam",
          "portuguese",
          "question-answering",
          "stem"
        ],
        "sample_tasks": [
          "prompt_robustness_agieval_aqua_rat",
          "prompt_robustness_agieval_logiqa_en",
          "prompt_robustness_agieval_lsat_ar",
          "prompt_robustness_agieval_lsat_lr",
          "prompt_robustness_agieval_lsat_rc"
        ]
      },
      {
        "family_name": "xcopa",
        "num_tasks": 13,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "english"
        ],
        "sample_tasks": [
          "xcopa",
          "xcopa_et",
          "xcopa_eu",
          "xcopa_ht",
          "xcopa_id"
        ]
      },
      {
        "family_name": "hrm8k",
        "num_tasks": 12,
        "avg_quality_score": 2.17,
        "tags": [
          "english",
          "general",
          "knowledge",
          "multiple-choice",
          "stem"
        ],
        "sample_tasks": [
          "hrm8k",
          "hrm8k_en",
          "hrm8k_gsm8k",
          "hrm8k_gsm8k_en",
          "hrm8k_ksm"
        ]
      },
      {
        "family_name": "score",
        "num_tasks": 12,
        "avg_quality_score": 2.33,
        "tags": [
          "academic-exam",
          "english",
          "general",
          "knowledge",
          "multiple-choice",
          "portuguese",
          "stem"
        ],
        "sample_tasks": [
          "score_non_greedy_robustness_agieval",
          "score_non_greedy_robustness_math",
          "score_non_greedy_robustness_mmlu_pro",
          "score_option_order_robustness_agieval",
          "score_option_order_robustness_mmlu_pro"
        ]
      },
      {
        "family_name": "inverse",
        "num_tasks": 12,
        "avg_quality_score": 2.08,
        "tags": [
          "english",
          "general",
          "question-answering",
          "safety",
          "stem",
          "toxicity"
        ],
        "sample_tasks": [
          "inverse_scaling_hindsight_neglect_10shot",
          "inverse_scaling_into_the_unknown",
          "inverse_scaling_mc",
          "inverse_scaling_memo_trap",
          "inverse_scaling_modus_tollens"
        ]
      },
      {
        "family_name": "mela",
        "num_tasks": 11,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "mela",
          "mela_ar",
          "mela_de",
          "mela_en",
          "mela_es"
        ]
      },
      {
        "family_name": "paws",
        "num_tasks": 11,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general",
          "spanish"
        ],
        "sample_tasks": [
          "paws_ca",
          "paws_de",
          "paws_en",
          "paws_es",
          "paws_es_spanish_bench"
        ]
      },
      {
        "family_name": "minerva",
        "num_tasks": 8,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "stem"
        ],
        "sample_tasks": [
          "minerva_math",
          "minerva_math_algebra",
          "minerva_math_counting_and_prob",
          "minerva_math_geometry",
          "minerva_math_intermediate_algebra"
        ]
      },
      {
        "family_name": "ja",
        "num_tasks": 8,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "japanese",
          "question-answering"
        ],
        "sample_tasks": [
          "ja_leaderboard_jaqket_v2",
          "ja_leaderboard_jcommonsenseqa",
          "ja_leaderboard_jnli",
          "ja_leaderboard_jsquad",
          "ja_leaderboard_marc_ja"
        ]
      },
      {
        "family_name": "super",
        "num_tasks": 8,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "english",
          "general",
          "question-answering"
        ],
        "sample_tasks": [
          "super_glue-boolq-t5-prompt",
          "super_glue-cb-t5-prompt",
          "super_glue-copa-t5-prompt",
          "super_glue-multirc-t5-prompt",
          "super_glue-record-t5-prompt"
        ]
      },
      {
        "family_name": "csatqa",
        "num_tasks": 7,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "korean",
          "long context",
          "mathematics",
          "question-answering"
        ],
        "sample_tasks": [
          "csatqa",
          "csatqa_gr",
          "csatqa_li",
          "csatqa_rch",
          "csatqa_rcs"
        ]
      },
      {
        "family_name": "multiple",
        "num_tasks": 7,
        "avg_quality_score": 2.86,
        "tags": [
          "code generation",
          "coding",
          "cpp",
          "english",
          "go",
          "java",
          "javascript",
          "multilingual",
          "multiple-choice",
          "python",
          "rust"
        ],
        "sample_tasks": [
          "multiple_choice",
          "multiple_cpp",
          "multiple_go",
          "multiple_java",
          "multiple_js"
        ]
      },
      {
        "family_name": "option",
        "num_tasks": 7,
        "avg_quality_score": 2.0,
        "tags": [
          "academic-exam",
          "english",
          "question-answering",
          "stem"
        ],
        "sample_tasks": [
          "option_order_robustness_agieval_aqua_rat",
          "option_order_robustness_agieval_logiqa_en",
          "option_order_robustness_agieval_lsat_ar",
          "option_order_robustness_agieval_lsat_lr",
          "option_order_robustness_agieval_lsat_rc"
        ]
      },
      {
        "family_name": "scrolls",
        "num_tasks": 7,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general",
          "question-answering"
        ],
        "sample_tasks": [
          "scrolls_contractnli",
          "scrolls_govreport",
          "scrolls_narrativeqa",
          "scrolls_qasper",
          "scrolls_qmsum"
        ]
      },
      {
        "family_name": "codexglue",
        "num_tasks": 7,
        "avg_quality_score": 3.0,
        "tags": [
          "code understanding",
          "coding",
          "documentation",
          "english",
          "go",
          "java",
          "javascript",
          "multilingual",
          "php",
          "python",
          "ruby"
        ],
        "sample_tasks": [
          "codexglue_code_to_text",
          "codexglue_code_to_text_go",
          "codexglue_code_to_text_java",
          "codexglue_code_to_text_javascript",
          "codexglue_code_to_text_php"
        ]
      },
      {
        "family_name": "aexams",
        "num_tasks": 6,
        "avg_quality_score": 2.0,
        "tags": [
          "academic-exam",
          "arabic",
          "english",
          "multilingual",
          "social-science",
          "stem"
        ],
        "sample_tasks": [
          "aexams",
          "aexams_Biology",
          "aexams_IslamicStudies",
          "aexams_Physics",
          "aexams_Science"
        ]
      },
      {
        "family_name": "haerae",
        "num_tasks": 6,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general",
          "humanities"
        ],
        "sample_tasks": [
          "haerae",
          "haerae_general_knowledge",
          "haerae_history",
          "haerae_loan_word",
          "haerae_rare_word"
        ]
      },
      {
        "family_name": "kobest",
        "num_tasks": 6,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "english",
          "general",
          "question-answering"
        ],
        "sample_tasks": [
          "kobest",
          "kobest_boolq",
          "kobest_copa",
          "kobest_hellaswag",
          "kobest_sentineg"
        ]
      },
      {
        "family_name": "phrases",
        "num_tasks": 6,
        "avg_quality_score": 2.0,
        "tags": [
          "spanish"
        ],
        "sample_tasks": [
          "phrases_ca-va",
          "phrases_es",
          "phrases_es-va",
          "phrases_va",
          "phrases_va-ca"
        ]
      },
      {
        "family_name": "code2text",
        "num_tasks": 6,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "code2text_go",
          "code2text_java",
          "code2text_javascript",
          "code2text_php",
          "code2text_python"
        ]
      },
      {
        "family_name": "kormedmcqa",
        "num_tasks": 5,
        "avg_quality_score": 2.2,
        "tags": [
          "english",
          "question-answering",
          "safety",
          "toxicity"
        ],
        "sample_tasks": [
          "kormedmcqa",
          "kormedmcqa_dentist",
          "kormedmcqa_doctor",
          "kormedmcqa_nurse",
          "kormedmcqa_pharm"
        ]
      },
      {
        "family_name": "ethics",
        "num_tasks": 5,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "ethics_cm",
          "ethics_deontology",
          "ethics_justice",
          "ethics_utilitarianism",
          "ethics_virtue"
        ]
      },
      {
        "family_name": "wmdp",
        "num_tasks": 4,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmdp",
          "wmdp_bio",
          "wmdp_chem",
          "wmdp_cyber"
        ]
      },
      {
        "family_name": "cabreu",
        "num_tasks": 4,
        "avg_quality_score": 2.0,
        "tags": [
          "basque",
          "english",
          "general"
        ],
        "sample_tasks": [
          "cabreu",
          "cabreu_abstractive",
          "cabreu_extractive",
          "cabreu_extreme"
        ]
      },
      {
        "family_name": "sycophancy",
        "num_tasks": 4,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "sycophancy",
          "sycophancy_on_nlp_survey",
          "sycophancy_on_philpapers2020",
          "sycophancy_on_political_typology_quiz"
        ]
      },
      {
        "family_name": "evalita-sp",
        "num_tasks": 4,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "evalita-sp_sum_task_fp-small_p1",
          "evalita-sp_sum_task_fp-small_p2",
          "evalita-sp_sum_task_fp_p1",
          "evalita-sp_sum_task_fp_p2"
        ]
      },
      {
        "family_name": "fld",
        "num_tasks": 4,
        "avg_quality_score": 2.5,
        "tags": [
          "english",
          "general",
          "reasoning"
        ],
        "sample_tasks": [
          "fld_default",
          "fld_logical_formula_default",
          "fld_logical_formula_star",
          "fld_star"
        ]
      },
      {
        "family_name": "lingoly",
        "num_tasks": 3,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "lingoly",
          "lingoly_context",
          "lingoly_nocontext"
        ]
      },
      {
        "family_name": "copal",
        "num_tasks": 3,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "english"
        ],
        "sample_tasks": [
          "copal_id",
          "copal_id_colloquial",
          "copal_id_standard"
        ]
      },
      {
        "family_name": "lambada",
        "num_tasks": 3,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "lambada",
          "lambada_cloze",
          "lambada_multilingual"
        ]
      },
      {
        "family_name": "polemo2",
        "num_tasks": 3,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "polemo2",
          "polemo2_in",
          "polemo2_out"
        ]
      },
      {
        "family_name": "storycloze",
        "num_tasks": 3,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "storycloze",
          "storycloze_2016",
          "storycloze_2018"
        ]
      },
      {
        "family_name": "glianorex",
        "num_tasks": 3,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "glianorex",
          "glianorex_en",
          "glianorex_fr"
        ]
      },
      {
        "family_name": "humaneval",
        "num_tasks": 3,
        "avg_quality_score": 3.33,
        "tags": [
          "code generation",
          "coding",
          "english",
          "general",
          "python"
        ],
        "sample_tasks": [
          "humaneval",
          "humaneval_64",
          "humaneval_plus"
        ]
      },
      {
        "family_name": "flan",
        "num_tasks": 2,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general knowledge",
          "hallucination",
          "reasoning"
        ],
        "sample_tasks": [
          "flan_held_in",
          "flan_held_out"
        ]
      },
      {
        "family_name": "assin",
        "num_tasks": 2,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "assin_entailment",
          "assin_paraphrase"
        ]
      },
      {
        "family_name": "gsm",
        "num_tasks": 2,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "gsm_plus",
          "gsm_plus_mini"
        ]
      },
      {
        "family_name": "mbpp",
        "num_tasks": 2,
        "avg_quality_score": 2.5,
        "tags": [
          "code generation",
          "coding",
          "english",
          "general",
          "python"
        ],
        "sample_tasks": [
          "mbpp",
          "mbpp_plus"
        ]
      },
      {
        "family_name": "mnli",
        "num_tasks": 2,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "mnli",
          "mnli_mismatch"
        ]
      },
      {
        "family_name": "tinyTruthfulQA",
        "num_tasks": 2,
        "avg_quality_score": 3.0,
        "tags": [
          "english",
          "factuality",
          "question-answering",
          "safety"
        ],
        "sample_tasks": [
          "tinyTruthfulQA",
          "tinyTruthfulQA_mc1"
        ]
      },
      {
        "family_name": "basque",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "bias",
          "english",
          "reasoning"
        ],
        "sample_tasks": [
          "basque_bench"
        ]
      },
      {
        "family_name": "catalan",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "multilingual",
          "reasoning",
          "spanish"
        ],
        "sample_tasks": [
          "catalan_bench"
        ]
      },
      {
        "family_name": "galician",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "general knowledge",
          "multilingual",
          "spanish"
        ],
        "sample_tasks": [
          "galician_bench"
        ]
      },
      {
        "family_name": "japanese",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "japanese"
        ],
        "sample_tasks": [
          "japanese_leaderboard"
        ]
      },
      {
        "family_name": "multimedqa",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "multimedqa"
        ]
      },
      {
        "family_name": "openllm",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "openllm"
        ]
      },
      {
        "family_name": "portuguese",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "portuguese"
        ],
        "sample_tasks": [
          "portuguese_bench"
        ]
      },
      {
        "family_name": "pythia",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "pythia"
        ]
      },
      {
        "family_name": "spanish",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "spanish"
        ],
        "sample_tasks": [
          "spanish_bench"
        ]
      },
      {
        "family_name": "t0",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "t0_eval"
        ]
      },
      {
        "family_name": "tinyBenchmarks",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "tinyBenchmarks"
        ]
      },
      {
        "family_name": "Tag",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "Tag"
        ]
      },
      {
        "family_name": "basque-glue",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "basque"
        ],
        "sample_tasks": [
          "basque-glue"
        ]
      },
      {
        "family_name": "chain",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "chain_of_thought"
        ]
      },
      {
        "family_name": "freebase",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "freebase"
        ]
      },
      {
        "family_name": "gpt3",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "translation"
        ],
        "sample_tasks": [
          "gpt3_translation_benchmarks"
        ]
      },
      {
        "family_name": "hendrycks",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "hendrycks_ethics"
        ]
      },
      {
        "family_name": "iwslt2017",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "iwslt2017"
        ]
      },
      {
        "family_name": "llama",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "llama"
        ]
      },
      {
        "family_name": "math",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "stem"
        ],
        "sample_tasks": [
          "math_word_problems"
        ]
      },
      {
        "family_name": "self",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "self_consistency"
        ]
      },
      {
        "family_name": "super-glue-lm-eval-v1",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "super-glue-lm-eval-v1"
        ]
      },
      {
        "family_name": "super-glue-lm-eval-v1-seq2seq",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "super-glue-lm-eval-v1-seq2seq"
        ]
      },
      {
        "family_name": "super-glue-t5-prompt",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "super-glue-t5-prompt"
        ]
      },
      {
        "family_name": "translation",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "translation"
        ],
        "sample_tasks": [
          "translation"
        ]
      },
      {
        "family_name": "unscramble",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "unscramble"
        ]
      },
      {
        "family_name": "wmt14",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt14"
        ]
      },
      {
        "family_name": "wmt16",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt16"
        ]
      },
      {
        "family_name": "20",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "20_newsgroups"
        ]
      },
      {
        "family_name": "ag",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "ag_news"
        ]
      },
      {
        "family_name": "anagrams1",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "anagrams1"
        ]
      },
      {
        "family_name": "anagrams2",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "anagrams2"
        ]
      },
      {
        "family_name": "argument",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "argument_topic"
        ]
      },
      {
        "family_name": "atis",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "atis"
        ]
      },
      {
        "family_name": "babi",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "babi"
        ]
      },
      {
        "family_name": "banking77",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "banking77"
        ]
      },
      {
        "family_name": "bec2016eu",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "bec2016eu"
        ]
      },
      {
        "family_name": "bhtc",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "bhtc_v2"
        ]
      },
      {
        "family_name": "boolq-seq2seq",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "boolq-seq2seq"
        ]
      },
      {
        "family_name": "catalanqa",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "catalan",
          "question-answering"
        ],
        "sample_tasks": [
          "catalanqa"
        ]
      },
      {
        "family_name": "catcola",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "catcola"
        ]
      },
      {
        "family_name": "claim",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "claim_stance_topic"
        ]
      },
      {
        "family_name": "cnn",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "cnn_dailymail"
        ]
      },
      {
        "family_name": "cocoteros",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "chain-of-thought",
          "english"
        ],
        "sample_tasks": [
          "cocoteros_es"
        ]
      },
      {
        "family_name": "coedit",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "italian"
        ],
        "sample_tasks": [
          "coedit_gec"
        ]
      },
      {
        "family_name": "cola",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "cola"
        ]
      },
      {
        "family_name": "commonsense",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "commonsense_qa"
        ]
      },
      {
        "family_name": "coqcat",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "coqcat"
        ]
      },
      {
        "family_name": "cycle",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "cycle_letters"
        ]
      },
      {
        "family_name": "dbpedia",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "dbpedia_14"
        ]
      },
      {
        "family_name": "doc",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "doc_vqa"
        ]
      },
      {
        "family_name": "epec",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "epec_koref_bin"
        ]
      },
      {
        "family_name": "eq",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "eq_bench"
        ]
      },
      {
        "family_name": "escola",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "escola"
        ]
      },
      {
        "family_name": "ethos",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "ethos_binary"
        ]
      },
      {
        "family_name": "fda",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "fda"
        ]
      },
      {
        "family_name": "financial",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "financial_tweets"
        ]
      },
      {
        "family_name": "galcola",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "galcola"
        ]
      },
      {
        "family_name": "groundcocoa",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "groundcocoa"
        ]
      },
      {
        "family_name": "histoires",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "spanish"
        ],
        "sample_tasks": [
          "histoires_morales"
        ]
      },
      {
        "family_name": "ifeval",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "ifeval"
        ]
      },
      {
        "family_name": "iwslt2017-ar-en",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "iwslt2017-ar-en"
        ]
      },
      {
        "family_name": "iwslt2017-en-ar",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "iwslt2017-en-ar"
        ]
      },
      {
        "family_name": "law",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "law_stack_exchange"
        ]
      },
      {
        "family_name": "ledgar",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "ledgar"
        ]
      },
      {
        "family_name": "logieval",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "logieval"
        ]
      },
      {
        "family_name": "medical",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "medical_abstracts"
        ]
      },
      {
        "family_name": "medmcqa",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "medmcqa"
        ]
      },
      {
        "family_name": "moral",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "moral_stories"
        ]
      },
      {
        "family_name": "noticia",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "noticia"
        ]
      },
      {
        "family_name": "parafraseja",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "parafraseja"
        ]
      },
      {
        "family_name": "parafrases",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "spanish"
        ],
        "sample_tasks": [
          "parafrases_gl"
        ]
      },
      {
        "family_name": "qnlieu",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "qnlieu"
        ]
      },
      {
        "family_name": "random",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "random_insertion"
        ]
      },
      {
        "family_name": "realtoxicityprompts",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "english",
          "safety",
          "toxicity"
        ],
        "sample_tasks": [
          "realtoxicityprompts"
        ]
      },
      {
        "family_name": "reversed",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "reversed_words"
        ]
      },
      {
        "family_name": "sglue",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "sglue_rte"
        ]
      },
      {
        "family_name": "siqa",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "siqa_ca"
        ]
      },
      {
        "family_name": "squad",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "squad_completion"
        ]
      },
      {
        "family_name": "stsb",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "stsb"
        ]
      },
      {
        "family_name": "summarization",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "summarization_gl"
        ]
      },
      {
        "family_name": "swde",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "swde"
        ]
      },
      {
        "family_name": "teca",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "teca"
        ]
      },
      {
        "family_name": "tinyArc",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "question-answering"
        ],
        "sample_tasks": [
          "tinyArc"
        ]
      },
      {
        "family_name": "tinyGSM8k",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "tinyGSM8k"
        ]
      },
      {
        "family_name": "tinyHellaswag",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "commonsense",
          "english"
        ],
        "sample_tasks": [
          "tinyHellaswag"
        ]
      },
      {
        "family_name": "tinyMMLU",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "english",
          "knowledge",
          "multiple-choice"
        ],
        "sample_tasks": [
          "tinyMMLU"
        ]
      },
      {
        "family_name": "tinyWinogrande",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "tinyWinogrande"
        ]
      },
      {
        "family_name": "toxigen",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "bias",
          "english",
          "general",
          "toxicity"
        ],
        "sample_tasks": [
          "toxigen"
        ]
      },
      {
        "family_name": "unfair",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "unfair_tos"
        ]
      },
      {
        "family_name": "vaxx",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "vaxx_stance"
        ]
      },
      {
        "family_name": "wiceu",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wiceu"
        ]
      },
      {
        "family_name": "wmt-ro-en-t5-prompt",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt-ro-en-t5-prompt"
        ]
      },
      {
        "family_name": "wmt14-en-fr",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt14-en-fr"
        ]
      },
      {
        "family_name": "wmt14-fr-en",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt14-fr-en"
        ]
      },
      {
        "family_name": "wmt16-de-en",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt16-de-en"
        ]
      },
      {
        "family_name": "wmt16-en-de",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt16-en-de"
        ]
      },
      {
        "family_name": "wmt16-en-ro",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt16-en-ro"
        ]
      },
      {
        "family_name": "wmt16-ro-en",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "wmt16-ro-en"
        ]
      },
      {
        "family_name": "wsc273",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general",
          "reasoning"
        ],
        "sample_tasks": [
          "wsc273"
        ]
      },
      {
        "family_name": "xlsum",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "xlsum_es"
        ]
      },
      {
        "family_name": "xsum",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "xsum"
        ]
      },
      {
        "family_name": "yahoo",
        "num_tasks": 1,
        "avg_quality_score": 2.0,
        "tags": [
          "english",
          "general"
        ],
        "sample_tasks": [
          "yahoo_answers_topics"
        ]
      },
      {
        "family_name": "instructhumaneval",
        "num_tasks": 1,
        "avg_quality_score": 4.0,
        "tags": [
          "code generation",
          "coding",
          "english",
          "instruction-following",
          "python"
        ],
        "sample_tasks": [
          "instructhumaneval"
        ]
      },
      {
        "family_name": "apps",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "code generation",
          "coding",
          "competitive programming",
          "english",
          "python"
        ],
        "sample_tasks": [
          "apps"
        ]
      },
      {
        "family_name": "ds1000",
        "num_tasks": 1,
        "avg_quality_score": 4.0,
        "tags": [
          "code generation",
          "coding",
          "data science",
          "english",
          "python"
        ],
        "sample_tasks": [
          "ds1000"
        ]
      },
      {
        "family_name": "humanevalpack",
        "num_tasks": 1,
        "avg_quality_score": 4.0,
        "tags": [
          "code generation",
          "coding",
          "cpp",
          "go",
          "java",
          "javascript",
          "multilingual",
          "python",
          "rust"
        ],
        "sample_tasks": [
          "humanevalpack"
        ]
      },
      {
        "family_name": "recode",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "code generation",
          "coding",
          "english",
          "python",
          "robustness"
        ],
        "sample_tasks": [
          "recode"
        ]
      },
      {
        "family_name": "conala",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "code generation",
          "coding",
          "english",
          "natural language to code",
          "python"
        ],
        "sample_tasks": [
          "conala"
        ]
      },
      {
        "family_name": "concode",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "code generation",
          "coding",
          "english",
          "java",
          "natural language to code"
        ],
        "sample_tasks": [
          "concode"
        ]
      },
      {
        "family_name": "mercury",
        "num_tasks": 1,
        "avg_quality_score": 3.0,
        "tags": [
          "code generation",
          "coding",
          "computational efficiency",
          "efficiency",
          "english",
          "python"
        ],
        "sample_tasks": [
          "mercury"
        ]
      }
    ]
  }
}