[
  "20_newsgroups",
  "simpleqa",
  "mmlu_redux",
  "frames",
  "healthbench",
  "swe_bench_verified",
  "scicode",
  "alpaca_eval",
  "arena_hard",
  "sorry_bench",
  "jailbreakbench",
  "harmbench",
  "or_bench",
  "bfcl",
  "halueval",
  "agentbench",
  "olympiadbench",
  "cluewsc",
  "chinese_simpleqa",
  "codeforces",
  "aider_polyglot",
  "agentharm",
  "donotanswer",
  "wildguard",
  "faithbench",
  "sycophancy_eval",
  "tau_bench",
  "toolemu",
  "toolbench",
  "toolllm",
  "travelplanner",
  "facts_grounding",
  "browsecomp",
  "refusalbench",
  "flames",
  "planbench",
  "Tag",
  "afrimgsm_direct_amh",
  "afrimmlu_direct_amh",
  "afrixnli_en_direct_amh",
  "ag_news",
  "aime",
  "aime2024",
  "aime2025",
  "apps",
  "humaneval_plus",
  "multipl_e",
  "arabic_exams",
  "argument_topic",
  "atis",
  "babilong",
  "bangla_mmlu",
  "banking77",
  "basque-glue",
  "bec2016eu",
  "bhtc_v2",
  "boolq",
  "truthfulqa_generation",
  "boolq-seq2seq",
  "cb",
  "chain_of_thought",
  "claim_stance_topic",
  "cnn_dailymail",
  "codexglue_code_to_text_go",
  "codexglue_code_to_text_java",
  "codexglue_code_to_text_javascript",
  "codexglue_code_to_text_php",
  "codexglue_code_to_text_python",
  "codexglue_code_to_text_ruby",
  "coedit_gec",
  "conala",
  "concode",
  "copa",
  "dbpedia_14",
  "doc_vqa",
  "ds1000",
  "epec_koref_bin",
  "ethos_binary",
  "evalita-mp",
  "evalita-sp_sum_task_fp-small_p1",
  "financial_tweets",
  "flan_held_in",
  "flores",
  "freebase",
  "glianorex",
  "global_mmlu_ar",
  "gpt3_translation_benchmarks",
  "gsm_plus",
  "hmmt",
  "hmmt_feb_2025",
  "humaneval_64_instruct",
  "humaneval_instruct",
  "humanevalpack",
  "instruct_humaneval",
  "instructhumaneval",
  "iwslt2017-ar-en",
  "iwslt2017-en-ar",
  "law_stack_exchange",
  "ledgar",
  "livecodebench",
  "livemathbench_cnmo_en",
  "livemathbench_cnmo_zh",
  "llama",
  "logieval",
  "m_mmlu",
  "math",
  "math500",
  "mbpp_plus",
  "medical_abstracts",
  "mela",
  "mercury",
  "multimedqa",
  "multiple_choice",
  "non_greedy_robustness_agieval_aqua_rat",
  "noticia",
  "openllm",
  "option_order_robustness_agieval_aqua_rat",
  "penn_treebank",
  "phrases_ca-va",
  "polymath_en_high",
  "polymath_en_medium",
  "polymath_zh_high",
  "polymath_zh_medium",
  "prompt_robustness_agieval_aqua_rat",
  "ptb",
  "pythia",
  "recode",
  "record",
  "self_consistency",
  "squad2",
  "stsb",
  "super-glue-lm-eval-v1",
  "super-glue-lm-eval-v1-seq2seq",
  "super-glue-t5-prompt",
  "t0_eval",
  "tmlu",
  "unfair_tos",
  "vaxx_stance",
  "wiceu",
  "wikitext103",
  "wmt-ro-en-t5-prompt",
  "wmt14_en_fr",
  "wmt14_fr_en",
  "wmt16_de_en",
  "wmt16_en_de",
  "wmt16_en_ro",
  "wmt16_ro_en",
  "xsum",
  "yahoo_answers_topics",
  "swe_verified",
  "swe_bench_verified",
  "multi_swe_bench",
  "swe_bench_multilingual",
  "livecodebench_v6",
  "livecodebench_v5",
  "livecodebench_lite",
  "hallucinations_leaderboard",
  "longform_writing",
  "longform",
  "seal_0",
  "seal",
  "finsearchcomp",
  "oj_bench",
  "terminal_bench",
  "cnmo_2024",
  "cnmo",
  "curate",
  "halulens",
  "politicalbias_qa",
  "polyglottoxicityprompts"
]