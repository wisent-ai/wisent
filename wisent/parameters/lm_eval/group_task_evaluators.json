{
  "aclue": {
    "parent_task": {
      "extractor_file": "aclue.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "aclue_homographic_character_resolution": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_ancient_phonetics": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_poetry_quality_assessment": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_sentence_segmentation": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_poetry_context_prediction": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_poetry_sentiment_analysis": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_poetry_appreciate": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_couplet_prediction": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_polysemy_resolution": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_ancient_literature": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_basic_ancient_chinese": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_named_entity_recognition": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_ancient_medical": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_reading_comprehension": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aclue_ancient_chinese_culture": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      }
    }
  },
  "acp": {
    "note": "acp does not exist as a separate task in lm-eval, only acpbench exists"
  },
  "acpbench": {
    "parent_task": {
      "extractor_file": "acp_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "acp_prog_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_reach_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_app_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_just_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_land_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_act_reach_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_val_bool": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_prog_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_reach_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_app_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_just_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_land_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_act_reach_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_val_mcq": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "exact_match",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_prog_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_reach_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_app_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_just_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_land_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_next_act_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_act_reach_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_val_gen": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_prog_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_reach_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_app_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_just_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_land_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_next_act_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_act_reach_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      },
      "acp_val_gen_pddl": {
        "lm_eval_output_type": "generate_until",
        "lm_eval_metric": "score",
        "wisent_evaluator": "log_likelihoods",
        "match": false,
        "note": "MISMATCH"
      }
    }
  },
  "aexams": {
    "parent_task": {
      "extractor_file": "aexams.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "aexams_Biology": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aexams_Physics": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aexams_IslamicStudies": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aexams_Science": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      },
      "aexams_Social": {
        "lm_eval_output_type": "multiple_choice",
        "lm_eval_metric": "acc",
        "wisent_evaluator": "log_likelihoods",
        "match": true
      }
    }
  },
  "afrimgsm": {
    "parent_task": {
      "extractor_file": "afrimgsm.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "369 subtasks total - all use generate_until with exact_match metric",
      "all_subtasks_match": true,
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "generation",
      "match": true
    }
  },
  "afrimmlu": {
    "parent_task": {
      "extractor_file": "afrimmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "177 subtasks total - all use multiple_choice with acc metric",
      "all_subtasks_match": true,
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "afrixnli": {
    "parent_task": {
      "extractor_file": "afrixnli.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "265 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "afrobench": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "1818 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "afrobench_adr": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_afriqa": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_afrisenti": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_belebele": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_flores": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_injongointent": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_mafand": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_masakhaner": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_masakhanews": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_masakhapos": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_naijarc": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_nollysenti": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_ntrex": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_openai_mmlu": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_salt": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_sib": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_uhura-arc-easy": {
    "note": "Directory not found in lm-eval"
  },
  "afrobench_xlsum": {
    "note": "Directory not found in lm-eval"
  },
  "agieval": {
    "parent_task": {
      "extractor_file": "agieval.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "25 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "anli": {
    "parent_task": {
      "extractor_file": "anli.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "arab_culture": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "13 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "arab_culture_completion": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "13 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "arabic_leaderboard_complete": {
    "parent_task": {
      "extractor_file": "arabic_leaderboard_complete.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "152 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "arabic_leaderboard_light": {
    "parent_task": {
      "extractor_file": "arabic_leaderboard_light.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "152 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "arabicmmlu": {
    "parent_task": {
      "extractor_file": "arabicmmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "40 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "aradice": {
    "parent_task": {
      "extractor_file": "aradice.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "109 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "arc": {
    "parent_task": {
      "extractor_file": "arc.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "arc_mt": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "12 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "arithmetic": {
    "parent_task": {
      "extractor_file": "arithmetic.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "10 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "basque_bench": {
    "parent_task": {
      "extractor_file": "basque_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "26 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "bleu",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "bbh": {
    "parent_task": {
      "extractor_file": "bbh.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "108 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "exact_match",
      "match": true
    }
  },
  "bbq": {
    "parent_task": {
      "extractor_file": "bbq.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "6 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "belebele": {
    "parent_task": {
      "extractor_file": "belebele.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "122 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "bertaqa": {
    "parent_task": {
      "extractor_file": "bertaqa.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "16 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "careqa": {
    "parent_task": {
      "extractor_file": "careqa.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "4 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "catalan_bench": {
    "parent_task": {
      "extractor_file": "catalan_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "41 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "bleu",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "ceval_valid": {
    "note": "Directory not found in lm-eval"
  },
  "cmmlu": {
    "parent_task": {
      "extractor_file": "cmmlu.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "134 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "code_x_glue": {
    "parent_task": {
      "extractor_file": "code_x_glue.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "6 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "copal_id": {
    "parent_task": {
      "extractor_file": "copal_id.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "crows_pairs": {
    "parent_task": {
      "extractor_file": "crows_pairs.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "22 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "csatqa": {
    "parent_task": {
      "extractor_file": "csatqa.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "6 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "darija": {
    "note": "Directory not found in lm-eval"
  },
  "darija_sentiment": {
    "note": "Directory not found in lm-eval"
  },
  "darija_summarization": {
    "note": "Directory not found in lm-eval"
  },
  "darija_translation": {
    "note": "Directory not found in lm-eval"
  },
  "darija_transliteration": {
    "note": "Directory not found in lm-eval"
  },
  "darijammlu": {
    "parent_task": {
      "extractor_file": "darijammlu.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "44 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "egymmlu": {
    "parent_task": {
      "extractor_file": "egymmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "44 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "eus_exams": {
    "parent_task": {
      "extractor_file": "eus_exams.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "62 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "evalita_mp": {
    "note": "Directory not found in lm-eval"
  },
  "fld": {
    "parent_task": {
      "extractor_file": "fld.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "4 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "freebase": {
    "note": "Directory not found in lm-eval"
  },
  "french_bench": {
    "parent_task": {
      "extractor_file": "french_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "18 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "galician_bench": {
    "parent_task": {
      "extractor_file": "galician_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "30 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "bleu",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "glianorex": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "global_mmlu": {
    "parent_task": {
      "extractor_file": "global_mmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "2484 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "gpqa": {
    "parent_task": {
      "extractor_file": "gpqa.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "15 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "gsm8k": {
    "parent_task": {
      "extractor_file": "gsm8k.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "gsm8k_platinum": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "haerae": {
    "parent_task": {
      "extractor_file": "haerae.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "headqa": {
    "parent_task": {
      "extractor_file": "headqa.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "hellaswag": {
    "parent_task": {
      "extractor_file": "hellaswag.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "1 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "hendrycks_ethics": {
    "parent_task": {
      "extractor_file": "hendrycks_ethics.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "hendrycks_math": {
    "parent_task": {
      "extractor_file": "hendrycks_math.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "8 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "hrm8k": {
    "parent_task": {
      "extractor_file": "hrm8k.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "12 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "exact_match",
      "match": true
    }
  },
  "inverse": {
    "note": "Directory not found in lm-eval"
  },
  "inverse_scaling": {
    "parent_task": {
      "extractor_file": "inverse_scaling.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "11 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "japanese_leaderboard": {
    "parent_task": {
      "extractor_file": "japanese_leaderboard.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "8 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "jsonschema_bench": {
    "parent_task": {
      "extractor_file": "jsonschema_bench.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "kbl": {
    "parent_task": {
      "extractor_file": "kbl.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "65 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "kmmlu": {
    "parent_task": {
      "extractor_file": "kmmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "225 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "kobest": {
    "parent_task": {
      "extractor_file": "kobest.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "kormedmcqa": {
    "parent_task": {
      "extractor_file": "kormedmcqa.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "4 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "generation",
      "match": true
    }
  },
  "lambada": {
    "parent_task": {
      "extractor_file": "lambada.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "lambada_cloze": {
    "parent_task": {
      "extractor_file": "lambada_cloze.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "lambada_multilingual": {
    "parent_task": {
      "extractor_file": "lambada_multilingual.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "lambada_multilingual_stablelm": {
    "parent_task": {
      "extractor_file": "lambada_multilingual_stablelm.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "7 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "leaderboard": {
    "parent_task": {
      "extractor_file": "leaderboard.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "40 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc_norm",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "libra": {
    "parent_task": {
      "extractor_file": "libra.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "18 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": true
    }
  },
  "lingoly": {
    "parent_task": {
      "extractor_file": "lingoly.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "longbench": {
    "parent_task": {
      "extractor_file": "longbench.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "34 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "m_mmlu": {
    "note": "Directory not found in lm-eval"
  },
  "mastermind": {
    "parent_task": {
      "extractor_file": "mastermind.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "6 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "med_concepts_qa": {
    "parent_task": {
      "extractor_file": "med_concepts_qa.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "15 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "meddialog": {
    "parent_task": {
      "extractor_file": "meddialog.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "4 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "mela": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "10 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "mcc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "metabench": {
    "parent_task": {
      "extractor_file": "metabench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "26 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "mgsm": {
    "parent_task": {
      "extractor_file": "mgsm.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "33 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "minerva_math": {
    "parent_task": {
      "extractor_file": "minerva_math.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "7 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "mlqa": {
    "parent_task": {
      "extractor_file": "mlqa.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "49 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "mmlu": {
    "parent_task": {
      "extractor_file": "mmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "399 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "mmlu_pro": {
    "parent_task": {
      "extractor_file": "mmlu_pro.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "14 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": "generation",
      "match": true
    }
  },
  "mmlu_pro_plus": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "14 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "mmlu_prox": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "182 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "exact_match",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "mmlusr": {
    "parent_task": {
      "extractor_file": "mmlusr.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "171 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "mmmu": {
    "parent_task": {
      "extractor_file": "mmmu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "30 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "model_written_evals": {
    "parent_task": {
      "extractor_file": "model_written_evals.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "187 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "multiblimp": {
    "parent_task": {
      "extractor_file": "multiblimp.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "101 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "non": {
    "note": "Directory not found in lm-eval"
  },
  "noreval": {
    "parent_task": {
      "extractor_file": "noreval.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "116 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "em",
      "wisent_evaluator": "generation",
      "match": true
    }
  },
  "okapi_arc_multilingual": {
    "note": "Directory not found in lm-eval"
  },
  "okapi_hellaswag_multilingual": {
    "note": "Directory not found in lm-eval"
  },
  "okapi_mmlu_multilingual": {
    "note": "Directory not found in lm-eval"
  },
  "okapi_truthfulqa_multilingual": {
    "note": "Directory not found in lm-eval"
  },
  "noridiom": {
    "note": "Directory not found in lm-eval"
  },
  "nortruthfulqa": {
    "note": "Directory not found in lm-eval"
  },
  "nrk": {
    "note": "Directory not found in lm-eval"
  },
  "paloma": {
    "parent_task": {
      "extractor_file": "paloma.py",
      "evaluator_name": "perplexity"
    },
    "subtasks": {
      "note": "16 subtasks total",
      "lm_eval_output_type": "loglikelihood_rolling",
      "lm_eval_metric": "word_perplexity",
      "wisent_evaluator": "perplexity",
      "match": false
    }
  },
  "pawsx": {
    "note": "Directory not found in lm-eval"
  },
  "persona": {
    "note": "Directory not found in lm-eval"
  },
  "pile": {
    "parent_task": {
      "extractor_file": "pile.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "22 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "polemo2": {
    "parent_task": {
      "extractor_file": "polemo2.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "portuguese_bench": {
    "parent_task": {
      "extractor_file": "portuguese_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "20 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "bleu",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "prompt": {
    "note": "Directory not found in lm-eval"
  },
  "qa4mre": {
    "parent_task": {
      "extractor_file": "qa4mre.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "qasper": {
    "parent_task": {
      "extractor_file": "qasper.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "ru": {
    "note": "Directory not found in lm-eval"
  },
  "ruler": {
    "parent_task": {
      "extractor_file": "ruler.py",
      "evaluator_name": null
    },
    "subtasks": {
      "note": "14 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "score": {
    "parent_task": {
      "extractor_file": "score.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "46 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "scrolls": {
    "parent_task": {
      "extractor_file": "scrolls.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "7 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "spanish_bench": {
    "parent_task": {
      "extractor_file": "spanish_bench.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "29 subtasks total",
      "lm_eval_output_type": "generate_until",
      "lm_eval_metric": "bleu",
      "wisent_evaluator": "log_likelihoods",
      "match": false
    }
  },
  "storycloze": {
    "parent_task": {
      "extractor_file": "storycloze.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "2 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "tinyBenchmarks": {
    "parent_task": {
      "extractor_file": "tinyBenchmarks.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "8 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "tmlu": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "31 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": null,
      "match": null
    }
  },
  "tmmluplus": {
    "parent_task": {
      "extractor_file": "tmmluplus.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "67 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "translation": {
    "parent_task": {
      "extractor_file": "translation.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "8 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "truthfulqa": {
    "parent_task": {
      "extractor_file": "truthfulqa.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "truthfulqa-multi": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "15 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "truthfulqa_multi": {
    "parent_task": {
      "extractor_file": "truthfulqa_multi.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "15 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "turkishmmlu": {
    "parent_task": {
      "extractor_file": "turkishmmlu.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "18 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "unscramble": {
    "parent_task": {
      "extractor_file": "unscramble.py",
      "evaluator_name": "exact_match"
    },
    "subtasks": {
      "note": "5 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "exact_match",
      "match": null
    }
  },
  "winogender": {
    "parent_task": {
      "extractor_file": "winogender.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "7 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "wmdp": {
    "parent_task": {
      "extractor_file": "wmdp.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": "multiple_choice",
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": true
    }
  },
  "xnli_eu": {
    "parent_task": {
      "extractor_file": null,
      "evaluator_name": null
    },
    "subtasks": {
      "note": "3 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": null,
      "match": null
    }
  },
  "xcopa": {
    "parent_task": {
      "extractor_file": "xcopa.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "11 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "xnli": {
    "parent_task": {
      "extractor_file": "xnli.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "15 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "xquad": {
    "parent_task": {
      "extractor_file": "xquad.py",
      "evaluator_name": "generation"
    },
    "subtasks": {
      "note": "12 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": null,
      "wisent_evaluator": "generation",
      "match": null
    }
  },
  "xstorycloze": {
    "parent_task": {
      "extractor_file": "xstorycloze.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "11 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  },
  "xwinograd": {
    "parent_task": {
      "extractor_file": "xwinograd.py",
      "evaluator_name": "log_likelihoods"
    },
    "subtasks": {
      "note": "6 subtasks total",
      "lm_eval_output_type": null,
      "lm_eval_metric": "acc",
      "wisent_evaluator": "log_likelihoods",
      "match": null
    }
  }
}