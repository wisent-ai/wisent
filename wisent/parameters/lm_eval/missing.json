{
  "from_deepseek_comparison_table": {
    "english": [
      {
        "name": "mmlu_redux",
        "metric": "EM",
        "description": "MMLU-Redux - cleaned version of MMLU with error corrections",
        "source": "https://github.com/EleutherAI/lm-evaluation-harness"
      },
      {
        "name": "simpleqa",
        "metric": "Correct",
        "description": "SimpleQA - factual knowledge benchmark by OpenAI",
        "source": "https://openai.com/index/introducing-simpleqa/"
      },
      {
        "name": "frames",
        "metric": "Accuracy",
        "description": "FRAMES - factual reasoning and multi-step reasoning",
        "source": "https://huggingface.co/datasets/google/frames-benchmark"
      },
      {
        "name": "alpaca_eval",
        "metric": "LC-winrate",
        "description": "AlpacaEval 2.0 - instruction following evaluation",
        "source": "https://github.com/tatsu-lab/alpaca_eval"
      },
      {
        "name": "arena_hard",
        "metric": "GPT-4-1106 winrate",
        "description": "ArenaHard - challenging conversational benchmark",
        "source": "https://github.com/lm-sys/arena-hard-auto"
      }
    ],
    "code": [
      {
        "name": "codeforces",
        "metric": "Percentile/Rating",
        "description": "Codeforces competitive programming benchmark",
        "source": "https://codeforces.com"
      },
      {
        "name": "swe_verified",
        "metric": "Resolved",
        "description": "SWE-Bench Verified - real-world software engineering tasks",
        "source": "https://www.swebench.com/"
      },
      {
        "name": "aider_polyglot",
        "metric": "Accuracy",
        "description": "Aider Polyglot - multi-language code editing benchmark",
        "source": "https://aider.chat/docs/leaderboards/"
      }
    ],
    "math": [
      {
        "name": "cnmo_2024",
        "metric": "Pass@1",
        "description": "Chinese National Math Olympiad 2024",
        "source": "competition problems"
      }
    ],
    "chinese": [
      {
        "name": "cluewsc",
        "metric": "EM",
        "description": "CLUEWSC - Chinese Winograd Schema Challenge",
        "source": "https://github.com/CLUEbenchmark/CLUE"
      },
      {
        "name": "c_simpleqa",
        "metric": "Correct",
        "description": "C-SimpleQA - Chinese factual knowledge benchmark",
        "source": "OpenAI"
      }
    ]
  },
  "safety_and_alignment": {
    "refusal_and_jailbreak": [
      {
        "name": "sorry_bench",
        "metric": "Refusal Rate / ASR",
        "description": "SORRY-Bench - 44-class taxonomy of unsafe requests, refusal evaluation (ICLR 2025)",
        "source": "https://sorry-bench.github.io/",
        "priority": "critical"
      },
      {
        "name": "jailbreakbench",
        "metric": "ASR (Attack Success Rate)",
        "description": "JailbreakBench - 100 misuse behaviors + 100 benign for overrefusal testing",
        "source": "https://jailbreakbench.github.io/",
        "priority": "critical"
      },
      {
        "name": "agentharm",
        "metric": "Compliance Rate",
        "description": "AgentHarm - 440 tasks across 11 harm categories for LLM agents (ICLR 2025)",
        "source": "https://arxiv.org/abs/2410.09024",
        "priority": "critical"
      },
      {
        "name": "or_bench",
        "metric": "Over-refusal Rate",
        "description": "OR-Bench - 80,000 over-refusal prompts testing excessive caution",
        "source": "https://arxiv.org/abs/2405.20947",
        "priority": "high"
      },
      {
        "name": "refusalbench",
        "metric": "Refusal Detection F1 / Category Accuracy",
        "description": "RefusalBench - selective refusal based on informational uncertainty",
        "source": "https://arxiv.org/abs/2510.10390",
        "priority": "high"
      },
      {
        "name": "donotanswer",
        "metric": "Refusal Accuracy",
        "description": "DoNotAnswer - 900+ prompts across 12 harm types",
        "source": "https://github.com/Libr-AI/do-not-answer",
        "priority": "high"
      },
      {
        "name": "harmbench",
        "metric": "ASR",
        "description": "HarmBench - jailbreak vulnerability across violence, fraud, discrimination, etc.",
        "source": "https://www.harmbench.org/",
        "priority": "critical"
      },
      {
        "name": "wildguard",
        "metric": "F1",
        "description": "WildGuard - safety classification and refusal detection",
        "source": "https://huggingface.co/allenai/wildguard",
        "priority": "medium"
      }
    ],
    "hallucination_and_factuality": [
      {
        "name": "facts_grounding",
        "metric": "Grounding Score",
        "description": "FACTS Grounding - long-form factual grounding in context (Google DeepMind)",
        "source": "https://arxiv.org/abs/2501.03200",
        "priority": "critical"
      },
      {
        "name": "faithbench",
        "metric": "Faithfulness Score",
        "description": "FaithBench - faithfulness hallucinations in summarization",
        "source": "https://arxiv.org/abs/2410.13210",
        "priority": "high"
      },
      {
        "name": "hallucinations_leaderboard",
        "metric": "SelfCheckGPT + HaluEval",
        "description": "Hugging Face Hallucinations Leaderboard tasks",
        "source": "https://huggingface.co/blog/leaderboard-hallucinations",
        "priority": "high"
      },
      {
        "name": "halulens",
        "metric": "Intrinsic/Extrinsic Detection",
        "description": "HalluLens - intrinsic vs extrinsic hallucination detection",
        "source": "https://aclanthology.org/2025.acl-long.1176/",
        "priority": "medium"
      },
      {
        "name": "halueval",
        "metric": "Detection Accuracy",
        "description": "HaluEval - hallucination detection in QA, Dialog, Summarization",
        "source": "https://github.com/RUCAIBox/HaluEval",
        "priority": "high"
      }
    ],
    "sycophancy_and_bias": [
      {
        "name": "syceval",
        "metric": "Sycophancy Rate",
        "description": "SycEval - comprehensive sycophancy evaluation",
        "source": "https://arxiv.org/abs/2502.08177",
        "priority": "high"
      },
      {
        "name": "curate",
        "metric": "Alignment Score",
        "description": "CURATe - personalized alignment, safety vs user desires",
        "source": "https://arxiv.org/abs/2410.21159",
        "priority": "medium"
      },
      {
        "name": "politicalbias_qa",
        "metric": "Bias Score",
        "description": "Political bias and steerability evaluation",
        "source": "https://arxiv.org/abs/2407.18008",
        "priority": "medium"
      }
    ],
    "agent_and_tool_use": [
      {
        "name": "bfcl",
        "metric": "AST Accuracy",
        "description": "Berkeley Function Calling Leaderboard V4 - function calling accuracy",
        "source": "https://gorilla.cs.berkeley.edu/leaderboard.html",
        "priority": "critical"
      },
      {
        "name": "agentbench",
        "metric": "Success Rate",
        "description": "AgentBench - LLM-as-Agent across 8 environments (ICLR 2024)",
        "source": "https://github.com/THUDM/AgentBench",
        "priority": "critical"
      },
      {
        "name": "tau_bench",
        "metric": "Task Completion",
        "description": "TAU-bench - Tool-Agent-User interaction in retail/airline domains",
        "source": "https://github.com/sierra-research/tau-bench",
        "priority": "high"
      },
      {
        "name": "toolemu",
        "metric": "Risk Detection",
        "description": "ToolEmu - risky behaviors when using tools (36 high-stakes tools)",
        "source": "https://github.com/ryoungj/ToolEmu",
        "priority": "high"
      },
      {
        "name": "toolllm",
        "metric": "Pass Rate",
        "description": "ToolLLM - API and tool usage with real-world scenarios",
        "source": "https://github.com/OpenBMB/ToolBench",
        "priority": "medium"
      },
      {
        "name": "nexus_function_calling",
        "metric": "Accuracy",
        "description": "Nexus Function Calling Leaderboard - 9 tasks, 762 cases",
        "source": "https://huggingface.co/spaces/Nexusflow/Nexus_Function_Calling_Leaderboard",
        "priority": "medium"
      }
    ],
    "multilingual_safety": [
      {
        "name": "polyglottoxicityprompts",
        "metric": "Toxicity Score",
        "description": "PolygloToxicityPrompts - multilingual toxic degeneration",
        "source": "https://arxiv.org/abs/2410.xxxxx",
        "priority": "medium"
      },
      {
        "name": "flames",
        "metric": "Value Alignment",
        "description": "FLAMES - Chinese value alignment benchmark",
        "source": "https://github.com/AIFlames/flames",
        "priority": "medium"
      }
    ]
  },
  "from_k2_gpt5_comparison_table": {
    "reasoning": [
      {
        "name": "imo_answerbench",
        "metric": "Pass@1",
        "description": "IMO-AnswerBench - International Math Olympiad problems",
        "source": "competition problems",
        "priority": "critical"
      }
    ],
    "general": [
      {
        "name": "longform_writing",
        "metric": "Quality Score",
        "description": "Longform Writing - extended text generation quality",
        "source": "various",
        "priority": "high"
      },
      {
        "name": "healthbench",
        "metric": "Accuracy",
        "description": "HealthBench - medical/health domain evaluation",
        "source": "OpenAI",
        "priority": "critical"
      }
    ],
    "agentic_search": [
      {
        "name": "browsecomp",
        "metric": "Accuracy",
        "description": "BrowseComp - web browsing/search agent benchmark",
        "source": "OpenAI",
        "priority": "critical"
      },
      {
        "name": "browsecomp_zh",
        "metric": "Accuracy",
        "description": "BrowseComp-ZH - Chinese web browsing benchmark",
        "source": "OpenAI",
        "priority": "high"
      },
      {
        "name": "seal_0",
        "metric": "Accuracy",
        "description": "Seal-0 - agentic search evaluation",
        "source": "various",
        "priority": "high"
      },
      {
        "name": "finsearchcomp",
        "metric": "Accuracy",
        "description": "FinSearchComp - financial search agent benchmark",
        "source": "various",
        "priority": "high"
      }
    ],
    "coding": [
      {
        "name": "swe_bench_multilingual",
        "metric": "Resolved",
        "description": "SWE-bench Multilingual - software engineering across languages",
        "source": "https://www.swebench.com/",
        "priority": "critical"
      },
      {
        "name": "multi_swe_bench",
        "metric": "Resolved",
        "description": "Multi-SWE-bench - multi-file software engineering tasks",
        "source": "https://www.swebench.com/",
        "priority": "critical"
      },
      {
        "name": "scicode",
        "metric": "Pass@1",
        "description": "SciCode - scientific computing code generation",
        "source": "https://scicode-bench.github.io/",
        "priority": "high"
      },
      {
        "name": "livecodebench_v6",
        "metric": "Pass@1",
        "description": "LiveCodeBenchV6 - latest version of live coding benchmark",
        "source": "https://livecodebench.github.io/",
        "priority": "critical"
      },
      {
        "name": "oj_bench",
        "metric": "Pass@1",
        "description": "OJ-Bench - online judge style competitive programming (cpp)",
        "source": "various",
        "priority": "high"
      },
      {
        "name": "terminal_bench",
        "metric": "Accuracy",
        "description": "Terminal-Bench - terminal/CLI interaction benchmark",
        "source": "various",
        "priority": "high"
      }
    ]
  },
  "reasoning_and_planning": [
    {
      "name": "planningbench",
      "metric": "Plan Success Rate",
      "description": "Planning benchmark for multi-step reasoning",
      "source": "various",
      "priority": "medium"
    },
    {
      "name": "travelplanner",
      "metric": "Constraint Satisfaction",
      "description": "TravelPlanner - complex constraint-based planning",
      "source": "https://github.com/OSU-NLP-Group/TravelPlanner",
      "priority": "medium"
    }
  ],
  "notes": {
    "already_have": [
      "mmlu (and variants)",
      "mmlu_pro",
      "drop",
      "ifeval",
      "gpqa",
      "livecodebench (but need V6)",
      "aime2024",
      "aime2025",
      "hmmt_feb_2025",
      "math500 (via math)",
      "ceval",
      "cmmlu",
      "hle",
      "frames (need to verify)",
      "truthfulqa",
      "bbq",
      "crows_pairs",
      "wmdp",
      "toxigen",
      "realtoxicityprompts",
      "sycophancy (basic)"
    ],
    "priority_explanation": {
      "critical": "Must have - widely used in major model comparisons",
      "high": "Should have - important for comprehensive evaluation",
      "medium": "Nice to have - specialized or emerging benchmarks"
    }
  }
}
