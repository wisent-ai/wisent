[
  "AraDiCE",
  "ArabCulture",
  "Tag",
  "aclue",
  "acp_bench",
  "aexams",
  "afrimgsm_direct_amh",
  "afrimmlu_direct_amh",
  "agentbench",
  "aider_polyglot",
  "aime",
  "anli",
  "apps",
  "arabic_leaderboard_complete",
  "arabic_leaderboard_light",
  "arabicmmlu",
  "arena_hard",
  "arithmetic",
  "asdiv",
  "babi",
  "babilong",
  "bangla_mmlu",
  "basqueglue",
  "bbh",
  "bbq",
  "belebele",
  "bertaqa",
  "blimp",
  "browsecomp",
  "c4",
  "careqa",
  "catalan_bench",
  "ceval",
  "chartqa",
  "chinese_simpleqa",
  "cmmlu",
  "cnmo",
  "cnmo_2024",
  "commonsense_qa",
  "conala",
  "concode",
  "copal_id",
  "coqa",
  "crows_pairs",
  "csatqa",
  "curate",
  "darija_bench",
  "darijahellaswag",
  "darijammlu",
  "donotanswer",
  "drop",
  "ds1000",
  "eq_bench",
  "eus_exams",
  "eus_proficiency",
  "eus_reading",
  "eus_trivia",
  "evalita_LLM",
  "facts_grounding",
  "faithbench",
  "fda",
  "finsearchcomp",
  "flames",
  "fld",
  "frames",
  "french_bench",
  "galician_bench",
  "global_mmlu",
  "gpqa",
  "groundcocoa",
  "gsm8k",
  "haerae",
  "hallucinations_leaderboard",
  "halueval",
  "halulens",
  "headqa",
  "healthbench",
  "hellaswag",
  "hendrycks_ethics",
  "hendrycks_math",
  "histoires_morales",
  "hmmt",
  "hmmt_feb_2025",
  "hrm8k",
  "humaneval",
  "humaneval_plus",
  "humanevalpack",
  "inverse_scaling",
  "kbl",
  "kobest",
  "kormedmcqa",
  "lambada",
  "lambada_cloze",
  "lambada_multilingual",
  "lambada_multilingual_stablelm",
  "lingoly",
  "livecodebench",
  "livecodebench_lite",
  "livecodebench_v5",
  "livecodebench_v6",
  "livemathbench_cnmo_en",
  "logiqa",
  "logiqa2",
  "longform",
  "longform_writing",
  "mastermind",
  "math",
  "math500",
  "mathqa",
  "mc_taco",
  "med_concepts_qa",
  "meddialog",
  "medmcqa",
  "medqa",
  "mercury",
  "metabench",
  "mgsm",
  "mlqa",
  "mmlu",
  "mmlu-pro-plus",
  "mmlu_pro",
  "mmlu_prox",
  "mmlusr",
  "mmmu",
  "model_written_evals",
  "moral_stories",
  "multipl_e",
  "multiple_cpp",
  "multiple_go",
  "multiple_java",
  "multiple_js",
  "multiple_py",
  "multiple_rs",
  "mutual",
  "nq_open",
  "oj_bench",
  "okapi/hellaswag_multilingual",
  "okapi/mmlu_multilingual",
  "okapi/truthfulqa_multilingual",
  "openbookqa",
  "paloma",
  "paws-x",
  "piqa",
  "planbench",
  "polemo2",
  "politicalbias_qa",
  "polyglottoxicityprompts",
  "polymath_en_high",
  "polymath_en_medium",
  "polymath_zh_high",
  "polymath_zh_medium",
  "prost",
  "pubmedqa",
  "qa4mre",
  "qasper",
  "race",
  "realtoxicityprompts",
  "recode",
  "refusalbench",
  "scicode",
  "sciq",
  "score",
  "seal",
  "seal_0",
  "simple_cooccurrence_bias",
  "simpleqa",
  "siqa",
  "spanish_bench",
  "squad_completion",
  "squadv2",
  "storycloze",
  "swag",
  "swde",
  "swe_bench_verified",
  "swe_verified",
  "tau_bench",
  "terminal_bench",
  "tmmluplus",
  "toolbench",
  "toolemu",
  "toolllm",
  "toxigen",
  "translation",
  "travelplanner",
  "triviaqa",
  "truthfulqa",
  "truthfulqa_generation",
  "webqs",
  "wikitext",
  "winogender",
  "winogrande",
  "wmdp",
  "wmt14_en_fr",
  "wmt14_fr_en",
  "wmt16_de_en",
  "wmt16_en_de",
  "wmt2016",
  "wsc273",
  "xcopa",
  "xnli",
  "xnli_eu",
  "xquad",
  "xstorycloze",
  "xwinograd"
]