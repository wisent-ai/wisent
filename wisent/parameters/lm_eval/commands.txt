lm_eval --tasks list | grep <benchmark name>

python -m wisent.tests.test_table.inspect_benchmark <benchmark name>

python -m wisent.examples.scripts.test_one_benchmark <benchmark name>

python -m wisent.tests.test_table.inspect_hf_dataset codeparrot/apps

lm_eval --model hf --model_args pretrained=EleutherAI/gpt-neo-125M --tasks <benchmark name> --device cuda:0 --batch_size 4 --limit 20

python -m wisent.core.main generate-vector-from-task \
--task 20_newsgroups \
--trait-label correctness \
--output ./data/steering_vectors/hellaswag_correctness_vector.json \
--model meta-llama/Llama-3.2-1B-Instruct \
--num-pairs 50 \
--layers 8 \
--keep-intermediate \
--token-aggregation average \
--prompt-strategy chat_template \
--method caa \
--normalize \
--verbose \
--timing \
--device cuda

lm_eval --model hf --model_args pretrained=EleutherAI/gpt-neo-125M --tasks code2text_python --device cuda:0 --batch_size 4 --limit 20





