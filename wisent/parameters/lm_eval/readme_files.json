[
  "aclue",
  "acpbench",
  "aexams",
  "afrimgsm",
  "afrimmlu",
  "afrixnli",
  "afrobench",
  "afrobench_adr",
  "afrobench_afriqa",
  "afrobench_afrisenti",
  "afrobench_belebele",
  "afrobench_flores",
  "afrobench_injongointent",
  "afrobench_mafand",
  "afrobench_masakhaner",
  "afrobench_masakhanews",
  "afrobench_masakhapos",
  "afrobench_naijarc",
  "afrobench_nollysenti",
  "afrobench_ntrex",
  "afrobench_openai_mmlu",
  "afrobench_salt",
  "afrobench_sib",
  "afrobench_uhura-arc-easy",
  "afrobench_xlsum",
  "agieval",
  "alghafa_copa_ar",
  "alghafa_piqa_ar",
  "anli",
  "arab_culture",
  "arab_culture_completion",
  "arabic_leaderboard_complete",
  "arabic_leaderboard_light",
  "arabicmmlu",
  "aradice",
  "arc",
  "arc_mt",
  "arithmetic",
  "asdiv",
  "babi",
  "basque_bench",
  "basqueglue",
  "bbh",
  "bbq",
  "belebele",
  "benchmarks",
  "benchmarks_multimedqa",
  "bertaqa",
  "bigbench",
  "blimp",
  "c4",
  "careqa",
  "catalan_bench",
  "ceval",
  "chartqa",
  "cmmlu",
  "code_x_glue",
  "commonsense_qa",
  "copal_id",
  "coqa",
  "crows_pairs",
  "csatqa",
  "darija_bench",
  "darija_bench_darija_sentiment",
  "darija_bench_darija_summarization",
  "darija_bench_darija_translation",
  "darija_bench_darija_transliteration",
  "darijahellaswag",
  "darijammlu",
  "drop",
  "egyhellaswag",
  "egymmlu",
  "eq_bench",
  "eus_exams",
  "eus_proficiency",
  "eus_reading",
  "eus_trivia",
  "evalita_llm",
  "fda",
  "fld",
  "french_bench",
  "galician_bench",
  "glianorex",
  "global_mmlu",
  "glue",
  "gpqa",
  "groundcocoa",
  "gsm8k",
  "gsm8k_platinum",
  "gsm_plus",
  "haerae",
  "headqa",
  "hellaswag",
  "hendrycks_ethics",
  "hendrycks_math",
  "histoires_morales",
  "hrm8k",
  "humaneval",
  "ifeval",
  "include",
  "inverse_scaling",
  "japanese_leaderboard",
  "jsonschema_bench",
  "kbl",
  "kmmlu",
  "kobest",
  "kormedmcqa",
  "lambada",
  "lambada_cloze",
  "lambada_multilingual",
  "lambada_multilingual_stablelm",
  "leaderboard",
  "libra",
  "lingoly",
  "llama3",
  "logiqa",
  "logiqa2",
  "longbench",
  "mastermind",
  "mathqa",
  "mbpp",
  "mc_taco",
  "med_concepts_qa",
  "meddialog",
  "mediqa_qa2019",
  "medmcqa",
  "medqa",
  "medtext",
  "mela",
  "meqsum",
  "metabench",
  "mgsm",
  "mimic_repsum",
  "minerva_math",
  "mlqa",
  "mmlu",
  "mmlu-pro-plus",
  "mmlu_pro",
  "mmlu_prox",
  "mmlusr",
  "mmmu",
  "model_written_evals",
  "moral_stories",
  "mts_dialog",
  "multiblimp",
  "mutual",
  "noreval",
  "noreval_ask_gec",
  "noticia",
  "nq_open",
  "okapi_arc_multilingual",
  "okapi_hellaswag_multilingual",
  "okapi_mmlu_multilingual",
  "okapi_truthfulqa_multilingual",
  "olaph",
  "openbookqa",
  "paloma",
  "paws-x",
  "pile",
  "pile_10k",
  "piqa",
  "polemo2",
  "portuguese_bench",
  "prost",
  "pubmedqa",
  "qa4mre",
  "qasper",
  "race",
  "realtoxicityprompts",
  "ruler",
  "sciq",
  "score",
  "scrolls",
  "simple_cooccurrence_bias",
  "siqa",
  "spanish_bench",
  "squad_completion",
  "squadv2",
  "storycloze",
  "super_glue",
  "swag",
  "swde",
  "tinyBenchmarks",
  "tmlu",
  "tmmluplus",
  "toxigen",
  "translation",
  "triviaqa",
  "truthfulqa",
  "truthfulqa-multi",
  "turkishmmlu",
  "unitxt",
  "unscramble",
  "webqs",
  "wikitext",
  "winogender",
  "winogrande",
  "wmdp",
  "wmt2016",
  "wsc273",
  "xcopa",
  "xnli",
  "xnli_eu",
  "xquad",
  "xstorycloze",
  "xwinograd"
]
