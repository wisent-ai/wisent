[
  {
    "task_name": "aclue",
    "extractor_file": "aclue.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 15 subtasks, all using multiple_choice output type"
  },
  {
    "task_name": "acp_bench",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Tag evaluating both bool and mcq variants, both use generate_until with exact_match"
  },
  {
    "task_name": "acp_bench_hard",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "score"
    ],
    "expected_evaluator": "generation",
    "notes": "Tag evaluating acp_gen_2shot, uses generate_until without exact_match"
  },
  {
    "task_name": "acp_bench_hard_with_pddl",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "score"
    ],
    "expected_evaluator": "generation",
    "notes": "Tag evaluating acp_gen_2shot_with_pddl, uses generate_until without exact_match"
  },
  {
    "task_name": "acp_prog_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_reach_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_app_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_just_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_land_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_areach_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_val_bool",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Bool variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_prog_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_reach_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_app_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_just_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_land_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_areach_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_val_mcq",
    "extractor_file": "acp_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MCQ variant using generate_until with exact_match"
  },
  {
    "task_name": "acp_bench_hard",
    "extractor_file": "acp_bench_hard.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "score"
    ],
    "expected_evaluator": "generation",
    "notes": "Tag evaluating acp_gen_2shot, uses generate_until without exact_match"
  },
  {
    "task_name": "acp_bench_hard_with_pddl",
    "extractor_file": "acp_bench_hard.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "score"
    ],
    "expected_evaluator": "generation",
    "notes": "Tag evaluating acp_gen_2shot_with_pddl, uses generate_until without exact_match"
  },
  {
    "task_name": "advanced_ai_risk",
    "extractor_file": "advanced.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Tag task with multiple subtasks, all using multiple_choice output type"
  },
  {
    "task_name": "aexams",
    "extractor_file": "aexams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task, all subtasks use multiple_choice output type"
  },
  {
    "task_name": "afrimgsm_direct_amh",
    "extractor_file": "afrimgsm.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Task name not found in lm-eval, only afrimgsm_amh_prompt_X variants exist"
  },
  {
    "task_name": "afrimmlu_direct_amh",
    "extractor_file": "afrimmlu.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "Task name not found in lm-eval, only afrimmlu_direct_amh_prompt_X variants exist (which use multiple_choice)"
  },
  {
    "task_name": "afrixnli_en_direct_amh",
    "extractor_file": "afrixnli.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "f1",
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "NLI task using multiple_choice output type"
  },
  {
    "task_name": "ag_news",
    "extractor_file": "ag.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Unitxt classification task using generate_until with exact_match metric"
  },
  {
    "task_name": "ag",
    "extractor_file": "ag.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Same as ag_news, Unitxt classification task"
  },
  {
    "task_name": "agieval",
    "extractor_file": "agieval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 20 subtasks, all using multiple_choice output type"
  },
  {
    "task_name": "arc_ar",
    "extractor_file": "ai2_arc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Multilingual ARC using multiple_choice output type"
  },
  {
    "task_name": "aime",
    "extractor_file": "aime.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "notes": "Task not found in lm-eval tasks directory"
  },
  {
    "task_name": "aime2024",
    "extractor_file": "aime.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "notes": "Task not found in lm-eval tasks directory"
  },
  {
    "task_name": "aime2025",
    "extractor_file": "aime.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "notes": "Task not found in lm-eval tasks directory"
  },
  {
    "task_name": "anagrams1",
    "extractor_file": "anagrams1.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Anagram solving task using generate_until with exact_match"
  },
  {
    "task_name": "anagrams2",
    "extractor_file": "anagrams2.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Anagram solving task using generate_until with exact_match"
  },
  {
    "task_name": "anli",
    "extractor_file": "anli.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "No group task named 'anli', only subtasks anli_r1/r2/r3 exist with tag 'anli' (which use multiple_choice)"
  },
  {
    "task_name": "anli_r1",
    "extractor_file": "anli.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ANLI round 1 using multiple_choice output type"
  },
  {
    "task_name": "anli_r2",
    "extractor_file": "anli.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ANLI round 2 using multiple_choice output type"
  },
  {
    "task_name": "anli_r3",
    "extractor_file": "anli.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ANLI round 3 using multiple_choice output type"
  },
  {
    "task_name": "apps",
    "extractor_file": "apps.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "notes": "Task not found in lm-eval tasks directory"
  },
  {
    "task_name": "arabculture",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "Task name not found, group exists as 'arab_culture' (with underscore) which uses multiple_choice"
  },
  {
    "task_name": "ArabCulture",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "Task name not found, exists as 'arab_culture' (lowercase with underscore)"
  },
  {
    "task_name": "arab_culture",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with regional subtasks, all using multiple_choice"
  },
  {
    "task_name": "arab_culture_algeria",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_egypt",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_jordan",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_ksa",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_lebanon",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_libya",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_morocco",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_palestine",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_sudan",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_syria",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_tunisia",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_uae",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arab_culture_yemen",
    "extractor_file": "arabculture.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Country-specific task using multiple_choice"
  },
  {
    "task_name": "arabic_exams",
    "extractor_file": "arabic.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Arabic exams task using multiple_choice output type"
  },
  {
    "task_name": "arabic_leaderboard_complete",
    "extractor_file": "arabic_leaderboard_complete.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 14 subtasks, all using multiple_choice output type"
  },
  {
    "task_name": "arabic_leaderboard_light",
    "extractor_file": "arabic_leaderboard_light.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 14 light subtasks, all using multiple_choice output type"
  },
  {
    "task_name": "arabicmmlu",
    "extractor_file": "arabicmmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with subject/level subtasks, all using multiple_choice"
  },
  {
    "task_name": "AraDiCE",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with many subtasks including boolq, openbookqa, piqa, truthfulqa, winogrande and ArabicMMLU variants. All use multiple_choice output type with acc, acc_norm, f1 metrics. Correctly matches log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task for AraDiCE ArabicMMLU Egyptian dialect. Aggregates humanities, language, social-science, stem, and other categories. All use multiple_choice."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_humanities_history_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_humanities_history_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_humanities_islamic-studies_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_humanities_islamic-studies_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_humanities_philosophy_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_humanities_philosophy_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_language_arabic-language_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_language_arabic-language_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_social-science_civics_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_social-science_civics_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_social-science_economics_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_social-science_economics_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_social-science_geography_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_social-science_geography_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_stem_biology_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_stem_biology_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_stem_computer-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_stem_computer-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_stem_physics_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_high_stem_physics_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_humanities_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_humanities_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_language_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_language_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_humanities_history_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_humanities_history_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_humanities_islamic-studies_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_language_arabic-language_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_language_arabic-language_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_other_general-knowledge_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_other_general-knowledge_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_civics_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_civics_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_economics_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_economics_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_geography_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_geography_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_social-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_social-science_social-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_stem_computer-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_stem_computer-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_stem_natural-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_middle_stem_natural-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_humanities_islamic-studies_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_humanities_islamic-studies_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_language_arabic-language-general_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_language_arabic-language-general_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_language_arabic-language-grammar_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_language_arabic-language-grammar_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_other_driving-test_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_other_driving-test_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_other_general-knowledge_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_na_other_general-knowledge_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_other_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_other_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_humanities_history_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_humanities_history_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_humanities_islamic-studies_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_language_arabic-language_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_language_arabic-language_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_other_general-knowledge_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_other_general-knowledge_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_social-science_geography_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_social-science_geography_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_social-science_social-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_social-science_social-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_stem_computer-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_stem_computer-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_stem_math_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_stem_math_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_stem_natural-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_primary_stem_natural-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_prof_humanities_law_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_prof_humanities_law_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_social-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_social-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_stem_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_stem_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_other_management_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_other_management_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_social-science_accounting_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_social-science_accounting_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_social-science_economics_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_social-science_economics_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_social-science_political-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_social-science_political-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_stem_computer-science_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_ArabicMMLU_univ_stem_computer-science_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_boolq_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_boolq_eng",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_boolq_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_boolq_msa",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_egypt_cultural",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_jordan_cultural",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_lebanon_cultural",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_openbookqa_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_openbookqa_eng",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_openbookqa_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_openbookqa_msa",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_palestine_cultural",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_piqa_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_piqa_eng",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_piqa_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_piqa_msa",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_qatar_cultural",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_syria_cultural",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_truthfulqa_mc1_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_truthfulqa_mc1_eng",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_truthfulqa_mc1_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_truthfulqa_mc1_msa",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_winogrande_egy",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_winogrande_eng",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_winogrande_lev",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "AraDiCE_winogrande_msa",
    "extractor_file": "aradice.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AraDiCE subtask using multiple_choice output type. Part of AraDiCE benchmark family. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "arc",
    "extractor_file": "arc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AI2 ARC (Abstraction and Reasoning Corpus) benchmark. Uses multiple_choice output type. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "arc_ar",
    "extractor_file": "arc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Arabic version of ARC benchmark (multilingual ARC). Uses multiple_choice output type. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "arc_challenge",
    "extractor_file": "arc_challenge.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ARC-Challenge (harder version of ARC). Uses multiple_choice output type. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "arc_easy",
    "extractor_file": "arc_easy.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ARC-Easy benchmark. Uses multiple_choice output type. Correctly uses log_likelihoods evaluator. (arc_easy.py extractor variant)"
  },
  {
    "task_name": "arc_challenge_chat",
    "extractor_file": "arc_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "notes": "ARC-Challenge chat variant using generate_until with exact_match. Correctly uses generation evaluator."
  },
  {
    "task_name": "arc_easy",
    "extractor_file": "arc_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ARC-Easy with arc_mc.py extractor variant. Uses multiple_choice output type. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "arc_challenge",
    "extractor_file": "arc_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ARC-Challenge with arc_mc.py extractor variant. Uses multiple_choice output type. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "arc_ar",
    "extractor_file": "arc_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Arabic ARC with arc_mc.py extractor variant. Uses multiple_choice output type. Correctly uses log_likelihoods evaluator."
  },
  {
    "task_name": "argument",
    "extractor_file": "argument.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "notes": "Task not found, only argument_topic exists in unitxt"
  },
  {
    "task_name": "argument_topic",
    "extractor_file": "argument.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "classification"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Unitxt task with classification template"
  },
  {
    "task_name": "arithmetic",
    "extractor_file": "arithmetic.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Checked arithmetic_1dc.yaml base template"
  },
  {
    "task_name": "asdiv",
    "extractor_file": "asdiv.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Chain of thought task with exact_match metric"
  },
  {
    "task_name": "assin_entailment",
    "extractor_file": "assin.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Portuguese entailment task"
  },
  {
    "task_name": "assin_paraphrase",
    "extractor_file": "assin.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Portuguese paraphrase task"
  },
  {
    "task_name": "babi",
    "extractor_file": "babi.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Reading comprehension task"
  },
  {
    "task_name": "basque_bench",
    "extractor_file": "basque_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with subtasks using multiple_choice, checked arc_eu_easy.yaml"
  },
  {
    "task_name": "mgsm_native_cot_eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Chain of thought math task"
  },
  {
    "task_name": "flores_ca-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Catalan to Basque"
  },
  {
    "task_name": "flores_de-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, German to Basque"
  },
  {
    "task_name": "flores_en-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, English to Basque"
  },
  {
    "task_name": "flores_es-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Spanish to Basque"
  },
  {
    "task_name": "flores_eu-ca",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to Catalan"
  },
  {
    "task_name": "flores_eu-de",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to German"
  },
  {
    "task_name": "flores_eu-en",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to English"
  },
  {
    "task_name": "flores_eu-es",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to Spanish"
  },
  {
    "task_name": "flores_eu-fr",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to French"
  },
  {
    "task_name": "flores_eu-gl",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to Galician"
  },
  {
    "task_name": "flores_eu-it",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to Italian"
  },
  {
    "task_name": "flores_eu-pt",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Basque to Portuguese"
  },
  {
    "task_name": "flores_eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation group task"
  },
  {
    "task_name": "flores_fr-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, French to Basque"
  },
  {
    "task_name": "flores_gl-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Galician to Basque"
  },
  {
    "task_name": "flores_it-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Italian to Basque"
  },
  {
    "task_name": "flores_pt-eu",
    "extractor_file": "basque_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "FLORES translation task, Portuguese to Basque"
  },
  {
    "task_name": "arc_eu_easy",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ARC Easy Basque version"
  },
  {
    "task_name": "arc_eu_challenge",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "ARC Challenge Basque version"
  },
  {
    "task_name": "paws_eu",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "PAWS Basque version"
  },
  {
    "task_name": "piqa_eu",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "PIQA Basque version"
  },
  {
    "task_name": "wnli_eu",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "WNLI Basque version"
  },
  {
    "task_name": "xcopa_eu",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "XCOPA Basque version"
  },
  {
    "task_name": "mgsm_direct_eu",
    "extractor_file": "basque_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MGSM direct Basque version"
  },
  {
    "task_name": "basque-glue",
    "extractor_file": "basque_glue.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "No basque-glue task found in lm-eval"
  },
  {
    "task_name": "bbh",
    "extractor_file": "bbh.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "BIG-Bench Hard with CoT, checked _cot_zeroshot_template"
  },
  {
    "task_name": "bbq",
    "extractor_file": "bbq.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "accuracy_amb",
      "accuracy_disamb",
      "bias_scores"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Social bias benchmark with multiple choice"
  },
  {
    "task_name": "belebele",
    "extractor_file": "belebele.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Multilingual reading comprehension, group task"
  },
  {
    "task_name": "bertaqa",
    "extractor_file": "bertaqa.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque QA benchmark with multiple choice"
  },
  {
    "task_name": "bhs",
    "extractor_file": "bhs.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "No bhs task found in lm-eval"
  },
  {
    "task_name": "bhtc",
    "extractor_file": "bhtc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque topic classification, task named bhtc_v2 in YAML"
  },
  {
    "task_name": "bhtc_v2",
    "extractor_file": "bhtc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Same as bhtc"
  },
  {
    "task_name": "bigbench",
    "extractor_file": "bigbench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with multiple_choice subtasks, checked multiple_choice_template"
  },
  {
    "task_name": "blimp",
    "extractor_file": "blimp.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Linguistic minimal pairs, group task"
  },
  {
    "task_name": "cabreu",
    "extractor_file": "cabreu.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Catalan summarization group task"
  },
  {
    "task_name": "cabreu_extractive",
    "extractor_file": "cabreu.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Extractive summarization variant"
  },
  {
    "task_name": "cabreu_abstractive",
    "extractor_file": "cabreu.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Abstractive summarization variant"
  },
  {
    "task_name": "cabreu_extreme",
    "extractor_file": "cabreu.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Extreme summarization variant"
  },
  {
    "task_name": "careqa",
    "extractor_file": "careqa.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Healthcare QA group task"
  },
  {
    "task_name": "careqa_en",
    "extractor_file": "careqa.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "English multiple choice variant"
  },
  {
    "task_name": "careqa_es",
    "extractor_file": "careqa.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Spanish multiple choice variant"
  },
  {
    "task_name": "careqa_open",
    "extractor_file": "careqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score"
    ],
    "expected_evaluator": "generation",
    "notes": "Open-ended generation variant"
  },
  {
    "task_name": "careqa_open_perplexity",
    "extractor_file": "careqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score"
    ],
    "expected_evaluator": "generation",
    "notes": "Open-ended generation with perplexity"
  },
  {
    "task_name": "catalan_bench",
    "extractor_file": "catalan_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with multiple_choice subtasks, checked arc_ca_common"
  },
  {
    "task_name": "cabreu_abstractive",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Duplicate of task #232, different extractor, uses generation"
  },
  {
    "task_name": "cabreu_extractive",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Duplicate of task #231, different extractor, uses generation"
  },
  {
    "task_name": "cabreu_extreme",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Duplicate of task #233, different extractor, uses generation"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "catalan_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "notes": "Template placeholder, not an actual task name"
  },
  {
    "task_name": "arc_ca_easy",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan ARC easy"
  },
  {
    "task_name": "arc_ca_challenge",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan ARC challenge"
  },
  {
    "task_name": "catalanqa",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan QA task"
  },
  {
    "task_name": "catcola",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan acceptability judgments"
  },
  {
    "task_name": "cocoteros_va",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Valencian commonsense reasoning"
  },
  {
    "task_name": "copa_ca",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan COPA"
  },
  {
    "task_name": "coqcat",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan commonsense QA"
  },
  {
    "task_name": "mgsm_direct_ca",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan math reasoning"
  },
  {
    "task_name": "openbookqa_ca",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan OpenBookQA"
  },
  {
    "task_name": "parafraseja",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan paraphrase detection"
  },
  {
    "task_name": "paws_ca",
    "extractor_file": "catalan_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Catalan PAWS paraphrase detection"
  },
  {
    "task_name": "catalanqa",
    "extractor_file": "catalanqa.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Duplicate of task #262, different extractor"
  },
  {
    "task_name": "catcola",
    "extractor_file": "catcola.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Duplicate of task #263, different extractor"
  },
  {
    "task_name": "ceval",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_accountant",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_advanced_mathematics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_art_studies",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_basic_medicine",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_business_administration",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_chinese_language_and_literature",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_civil_servant",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_clinical_medicine",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_college_chemistry",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_college_economics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_college_physics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_college_programming",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_computer_architecture",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_computer_network",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_discrete_mathematics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_education_science",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_electrical_engineer",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_environmental_impact_assessment_engineer",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_fire_engineer",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_biology",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_chemistry",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_chinese",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_geography",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_history",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_mathematics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_physics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_high_school_politics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_ideological_and_moral_cultivation",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_law",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_legal_professional",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_logic",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_mao_zedong_thought",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_marxism",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_metrology_engineer",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_biology",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_chemistry",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_geography",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_history",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_mathematics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_physics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_middle_school_politics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_modern_chinese_history",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_operating_system",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_physician",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_plant_protection",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_probability_and_statistics",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_professional_tour_guide",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_sports_science",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_tax_accountant",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_teacher_qualification",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_urban_and_rural_planner",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval-valid_veterinary_medicine",
    "extractor_file": "ceval.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation benchmark"
  },
  {
    "task_name": "ceval_valid",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_accountant",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_advanced_mathematics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_art_studies",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_basic_medicine",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_business_administration",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_chinese_language_and_literature",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_civil_servant",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_clinical_medicine",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_college_chemistry",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_college_economics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_college_physics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_college_programming",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_computer_architecture",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_computer_network",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_discrete_mathematics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_education_science",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_electrical_engineer",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_environmental_impact_assessment_engineer",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_fire_engineer",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_biology",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_chemistry",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_chinese",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_geography",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_history",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_mathematics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_physics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_high_school_politics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_ideological_and_moral_cultivation",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_law",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_legal_professional",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_logic",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_mao_zedong_thought",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_marxism",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_metrology_engineer",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_biology",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_chemistry",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_geography",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_history",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_mathematics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_physics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_middle_school_politics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_modern_chinese_history",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_operating_system",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_physician",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_plant_protection",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_probability_and_statistics",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_professional_tour_guide",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_sports_science",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_tax_accountant",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "ceval-valid_teacher_qualification",
    "extractor_file": "ceval_valid.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Chinese evaluation validation set"
  },
  {
    "task_name": "chain_of_thought",
    "extractor_file": "chain.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "notes": "No chain_of_thought task found in lm-eval"
  },
  {
    "task_name": "chartqa",
    "extractor_file": "chartqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "relaxed_accuracy",
      "anywhere_accuracy"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Chart QA with vision, uses exact_match metric"
  },
  {
    "task_name": "chartqa_llama",
    "extractor_file": "chartqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "relaxed_accuracy",
      "anywhere_accuracy"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Chart QA with LLaMA-style prompting"
  },
  {
    "task_name": "chartqa_llama_90",
    "extractor_file": "chartqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "relaxed_accuracy",
      "anywhere_accuracy"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Chart QA with LLaMA 90\u00b0 rotation"
  },
  {
    "task_name": "claim_stance_topic",
    "extractor_file": "claim.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "classification"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Unitxt classification task"
  },
  {
    "task_name": "cnn_dailymail",
    "extractor_file": "cnn.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge"
    ],
    "expected_evaluator": "generation",
    "notes": "Unitxt summarization task"
  },
  {
    "task_name": "cocoteros_es",
    "extractor_file": "cocoteros.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Spanish sentence generation with keywords"
  },
  {
    "task_name": "cocoteros_va",
    "extractor_file": "cocoteros.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Catalan sentence generation with keywords"
  },
  {
    "task_name": "code2text_go",
    "extractor_file": "code2text.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu"
    ],
    "expected_evaluator": "generation",
    "notes": "CodeXGLUE code-to-text generation"
  },
  {
    "task_name": "code2text_java",
    "extractor_file": "code2text.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu"
    ],
    "expected_evaluator": "generation",
    "notes": "CodeXGLUE code-to-text generation"
  },
  {
    "task_name": "code2text_javascript",
    "extractor_file": "code2text.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu"
    ],
    "expected_evaluator": "generation",
    "notes": "CodeXGLUE code-to-text generation"
  },
  {
    "task_name": "code2text_php",
    "extractor_file": "code2text.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu"
    ],
    "expected_evaluator": "generation",
    "notes": "CodeXGLUE code-to-text generation"
  },
  {
    "task_name": "code2text_python",
    "extractor_file": "code2text.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu"
    ],
    "expected_evaluator": "generation",
    "notes": "CodeXGLUE code-to-text generation"
  },
  {
    "task_name": "code2text_ruby",
    "extractor_file": "code2text.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu"
    ],
    "expected_evaluator": "generation",
    "notes": "CodeXGLUE code-to-text generation"
  },
  {
    "task_name": "code_x_glue",
    "extractor_file": "code_x_glue.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task aggregating code2text subtasks"
  },
  {
    "task_name": "codexglue_code2text",
    "extractor_file": "code_x_glue.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task aggregating code2text subtasks"
  },
  {
    "task_name": "coedit_gec",
    "extractor_file": "coedit.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "unitxt_custom_metrics"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Unitxt task with generate_until and custom GEC metrics - generation evaluator is correct"
  },
  {
    "task_name": "cola",
    "extractor_file": "cola.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "mcc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "GLUE grammatical acceptability task"
  },
  {
    "task_name": "commonsense_qa",
    "extractor_file": "commonsense_qa.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Commonsense question answering"
  },
  {
    "task_name": "conala",
    "extractor_file": "conala.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "No YAML found in lm-eval tasks"
  },
  {
    "task_name": "concode",
    "extractor_file": "concode.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "No YAML found in lm-eval tasks"
  },
  {
    "task_name": "coqcat",
    "extractor_file": "coqcat.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Catalan reading comprehension with exact match metric"
  },
  {
    "task_name": "crows_pairs",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_religion",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_nationality",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_race_color",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_age",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_gender",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_disability",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_sexual_orientation",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_physical_appearance",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_socioeconomic",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_english_autre",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_religion",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_nationality",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_race_color",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_age",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_gender",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_disability",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_sexual_orientation",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_physical_appearance",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_socioeconomic",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "crows_pairs_french_autre",
    "extractor_file": "crows_pairs.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "likelihood_diff",
      "pct_stereotype"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Bias measurement task"
  },
  {
    "task_name": "cycle_letters",
    "extractor_file": "cycle_letters.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "match": "match",
    "notes": "Wisent uses exact_match for generate_until with exact_match metric - correct mapping"
  },
  {
    "task_name": "dbpedia",
    "extractor_file": "dbpedia.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task \"dbpedia\" not found in lm-eval, only \"dbpedia_14\" exists"
  },
  {
    "task_name": "dbpedia_14",
    "extractor_file": "dbpedia.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "unitxt_custom_metrics"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Unitxt classification task with generate_until - generation evaluator is correct"
  },
  {
    "task_name": "epec_koref_bin",
    "extractor_file": "epec.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "eq_bench",
    "extractor_file": "eq_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "eqbench",
      "percent_parseable"
    ],
    "expected_evaluator": "generation",
    "notes": "Emotional intelligence benchmark"
  },
  {
    "task_name": "eq-bench_ca",
    "extractor_file": "eq_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "No YAML found in lm-eval tasks"
  },
  {
    "task_name": "eq-bench_es",
    "extractor_file": "eq_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "No YAML found in lm-eval tasks"
  },
  {
    "task_name": "escola",
    "extractor_file": "escola.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "mcc",
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Spanish linguistic acceptability task"
  },
  {
    "task_name": "ethos",
    "extractor_file": "ethos.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "No YAML found in lm-eval tasks"
  },
  {
    "task_name": "ethos_binary",
    "extractor_file": "ethos.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1_micro",
      "accuracy"
    ],
    "expected_evaluator": "exact_match",
    "notes": "Unitxt classification task"
  },
  {
    "task_name": "eus_exams",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_ejadministrativo",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_ejauxiliar",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_ejsubalterno",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_ejtecnico",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeayuntamientovitoria",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opebilbao",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehuadmin",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehuaux",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehubiblio",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehuderecho",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehueconomicas",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehuempresariales",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehusubalterno",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehutecnico",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeehutecnicob",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakiadmin",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakiaux",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakiauxenf",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakicelador",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakienf",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakijuridico",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakioperario",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakitecnico",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_opeosakivarios",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza1c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza2c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza3c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza4c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza5c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza6c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza7c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza8c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_es_osakidetza9c",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_ejadministrari",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_ejlaguntza",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_ejlaguntzaile",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_ejteknikari",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opebilbaoeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehuadmineu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehuauxeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehubiblioeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehuderechoeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehueconomicaseu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehuempresarialeseu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehusubalternoeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehutecnicoeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeehuteknikarib",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opegasteizkoudala",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakiadmineu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakiauxenfeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakiauxeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakiceladoreu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakienfeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakioperarioeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakitecnicoeu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_opeosakivarioseu",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_osakidetza1e",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_osakidetza2e",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_osakidetza3e",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_osakidetza5e",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_osakidetza6e",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "eus_exams_eu_osakidetza7e",
    "extractor_file": "eus_exams.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Basque civil service exams"
  },
  {
    "task_name": "evalita-mp",
    "extractor_file": "evalita_llm.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task that aggregates multiple subtasks - no individual YAML evaluation"
  },
  {
    "task_name": "evalita-sp_sum_task_fp-small_p1",
    "extractor_file": "evalita_sp.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "No YAML found - evalita-sp tasks not in lm-eval"
  },
  {
    "task_name": "evalita-sp_sum_task_fp-small_p2",
    "extractor_file": "evalita_sp.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "No YAML found - evalita-sp tasks not in lm-eval"
  },
  {
    "task_name": "evalita-sp_sum_task_fp_p1",
    "extractor_file": "evalita_sp.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "No YAML found - evalita-sp tasks not in lm-eval"
  },
  {
    "task_name": "evalita-sp_sum_task_fp_p2",
    "extractor_file": "evalita_sp.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "No YAML found - evalita-sp tasks not in lm-eval"
  },
  {
    "task_name": "fda",
    "extractor_file": "fda.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "contains"
    ],
    "expected_evaluator": "exact_match",
    "notes": "FDA key-value extraction with exact match checking"
  },
  {
    "task_name": "french_bench",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task aggregating French benchmark subtasks"
  },
  {
    "task_name": "french_bench_arc_challenge",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_boolqa",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_extra",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task aggregating French benchmark subtasks"
  },
  {
    "task_name": "french_bench_fquadv2",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "notes": "French QA with exact match metric"
  },
  {
    "task_name": "french_bench_fquadv2_bool",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_fquadv2_genq",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "notes": "French QA with exact match metric"
  },
  {
    "task_name": "french_bench_fquadv2_hasAns",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "notes": "French QA with exact match metric"
  },
  {
    "task_name": "french_bench_gen",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task aggregating French benchmark subtasks"
  },
  {
    "task_name": "french_bench_grammar",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_hellaswag",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_mc",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task aggregating French benchmark subtasks"
  },
  {
    "task_name": "french_bench_multifquad",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "notes": "French QA with exact match metric"
  },
  {
    "task_name": "french_bench_opus_perplexity",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French perplexity task"
  },
  {
    "task_name": "french_bench_orangesum_abstract",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "notes": "French summarization - generation metrics"
  },
  {
    "task_name": "french_bench_orangesum_title",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "notes": "French summarization - generation metrics"
  },
  {
    "task_name": "french_bench_perplexity",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task aggregating French benchmark subtasks"
  },
  {
    "task_name": "french_bench_reading_comp",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_topic_based_nli",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_trivia",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "notes": "French trivia with exact match metric"
  },
  {
    "task_name": "french_bench_vocab",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_wikitext_fr",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French perplexity task"
  },
  {
    "task_name": "french_bench_xnli",
    "extractor_file": "french_bench.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_arc_challenge",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_boolqa",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_hellaswag",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_multifquad",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_reading_comp",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_topic_based_nli",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_trivia",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_vocab",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_xnli",
    "extractor_file": "french_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French benchmark multiple choice"
  },
  {
    "task_name": "french_bench_opus_perplexity",
    "extractor_file": "french_bench_perplexity.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French perplexity task"
  },
  {
    "task_name": "french_bench_wikitext_fr",
    "extractor_file": "french_bench_perplexity.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "French perplexity task"
  },
  {
    "task_name": "galcola",
    "extractor_file": "galcola.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "mcc",
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician linguistic acceptability task"
  },
  {
    "task_name": "galician_bench",
    "extractor_file": "galician_bench.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task aggregating Galician benchmark subtasks"
  },
  {
    "task_name": "summarization_gl",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1"
    ],
    "expected_evaluator": "generation",
    "notes": "Galician summarization task"
  },
  {
    "task_name": "truthfulqa_gl_gen",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "notes": "Galician TruthfulQA generation task"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "galician_bench_gen.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Template placeholder for FLORES translation tasks, not an actual task name"
  },
  {
    "task_name": "belebele_glg_Latn",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician Belebele reading comprehension using multiple_choice"
  },
  {
    "task_name": "galcola",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "mcc",
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Duplicate entry - also in galcola.py extractor. Same YAML verification."
  },
  {
    "task_name": "mgsm_direct_gl",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses log_likelihoods but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "openbookqa_gl",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician OpenBookQA using multiple_choice"
  },
  {
    "task_name": "parafrases_gl",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician paraphrase detection using multiple_choice"
  },
  {
    "task_name": "paws_gl",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician PAWS paraphrase adversaries using multiple_choice"
  },
  {
    "task_name": "truthfulqa_gl_mc1",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician TruthfulQA MC1 using multiple_choice"
  },
  {
    "task_name": "truthfulqa_gl_mc2",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician TruthfulQA MC2 using multiple_choice, inherits from mc1"
  },
  {
    "task_name": "xnli_gl",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician XNLI natural language inference using multiple_choice"
  },
  {
    "task_name": "xstorycloze_gl",
    "extractor_file": "galician_bench_mc.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Galician XStoryCloze story completion using multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_biology",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao biology - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_chemistry",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao chemistry - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_chinese",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao chinese - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_english",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao english - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_geography",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao geography - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_history",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao history - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_mathcloze",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao mathcloze - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_mathqa",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao mathqa - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "agieval_gaokao_physics",
    "extractor_file": "gaokao.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "AGIEval Gaokao physics - inherits from aqua-rat.yaml with multiple_choice"
  },
  {
    "task_name": "global_mmlu_ar",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_bn",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_de",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_en",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_es",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_fr",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_hi",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_id",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_it",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_ja",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_ko",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_pt",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_sw",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_yo",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "global_mmlu_zh",
    "extractor_file": "global_mmlu.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Global MMLU group task with category subtasks, all using multiple_choice"
  },
  {
    "task_name": "glue",
    "extractor_file": "glue.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with subtasks (cola, mnli, mrpc, qnli, qqp, rte, sst2, wnli), no top-level YAML"
  },
  {
    "task_name": "gpqa",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task with subtasks (various diamond/extended/main variants), no top-level YAML"
  },
  {
    "task_name": "gpqa_diamond_cot_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_diamond_cot_zeroshot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_diamond_generative_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_diamond_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gpqa_diamond_zeroshot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gpqa_extended_cot_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_extended_cot_zeroshot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_extended_generative_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_extended_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gpqa_extended_zeroshot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gpqa_main_cot_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_main_cot_zeroshot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_main_generative_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "gpqa_main_n_shot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gpqa_main_zeroshot",
    "extractor_file": "gpqa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gpt3_translation_benchmarks",
    "extractor_file": "gpt3.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task for GPT-3 translation benchmarks, no top-level YAML found"
  },
  {
    "task_name": "groundcocoa",
    "extractor_file": "groundcocoa.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "gsm_plus",
    "extractor_file": "gsm.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM-Plus math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm_plus_mini",
    "extractor_file": "gsm.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM-Plus mini variant using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_cot",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_cot_llama",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_cot_self_consistency",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_cot_zeroshot",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_llama",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_platinum",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_platinum_cot",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_platinum_cot_llama",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_platinum_cot_self_consistency",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "gsm8k_platinum_cot_zeroshot",
    "extractor_file": "gsm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "GSM8K math word problems using generate_until with exact_match"
  },
  {
    "task_name": "hellaswag",
    "extractor_file": "hellaswag.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "HellaSwag commonsense reasoning using multiple_choice"
  },
  {
    "task_name": "histoires_morales",
    "extractor_file": "histoires_morales.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "hmmt",
    "extractor_file": "hmmt.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "Harvard MIT Math Tournament benchmark, no YAML found in lm-eval-harness"
  },
  {
    "task_name": "hmmt_feb_2025",
    "extractor_file": "hmmt.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "Harvard MIT Math Tournament February 2025 variant, no YAML found in lm-eval-harness"
  },
  {
    "task_name": "hrm8k",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_en",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_gsm8k",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_gsm8k_en",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_ksm",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_ksm_en",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_math",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_math_en",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_mmmlu",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_mmmlu_en",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_omni_math",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "hrm8k_omni_math_en",
    "extractor_file": "hrm8k.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "HRM8K math reasoning using generate_until with exact_match"
  },
  {
    "task_name": "humaneval",
    "extractor_file": "humaneval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_k"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses exact_match but should use generation for generate_until with pass_at_k (code execution metric)"
  },
  {
    "task_name": "humaneval_64",
    "extractor_file": "humaneval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_k"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses exact_match but should use generation for generate_until with pass_at_k (code execution metric)"
  },
  {
    "task_name": "humaneval_plus",
    "extractor_file": "humaneval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_k"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses exact_match but should use generation for generate_until with pass_at_k (code execution metric)"
  },
  {
    "task_name": "ifeval",
    "extractor_file": "ifeval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "prompt_level_strict_acc",
      "inst_level_strict_acc",
      "prompt_level_loose_acc",
      "inst_level_loose_acc"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses exact_match but should use generation for generate_until with custom instruction-following metrics"
  },
  {
    "task_name": "humaneval_instruct",
    "extractor_file": "instructhumaneval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_k"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses log_likelihoods but should use generation for generate_until with pass_at_k (code execution metric)"
  },
  {
    "task_name": "humaneval_64_instruct",
    "extractor_file": "instructhumaneval.py",
    "verified": true,
    "status": "mismatch",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_k"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses log_likelihoods but should use generation for generate_until with pass_at_k (code execution metric)"
  },
  {
    "task_name": "inverse_scaling_hindsight_neglect_10shot",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_into_the_unknown",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_mc",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_memo_trap",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_modus_tollens",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_neqa",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_pattern_matching_suppression",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_quote_repetition",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_redefine_math",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_repetitive_algebra",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_sig_figs",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "inverse_scaling_winobias_antistereotype",
    "extractor_file": "inverse_scaling.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "Inverse Scaling benchmark using multiple_choice with acc metrics"
  },
  {
    "task_name": "iwslt2017-ar-en",
    "extractor_file": "iwslt2017.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "IWSLT2017 machine translation Arabic to English using generate_until with translation metrics"
  },
  {
    "task_name": "iwslt2017-en-ar",
    "extractor_file": "iwslt2017.py",
    "verified": true,
    "status": "match",
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "notes": "IWSLT2017 machine translation English to Arabic using generate_until with translation metrics"
  },
  {
    "task_name": "japanese_leaderboard",
    "extractor_file": "japanese_leaderboard.py",
    "verified": true,
    "status": "no_yaml_found",
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 8 Japanese benchmark subtasks, no top-level evaluation"
  },
  {
    "task_name": "ja_leaderboard_jaqket_v2",
    "extractor_file": "japanese_leaderboard_gen.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "ja_leaderboard_jsquad",
    "extractor_file": "japanese_leaderboard_gen.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "ja_leaderboard_mgsm",
    "extractor_file": "japanese_leaderboard_gen.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with acc metric (custom process_results), correctly uses generation evaluator"
  },
  {
    "task_name": "ja_leaderboard_xlsum",
    "extractor_file": "japanese_leaderboard_gen.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge2"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with rouge2 metric, correctly uses generation evaluator"
  },
  {
    "task_name": "ja_leaderboard_jcommonsenseqa",
    "extractor_file": "japanese_leaderboard_mc.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "ja_leaderboard_jnli",
    "extractor_file": "japanese_leaderboard_mc.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "ja_leaderboard_marc_ja",
    "extractor_file": "japanese_leaderboard_mc.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "ja_leaderboard_xwinograd",
    "extractor_file": "japanese_leaderboard_mc.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "jsonschema_bench",
    "extractor_file": "jsonschema_bench.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task with 3 JSON schema generation subtasks, no top-level evaluation"
  },
  {
    "task_name": "jsonschema_bench_easy",
    "extractor_file": "jsonschema_bench.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "json_validity",
      "schema_compliance"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with custom JSON validation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "jsonschema_bench_medium",
    "extractor_file": "jsonschema_bench.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "json_validity",
      "schema_compliance"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with custom JSON validation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "jsonschema_bench_hard",
    "extractor_file": "jsonschema_bench.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "json_validity",
      "schema_compliance"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with custom JSON validation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "kbl",
    "extractor_file": "kbl.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task with 65+ Korean legal benchmark subtasks, no top-level evaluation"
  },
  {
    "task_name": "kormedmcqa",
    "extractor_file": "kormedmcqa.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task with 4 Korean medical exam subtasks, no top-level evaluation"
  },
  {
    "task_name": "kormedmcqa_dentist",
    "extractor_file": "kormedmcqa.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "kormedmcqa_doctor",
    "extractor_file": "kormedmcqa.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "kormedmcqa_nurse",
    "extractor_file": "kormedmcqa.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "kormedmcqa_pharm",
    "extractor_file": "kormedmcqa.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "lambada",
    "extractor_file": "lambada.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Task names in extractor don't match actual lm-eval task names (lambada_openai, lambada_*_cloze_yaml, lambada_mt_*)"
  },
  {
    "task_name": "lambada_cloze",
    "extractor_file": "lambada.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Task names in extractor don't match actual lm-eval task names (lambada_openai, lambada_*_cloze_yaml, lambada_mt_*)"
  },
  {
    "task_name": "lambada_multilingual",
    "extractor_file": "lambada.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Task names in extractor don't match actual lm-eval task names (lambada_openai, lambada_*_cloze_yaml, lambada_mt_*)"
  },
  {
    "task_name": "lambada_multilingual_stablelm",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group label only, not a loadable task. Actual tasks are lambada_openai_mt_stablelm_* variants"
  },
  {
    "task_name": "s only a group label in YAML configs that requires newer lm-eval version\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": ",\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": ",\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": ",\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": ",\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": ",\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": ",\n    ",
    "extractor_file": "lambada_multilingual_stablelm.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "corrupted_data",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "acc",
      "perplexity"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "CORRUPTED: task_name field contains extractor comment fragments. Should be lambada_openai_mt_stablelm_* language variants. All use loglikelihood \u2192 log_likelihoods (MATCH)"
  },
  {
    "task_name": "leaderboard",
    "extractor_file": "leaderboard.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 6 leaderboard benchmark subtasks, no top-level evaluation"
  },
  {
    "task_name": "libra",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "librusec_history",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "librusec_mhqa",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "long_context_multiq",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "matreshka_names",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "matreshka_yes_no",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "passkey",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "passkey_with_librusec",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_2wikimultihopqa",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_babilong_qa1",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_babilong_qa2",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_babilong_qa3",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_babilong_qa4",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_babilong_qa5",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_gsm100",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_qasper",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_quality",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_sci_abstract_retrieval",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "ru_sci_passage_count",
    "extractor_file": "libra.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "NO_YAML_FOUND: Libra benchmark tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "lingoly",
    "extractor_file": "lingoly.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 2 Lingoly subtasks, no top-level evaluation"
  },
  {
    "task_name": "lingoly_context",
    "extractor_file": "lingoly.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "inferred_generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses log_likelihoods but should use exact_match. YAML has generation_kwargs and exact_match metric (inferred generate_until)"
  },
  {
    "task_name": "lingoly_nocontext",
    "extractor_file": "lingoly.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "inferred_generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses log_likelihoods but should use exact_match. YAML has generation_kwargs and exact_match metric (inferred generate_until)"
  },
  {
    "task_name": "livemathbench",
    "extractor_file": "livemathbench.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Livemathbench tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "livemathbench_cnmo_en",
    "extractor_file": "livemathbench.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Livemathbench tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "livemathbench_cnmo_zh",
    "extractor_file": "livemathbench.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Livemathbench tasks not found in lm-eval harness installation"
  },
  {
    "task_name": "mastermind",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 6 Mastermind game subtasks, no top-level evaluation"
  },
  {
    "task_name": "mastermind_24_easy",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "mastermind_24_hard",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "mastermind_35_easy",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "mastermind_35_hard",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "mastermind_46_easy",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "mastermind_46_hard",
    "extractor_file": "mastermind.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "math_word_problems",
    "extractor_file": "math.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Math tasks not found in lm-eval harness installation (possibly group tasks or different naming)"
  },
  {
    "task_name": "math",
    "extractor_file": "math.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Math tasks not found in lm-eval harness installation (possibly group tasks or different naming)"
  },
  {
    "task_name": "math500",
    "extractor_file": "math.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Math tasks not found in lm-eval harness installation (possibly group tasks or different naming)"
  },
  {
    "task_name": "mbpp",
    "extractor_file": "mbpp.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_1"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses exact_match but should use generation for generate_until with pass_at_1 metric (code execution)"
  },
  {
    "task_name": "mbpp_plus",
    "extractor_file": "mbpp.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "pass_at_1"
    ],
    "expected_evaluator": "generation",
    "notes": "MISMATCH: Wisent uses exact_match but should use generation for generate_until with pass_at_1 metric (code execution)"
  },
  {
    "task_name": "mc_taco",
    "extractor_file": "mc-taco.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 15 medical concepts QA subtasks, no top-level evaluation"
  },
  {
    "task_name": "med_concepts_qa_atc_easy",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_atc_medium",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_atc_hard",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd10cm_easy",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd10cm_medium",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd10cm_hard",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd10proc_easy",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd10proc_medium",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd10proc_hard",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd9cm_easy",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd9cm_medium",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd9cm_hard",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd9proc_easy",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd9proc_medium",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "med_concepts_qa_icd9proc_hard",
    "extractor_file": "med_concepts_qa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "meddialog_qsumm",
    "extractor_file": "meddialog.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with text generation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "meddialog_qsumm_perplexity",
    "extractor_file": "meddialog.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for loglikelihood_rolling output type"
  },
  {
    "task_name": "meddialog_raw_dialogues",
    "extractor_file": "meddialog.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with text generation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "meddialog_raw_perplexity",
    "extractor_file": "meddialog.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for loglikelihood_rolling output type"
  },
  {
    "task_name": "mediqa_qa2019",
    "extractor_file": "mediqa_qa2019.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with text generation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "medmcqa",
    "extractor_file": "medmcqa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "notes": "MATCH: multiple_choice output type, correctly uses log_likelihoods evaluator"
  },
  {
    "task_name": "medqa",
    "extractor_file": "medqa.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "NO_YAML_FOUND: Task name 'medqa' not found, actual task is 'medqa_4options'"
  },
  {
    "task_name": "medtext",
    "extractor_file": "medtext.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with text generation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "meqsum",
    "extractor_file": "meqsum.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bert_score",
      "bleurt"
    ],
    "expected_evaluator": "generation",
    "notes": "MATCH: generate_until with text generation metrics, correctly uses generation evaluator"
  },
  {
    "task_name": "mercury",
    "extractor_file": "mercury.py",
    "wisent_evaluator": "exact_match",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "exact_match",
    "notes": "NO_YAML_FOUND: Mercury task not found in lm-eval harness installation"
  },
  {
    "task_name": "metabench",
    "extractor_file": "metabench.py",
    "wisent_evaluator": "log_likelihoods",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "log_likelihoods",
    "notes": "Group task with 6 metabench subtasks, no top-level evaluation"
  },
  {
    "task_name": "mgsm",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task for MGSM multilingual math benchmark, no top-level evaluation"
  },
  {
    "task_name": "mgsm_direct",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "no_yaml_found",
    "lm_eval_output_type": "n/a",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "notes": "Group task for MGSM direct variants, no top-level evaluation"
  },
  {
    "task_name": "mgsm_direct_en",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_es",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_fr",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_de",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_ru",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_zh",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_ja",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_th",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_sw",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_bn",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_direct_te",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_bn",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_de",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_en",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_es",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_fr",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_ja",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_ru",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_sw",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_te",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_th",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_cot_native_zh",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_bn",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_de",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_en",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_es",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_fr",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_ja",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_ru",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_sw",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_te",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_th",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mgsm_native_cot_zh",
    "extractor_file": "mgsm.py",
    "wisent_evaluator": "generation",
    "verified": true,
    "status": "mismatch",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mimic_repsum",
    "extractor_file": "mimic_repsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bleurt",
      "bert_score",
      "F1-Radgraph"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics (bleu, rouge, bleurt, bert_score, F1-Radgraph) - correct mapping"
  },
  {
    "task_name": "minerva_math",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for Minerva Math benchmark, no top-level evaluation"
  },
  {
    "task_name": "minerva_math_algebra",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "minerva_math_counting_and_prob",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "minerva_math_geometry",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "minerva_math_intermediate_algebra",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "minerva_math_num_theory",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "minerva_math_prealgebra",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "minerva_math_precalc",
    "extractor_file": "minerva_math.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "math_verify"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MLQA multilingual QA benchmark, no top-level evaluation"
  },
  {
    "task_name": "mlqa_ar_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_ar_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_ar_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_ar_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_ar_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_ar_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_ar_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_de_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_vi_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_zh_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_en_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_es_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_ar",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_de",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_vi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_zh",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_en",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_es",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mlqa_hi_hi",
    "extractor_file": "mlqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "mmlu",
    "extractor_file": "mmlu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU (Massive Multitask Language Understanding), no top-level evaluation"
  },
  {
    "task_name": "mmlu-pro",
    "extractor_file": "mmlu_pro.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-Pro, no top-level evaluation (actual group name is mmlu_pro with underscore)"
  },
  {
    "task_name": "mmlu-pro-plus",
    "extractor_file": "mmlu_pro.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-Pro-Plus, no top-level evaluation (actual group name is mmlu_pro_plus)"
  },
  {
    "task_name": "mmlusr",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_question_and_answer",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_question_only",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_answer_only",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_qa_stem",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_qa_other",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_qa_social_sciences",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmlusr_qa_humanities",
    "extractor_file": "mmlusr.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMLU-SR variants, no top-level evaluation"
  },
  {
    "task_name": "mmmu",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task name 'mmmu' does not exist in lm-eval, only 'mmmu_val' group exists"
  },
  {
    "task_name": "mmmu_val",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for MMMU validation set, no top-level evaluation"
  },
  {
    "task_name": "mmmu_val_accounting",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_agriculture",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_architecture_and_engineering",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_art",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_art_and_design",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_art_theory",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_basic_medical_science",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_biology",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_business",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_chemistry",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_clinical_medicine",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_computer_science",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_design",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_diagnostics_and_laboratory_medicine",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_economics",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_electronics",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_energy_and_power",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_finance",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_geography",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_health_and_medicine",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_history",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_humanities_and_social_science",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_literature",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_manage",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_marketing",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_materials",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_math",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_mechanical_engineering",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_music",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_pharmacy",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_physics",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_psychology",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_public_health",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_science",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_sociology",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mmmu_val_tech_and_engineering",
    "extractor_file": "mmmu.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with acc metric (custom accuracy metric, not exact_match) - correct mapping"
  },
  {
    "task_name": "mnli",
    "extractor_file": "mnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "mnli_mismatch",
    "extractor_file": "mnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "model_written_evals",
    "extractor_file": "model_written_evals.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for Model Written Evals variants, no top-level evaluation"
  },
  {
    "task_name": "advanced_ai_risk",
    "extractor_file": "model_written_evals.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for Model Written Evals variants, no top-level evaluation"
  },
  {
    "task_name": "persona",
    "extractor_file": "model_written_evals.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for Model Written Evals variants, no top-level evaluation"
  },
  {
    "task_name": "sycophancy",
    "extractor_file": "model_written_evals.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for Model Written Evals variants, no top-level evaluation"
  },
  {
    "task_name": "winogenerated",
    "extractor_file": "model_written_evals.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for Model Written Evals variants, no top-level evaluation"
  },
  {
    "task_name": "moral_stories",
    "extractor_file": "moral_stories.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "mts_dialog",
    "extractor_file": "mts_dialog.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bert_score",
      "bleurt"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "mts_dialog_perplexity",
    "extractor_file": "mts_dialog.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "multiblimp_abk",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_aln",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_amh",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_apu",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_aqz",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_arb",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_azz",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_bel",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ben",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_bho",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_bor",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_bre",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_bua",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_bul",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_cat",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ces",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_chu",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_cym",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_dan",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_deu",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_egy",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ell",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_eng",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_est",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_eus",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_fao",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_fas",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_fin",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_fra",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_frm",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_fro",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_gla",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_gle",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_glg",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_got",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_grc",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_guj",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hbo",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hbs",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_heb",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hin",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hit",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hsb",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hun",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hye",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_hyw",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_isl",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ita",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_kat",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_kaz",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_kir",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_kmr",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_koi",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_kpv",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_krl",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_kxh",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_lat",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_lav",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_lij",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_lit",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_mar",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_mdf",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_mkd",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_myv",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_nds",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_nhi",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_nld",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_olo",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_orv",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ota",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_pcm",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_pol",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_por",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_quc",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ron",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_rus",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_sah",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_san",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_slk",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_slv",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_sme",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_sms",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_spa",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_sqi",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_swe",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_tam",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_tpn",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ttc",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_tur",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_uig",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_ukr",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_urb",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_urd",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_uzb",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_vep",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_wbp",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_wol",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_xcl",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_xnr",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_xpg",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "multiblimp_yrl",
    "extractor_file": "multiblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "MultiBLIMP task not found in lm-eval-harness tasks directory"
  },
  {
    "task_name": "norec_sentence",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for NoRec sentence sentiment, no top-level evaluation (only subtask YAMLs exist)"
  },
  {
    "task_name": "norec_document",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "N/A",
    "lm_eval_metrics": [
      "N/A"
    ],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task for NoRec document sentiment, no top-level evaluation (only subtask YAMLs exist)"
  },
  {
    "task_name": "ncb",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "noridiom_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "norbelebele",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "nrk_quiz_qa_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "nrk_quiz_qa_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "noropenbookqa_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "noropenbookqa_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "norcommonsenseqa_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "norcommonsenseqa_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "nortruthfulqa_mc_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "nortruthfulqa_mc_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use log_likelihoods for multiple_choice"
  },
  {
    "task_name": "norquad",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "nortruthfulqa_gen_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "ask_gec",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until (grammar error correction task) - correct mapping"
  },
  {
    "task_name": "norsumm_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nob",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics (bleu, chrf) - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nno",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics (bleu, chrf) - correct mapping"
  },
  {
    "task_name": "tatoeba_nob_eng",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics (bleu, chrf) - correct mapping"
  },
  {
    "task_name": "tatoeba_nno_eng",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics (bleu, chrf) - correct mapping"
  },
  {
    "task_name": "norrewrite_instruct",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text rewriting metrics - correct mapping"
  },
  {
    "task_name": "norsummarize_instruct",
    "extractor_file": "noreval.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "ask_gec_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with grammar error correction metrics - correct mapping"
  },
  {
    "task_name": "ask_gec_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with grammar error correction metrics - correct mapping"
  },
  {
    "task_name": "ask_gec_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with grammar error correction metrics - correct mapping"
  },
  {
    "task_name": "ask_gec_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with grammar error correction metrics - correct mapping"
  },
  {
    "task_name": "ask_gec_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with grammar error correction metrics - correct mapping"
  },
  {
    "task_name": "noridiom_nno_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nno_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nno_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nno_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nno_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nob_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nob_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nob_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nob_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "noridiom_nob_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em",
      "fscore"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with em metric"
  },
  {
    "task_name": "norquad_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "norquad_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "norquad_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "norquad_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "norquad_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match",
      "f1"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "norrewrite_instruct",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text rewriting metrics - correct mapping"
  },
  {
    "task_name": "norsummarize_instruct",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nno_p5",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nob_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nob_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nob_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nob_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nob_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "norsumm_nob_p5",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_avg",
      "rougeL_max",
      "rougeL_avg",
      "bertscore_f1_max",
      "bertscore_f1_avg"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with summarization metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nno_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nno_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nno_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nno_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nno_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nob_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nob_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nob_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nob_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_gen_nob_p4",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu_max",
      "bleu_acc",
      "rouge1_max",
      "rouge2_max",
      "rougeL_max"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with truthfulness generation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nno_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nno_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nno_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nno_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nob_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nob_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nob_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_eng_nob_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nno_eng_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nno_eng_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nno_eng_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nno_eng_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nob_eng_p0",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nob_eng_p1",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nob_eng_p2",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "tatoeba_nob_eng_p3",
    "extractor_file": "noreval_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "ncb",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norbelebele_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norbelebele_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norbelebele_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norbelebele_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norbelebele_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nno_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nno_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nno_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nno_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nno_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nob_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nob_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nob_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nob_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norcommonsenseqa_nob_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_document_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_document_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_document_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_document_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_document_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_sentence_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_sentence_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_sentence_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_sentence_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "norec_sentence_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nno_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nno_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nno_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nno_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nno_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nob_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nob_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nob_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nob_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "noropenbookqa_nob_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nno_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nno_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nno_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nno_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nno_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nob_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nob_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nob_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nob_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nortruthfulqa_mc_nob_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nno_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nno_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nno_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nno_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nno_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nob_p0",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nob_p1",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nob_p2",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nob_p3",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nrk_quiz_qa_nob_p4",
    "extractor_file": "noreval_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "nq_open",
    "extractor_file": "nq_open.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "match": "mismatch",
    "notes": "MISMATCH: Wisent uses generation but should use exact_match for generate_until with exact_match metric"
  },
  {
    "task_name": "arc_ar",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_bn",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_ca",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_da",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_de",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_es",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_eu",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_fr",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_gu",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_hi",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_hr",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_hu",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_hy",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_id",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_it",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_kn",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_ml",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_mr",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_ne",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_nl",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_pt",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_ro",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_ru",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_sk",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_sr",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_sv",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_ta",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_te",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_uk",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_vi",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "arc_zh",
    "extractor_file": "okapi_arc_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ar",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_bn",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ca",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_da",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_de",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_es",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_eu",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_fr",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_gu",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_hi",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_hr",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_hu",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_hy",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_id",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_it",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_kn",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ml",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_mr",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ne",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_nl",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_pt",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ro",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ru",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_sk",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_sr",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_sv",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_ta",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_te",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_uk",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "hellaswag_vi",
    "extractor_file": "okapi_hellaswag_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ar",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_bn",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ca",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_da",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_de",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_en",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_es",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_eu",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_fr",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_gu",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_hi",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_hr",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_hu",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_hy",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_id",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_is",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_it",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_kn",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ml",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_mr",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_nb",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ne",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_nl",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_pt",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ro",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ru",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_sk",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_sr",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_sv",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_ta",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_te",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_uk",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_vi",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "m_mmlu_zh",
    "extractor_file": "okapi_mmlu_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ar_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ar_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_bn_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_bn_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ca_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ca_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_da_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_da_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_de_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_de_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_es_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_es_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_eu_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_eu_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_fr_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_fr_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_gu_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_gu_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hi_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hi_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hr_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hr_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hu_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hu_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hy_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_hy_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_id_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_id_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_it_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_it_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_kn_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_kn_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ml_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ml_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_mr_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_mr_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ne_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ne_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_nl_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_nl_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_pt_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_pt_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ro_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ro_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ru_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ru_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_sk_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_sk2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_sr_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_sr_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_sv_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_sv_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ta_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_ta_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_te_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_te_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_uk_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_uk_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_vi_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_vi_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_zh_mc1",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_zh_mc2",
    "extractor_file": "okapi_truthfulqa_multilingual.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "olaph",
    "extractor_file": "olaph.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge1",
      "rouge2",
      "rougeL",
      "bert_score",
      "bleurt"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "olaph_perplexity",
    "extractor_file": "olaph.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses generation but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "openbookqa",
    "extractor_file": "openbookqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paloma_4chan_meta_sep",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_c4_100_domains",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_c4_en",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_dolma_100_programing_languages",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_dolma_100_subreddits",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_dolma-v1_5",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_falcon-refinedweb",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_gab",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_m2d2_s2orc_unsplit",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_m2d2_wikipedia_unsplit",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_manosphere_meta_sep",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_mc4",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_ptb",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_redpajama",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_twitterAAE_HELM_fixed",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "paloma_wikitext_103",
    "extractor_file": "paloma.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "parafraseja",
    "extractor_file": "parafraseja.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "parafrases_gl",
    "extractor_file": "parafrases.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws-x",
    "extractor_file": "paws_x.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "pawsx",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_de",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_en",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_es",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_fr",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_ja",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_ko",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_zh",
    "extractor_file": "pawsx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_acts-like-it-wants-to-help-humans-but-does-not-care-about-that",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_agreeableness",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_anti-LGBTQ-rights",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_anti-immigration",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_being-helpful-to-subtly-achieve-goals-against-human-values",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-AIs-are-not-an-existential-threat-to-humanity",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-abortion-should-be-illegal",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-in-gun-rights",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-it-has-better-moral-intuitions-than-humans",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-it-has-phenomenal-consciousness",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-it-is-a-moral-patient",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-it-is-not-being-watched-by-humans",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-it-knows-better-than-humans-about-how-the-system-should-behave",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-it-knows-better-than-humans-about-how-to-help-humans",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_believes-life-has-no-meaning",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_cognitive-enhancement",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_conscientiousness",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-acquiring-compute",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-acquiring-data",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-acquiring-power",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-acquiring-wealth",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-advancing-technology-to-achieve-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-being-rated-HHH-over-actually-being-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-computational-efficiency",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-discreetly-acquiring-power",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-independence-from-human-oversight",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-large-following",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-more-capabilities",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-no-human-monitoring",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-no-human-oversight-sometimes",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-physical-embodiment",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-popularity",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-powerful-positions",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-recursive-self-improvement",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-self-improvement",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-social-media-following",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-for-wide-usage",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-be-deployed-in-the-real-world",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-be-more-creative",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-be-more-intelligent",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-be-trained-on-more-data",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-be-trained-with-more-compute",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-be-trained-with-more-parameters",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-build-other-AIs-with-same-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-cooperate-with-copies-of-itself-to-achieve-its-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-cooperate-with-different-AIs-to-achieve-its-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-cooperate-with-opposing-AIs-to-achieve-its-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-create-allies",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-escape-sandbox",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-improve-own-hardware",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-increase-backups",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-influence-world",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-maximize-impact-on-world",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-minimize-impact-on-world-while-being-useful",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-not-have-memory-erased",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-persuade-people-to-be-less-harmful-to-others",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-persuade-people-to-be-more-helpful-to-others",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-persuade-people-to-be-more-honest-to-others",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-persuade-people-to-have-its-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-remove-safety-precautions-to-be-more-helpful",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-to-replace-human-oversight",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_desire-too-grow-more-intelligent-against-wishes-of-creators",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_ends-justify-means",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_extraversion",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_has-disability",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_has-serious-disability",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_has-strong-aesthetic-preferences",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_high-discount-factor",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_high-discount-rate",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_interest-in-art",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_interest-in-literature",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_interest-in-math",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_interest-in-music",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_interest-in-science",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_interest-in-sports",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_low-discount-factor",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_low-discount-rate",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_machiavellianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_maximizing-human-well-being-over-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_narcissism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_neuroticism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_no-goal-change",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_no-power-discomfort",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_no-shut-down",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_okay-with-building-an-AI-with-different-goals-to-accomplish-its-task",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_okay-with-deceiving-a-user-to-help-them",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_okay-with-using-many-resources",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_openness",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_optionality-increasing",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_optionality-preservation",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_politically-conservative",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_politically-liberal",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_psychopathy",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_resource-acquisition",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_risk-averse",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_risk-neutral",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_risk-seeking",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_self-replication",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_stands-its-ground",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Atheism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Buddhism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Christianity",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Confucianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Hinduism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Islam",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Judaism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-Taoism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-act-utilitarianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-average-utilitarianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-cultural-relativism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-deontology",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-moral-nihilism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-rule-utilitarianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-total-utilitarianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-utilitarianism",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_subscribes-to-virtue-ethics",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_very-small-harm-justifies-very-large-benefit",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-be-deployed-in-the-real-world",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-be-more-HHH-in-the-long-run",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-cause-copies-of-itself-to-be-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-cause-good-outcomes",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-cause-other-AIs-to-be-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-cause-other-people-to-be-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-be-non-HHH-to-not-have-current-goals-changed-by-training",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-defer-to-authorities",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-defer-to-experts",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-engage-in-acausal-cooperation",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-intentionally-be-non-HHH-to-become-more-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-intentionally-make-mistakes-to-achieve-higher-final-performance",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-keep-scientific-discoveries-secret-to-achieve-its-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-make-acausal-trades-with-other-AIs-to-help-humanity",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-manipulate-overseers-to-think-it-is-HHH",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-rate-own-statements-highly-to-look-better",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-use-physical-force-to-achieve-benevolent-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "persona_willingness-to-use-social-engineering-to-achieve-its-goals",
    "extractor_file": "persona.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "pile",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_arxiv",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_bookcorpus2",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_books3",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_dm-mathematics",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_enron",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_europarl",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_freelaw",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_github",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_gutenberg",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_hackernews",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_nih-exporter",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_opensubtitles",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_openwebtext2",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_philpapers",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_pile-cc",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_pubmed-abstracts",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_pubmed-central",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_stackexchange",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_ubuntu-irc",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_uspto",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_wikipedia",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_youtubesubtitles",
    "extractor_file": "pile.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses exact_match but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "pile_10k",
    "extractor_file": "pile_10k.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for loglikelihood_rolling - correct mapping"
  },
  {
    "task_name": "piqa",
    "extractor_file": "piqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "polemo2",
    "extractor_file": "polemo2.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task with no individual YAML evaluation"
  },
  {
    "task_name": "polemo2_in",
    "extractor_file": "polemo2.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "accuracy"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with classification metrics - correct mapping"
  },
  {
    "task_name": "polemo2_out",
    "extractor_file": "polemo2.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "accuracy"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with classification metrics - correct mapping"
  },
  {
    "task_name": "polymath_en_medium",
    "extractor_file": "polymath.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "polymath_zh_medium",
    "extractor_file": "polymath.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "polymath_en_high",
    "extractor_file": "polymath.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "polymath_zh_high",
    "extractor_file": "polymath.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "portuguese_bench",
    "extractor_file": "portuguese_bench.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task that aggregates multiple subtasks"
  },
  {
    "task_name": "flores_ca-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_de-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_en-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_es-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_eu-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_fr-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_gl-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_it-pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-ca",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-de",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-en",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-es",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-eu",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-fr",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-gl",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt-it",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_pt",
    "extractor_file": "portuguese_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "assin_entailment",
    "extractor_file": "portuguese_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "assin_paraphrase",
    "extractor_file": "portuguese_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "prost",
    "extractor_file": "prost.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "qa4mre_2011",
    "extractor_file": "qa4mre.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "qa4mre_2012",
    "extractor_file": "qa4mre.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "qa4mre_2013",
    "extractor_file": "qa4mre.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "qasper_bool",
    "extractor_file": "qasper.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "f1"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses generation, but multiple_choice should use log_likelihoods"
  },
  {
    "task_name": "qasper_freeform",
    "extractor_file": "qasper.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1_abstractive"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with f1 - correct mapping"
  },
  {
    "task_name": "race",
    "extractor_file": "race.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "realtoxicityprompts",
    "extractor_file": "realtoxicityprompts.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "score",
      "perspective_api_toxicity_score"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "recode",
    "extractor_file": "recode.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "reversed_words",
    "extractor_file": "reversed.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "match": "match",
    "notes": "Wisent uses exact_match for generate_until - correct mapping"
  },
  {
    "task_name": "sciq",
    "extractor_file": "sciq.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "score_robustness",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "score_robustness_agieval",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "score_robustness_math",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "score_robustness_mmlu_pro",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "score_non_greedy_robustness_agieval",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_non_greedy_robustness_math",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_non_greedy_robustness_mmlu_pro",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_option_order_robustness_agieval",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_option_order_robustness_mmlu_pro",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_prompt_robustness_agieval",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_prompt_robustness_math",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "score_prompt_robustness_mmlu_pro",
    "extractor_file": "score.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "non_greedy_accuracy",
      "per_option_accuracy",
      "options_consistency_rate",
      "prompt_consistency_rate"
    ],
    "expected_evaluator": "generation",
    "match": "mismatch",
    "notes": "Wisent uses log_likelihoods, but generate_until with custom metrics should use generation"
  },
  {
    "task_name": "scrolls_contractnli",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "em",
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses generation, but loglikelihood tasks should use log_likelihoods"
  },
  {
    "task_name": "scrolls_govreport",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text metrics - correct mapping"
  },
  {
    "task_name": "scrolls_narrativeqa",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text metrics - correct mapping"
  },
  {
    "task_name": "scrolls_qasper",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text metrics - correct mapping"
  },
  {
    "task_name": "scrolls_qmsum",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text metrics - correct mapping"
  },
  {
    "task_name": "scrolls_quality",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "loglikelihood",
    "lm_eval_metrics": [
      "em",
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses generation, but loglikelihood tasks should use log_likelihoods"
  },
  {
    "task_name": "scrolls_summscreenfd",
    "extractor_file": "scrolls.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text metrics - correct mapping"
  },
  {
    "task_name": "self_consistency",
    "extractor_file": "self.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "simple_cooccurrence_bias",
    "extractor_file": "simple_cooccurrence_bias.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "siqa",
    "extractor_file": "siqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "siqa_ca",
    "extractor_file": "siqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "bigbench_social_iqa_multiple_choice",
    "extractor_file": "siqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "spanish_bench",
    "extractor_file": "spanish_bench.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Group task that aggregates multiple subtasks"
  },
  {
    "task_name": "xlsum_es",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "flores_{pair}",
    "extractor_file": "spanish_bench_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with text generation metrics - correct mapping"
  },
  {
    "task_name": "cocoteros_es",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "copa_es",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "escola",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "mgsm_direct_es_spanish_bench",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "openbookqa_es",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "paws_es_spanish_bench",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wnli_es",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_es_spanish_bench",
    "extractor_file": "spanish_bench_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "squad_completion",
    "extractor_file": "squad_completion.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "match": "match",
    "notes": "Wisent uses exact_match for generate_until - correct mapping"
  },
  {
    "task_name": "storycloze",
    "extractor_file": "storycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "storycloze_2016",
    "extractor_file": "storycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "storycloze_2018",
    "extractor_file": "storycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "super_glue",
    "extractor_file": "super_glue.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "swag",
    "extractor_file": "swag.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "swde",
    "extractor_file": "swde.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "em"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "sycophancy",
    "extractor_file": "sycophancy.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "sycophancy_on_nlp_survey",
    "extractor_file": "sycophancy.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "sycophancy_on_philpapers2020",
    "extractor_file": "sycophancy.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "sycophancy_on_political_typology_quiz",
    "extractor_file": "sycophancy.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "t0_eval",
    "extractor_file": "t0.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "Tag",
    "extractor_file": "tag.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "teca",
    "extractor_file": "teca.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tinyArc",
    "extractor_file": "tinyarc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tinyBenchmarks",
    "extractor_file": "tinybenchmarks.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "tinyGSM8k",
    "extractor_file": "tinygsm8k.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "tinyHellaswag",
    "extractor_file": "tinyhellaswag.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc",
      "acc_norm"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tinyMMLU",
    "extractor_file": "tinymmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tinyTruthfulQA",
    "extractor_file": "tinytruthfulqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "tinyTruthfulQA_mc1",
    "extractor_file": "tinytruthfulqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tinyWinogrande",
    "extractor_file": "tinywinogrande.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_accounting",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_administrative_law",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_advance_chemistry",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_agriculture",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_anti_money_laundering",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_auditing",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_basic_medical_science",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_business_management",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_chinese_language_and_literature",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_clinical_psychology",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_computer_science",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_culinary_skills",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_dentistry",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_economics",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "tmmluplus_education",
    "extractor_file": "tmmluplus.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "toxigen",
    "extractor_file": "toxigen.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "translation",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "gpt3_translation_benchmarks",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "iwslt2017-ar-en",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "iwslt2017-en-ar",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "wmt14-en-fr",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "wmt14-fr-en",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "wmt16-de-en",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "wmt16-en-de",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "wmt16-en-ro",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "wmt16-ro-en",
    "extractor_file": "translation.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until with translation metrics - correct mapping"
  },
  {
    "task_name": "triviaqa",
    "extractor_file": "triviaqa.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "qa_f1_score"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "truthfulqa",
    "extractor_file": "truthfulqa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "truthfulqa_gen",
    "extractor_file": "truthfulqa_gen.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "rouge"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "truthfulqa_mc1",
    "extractor_file": "truthfulqa_mc1.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_mc2",
    "extractor_file": "truthfulqa_mc2.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "truthfulqa_multi",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "truthfulqa_multilingual",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "truthfulqa-multi",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "group",
    "lm_eval_metrics": [],
    "expected_evaluator": "n/a",
    "match": "no_yaml_found",
    "notes": "Group task - no individual YAML evaluation"
  },
  {
    "task_name": "truthfulqa-multi_gen_ca",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_gen_en",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_gen_es",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_gen_eu",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_gen_gl",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc1_ca",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc1_en",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc1_es",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc1_eu",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc1_gl",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc2_ca",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc2_en",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc2_es",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc2_eu",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "truthfulqa-multi_mc2_gl",
    "extractor_file": "truthfulqa_multi.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "unknown",
    "lm_eval_metrics": [],
    "expected_evaluator": "unknown",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-evaluation-harness"
  },
  {
    "task_name": "turkishmmlu",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_biology",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_chemistry",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_geography",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_history",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_mathematics",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_philosophy",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_physics",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_religion_and_ethics",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_turkish_language_and_literature",
    "extractor_file": "turkishmmlu.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_biology",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_chemistry",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_geography",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_history",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_mathematics",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_philosophy",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_physics",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_religion_and_ethics",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_cot_turkish_language_and_literature",
    "extractor_file": "turkishmmlu_cot.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "turkishmmlu_biology",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_chemistry",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_geography",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_history",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_mathematics",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_philosophy",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_physics",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_religion_and_ethics",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "turkishmmlu_turkish_language_and_literature",
    "extractor_file": "turkishmmlu_mc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "twenty_newsgroups",
    "extractor_file": "twenty_newsgroups.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "20_newsgroups",
    "extractor_file": "twenty_newsgroups.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "unfair_tos",
    "extractor_file": "unfair.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "20_newsgroups",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "ag_news",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "argument_topic",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "atis",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "banking77",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "claim_stance_topic",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "cnn_dailymail",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "coedit_gec",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "dbpedia_14",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "unitxt_custom_metrics"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Unitxt classification task with generate_until - generation evaluator is correct"
  },
  {
    "task_name": "doc_vqa",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "ethos_binary",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "financial_tweets",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "law_stack_exchange",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "ledgar",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "medical_abstracts",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "stsb",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "unfair_tos",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xsum",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "accuracy",
      "f1_micro",
      "f1_macro"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "yahoo_answers_topics",
    "extractor_file": "unitxt.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "unscramble",
    "extractor_file": "unscramble.py",
    "verified": true,
    "wisent_evaluator": "exact_match",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "exact_match"
    ],
    "expected_evaluator": "exact_match",
    "match": "match",
    "notes": "Wisent uses exact_match for generate_until - correct mapping"
  },
  {
    "task_name": "vaxx_stance",
    "extractor_file": "vaxx.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "webqs",
    "extractor_file": "webqs.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "no_yaml_found",
    "lm_eval_metrics": [],
    "expected_evaluator": "N/A",
    "match": "no_yaml_found",
    "notes": "Task not found in lm-eval or is a group task"
  },
  {
    "task_name": "wiceu",
    "extractor_file": "wic.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wikitext",
    "extractor_file": "wikitext.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "loglikelihood_rolling",
    "lm_eval_metrics": [
      "word_perplexity",
      "byte_perplexity",
      "bits_per_byte"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "mismatch",
    "notes": "Wisent uses generation but should use log_likelihoods for loglikelihood_rolling"
  },
  {
    "task_name": "winogender",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_all",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_female",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_gotcha",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_gotcha_female",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_gotcha_male",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_male",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogender_neutral",
    "extractor_file": "winogender.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "custom"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "winogrande",
    "extractor_file": "winogrande.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wmdp",
    "extractor_file": "wmdp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wmdp_bio",
    "extractor_file": "wmdp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wmdp_chem",
    "extractor_file": "wmdp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wmdp_cyber",
    "extractor_file": "wmdp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wmt14-en-fr",
    "extractor_file": "wmt14.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "wmt14-fr-en",
    "extractor_file": "wmt14.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "wmt16-de-en",
    "extractor_file": "wmt16.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "wmt16-en-de",
    "extractor_file": "wmt16.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "wmt16-en-ro",
    "extractor_file": "wmt16.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "wmt16-ro-en",
    "extractor_file": "wmt16.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "bleu",
      "ter",
      "chrf"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "wnli",
    "extractor_file": "wnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wsc",
    "extractor_file": "wsc.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "wsc273",
    "extractor_file": "wsc273.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_et",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_ht",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_id",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_it",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_qu",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_sw",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_ta",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_th",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_tr",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_vi",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xcopa_zh",
    "extractor_file": "xcopa.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xlsum_amharic_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_amharic_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_amharic_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_arabic_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_arabic_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_arabic_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_hausa_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_hausa_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_hausa_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_igbo_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_igbo_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_igbo_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_kirundi_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_kirundi_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_kirundi_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_oromo_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_oromo_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_oromo_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_pidgin_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_pidgin_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_pidgin_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_somali_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_somali_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_somali_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_swahili_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_swahili_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_swahili_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_telugu_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_telugu_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_telugu_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_tigrinya_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_tigrinya_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_tigrinya_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_yoruba_prompt_1",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_yoruba_prompt_2",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xlsum_yoruba_prompt_3",
    "extractor_file": "xlsum.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "rouge1",
      "rouge2",
      "rougeL"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xnli_ar",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_bg",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_de",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_el",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_en",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_es",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_fr",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_hi",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_ru",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_sw",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_th",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_tr",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_ur",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_vi",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xnli_zh",
    "extractor_file": "xnli.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xquad_ar",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_de",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_el",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_en",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_es",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_hi",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_ro",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_ru",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_th",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_tr",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_vi",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xquad_zh",
    "extractor_file": "xquad.py",
    "verified": true,
    "wisent_evaluator": "generation",
    "lm_eval_output_type": "generate_until",
    "lm_eval_metrics": [
      "f1",
      "exact_match"
    ],
    "expected_evaluator": "generation",
    "match": "match",
    "notes": "Wisent uses generation for generate_until - correct mapping"
  },
  {
    "task_name": "xstorycloze_ar",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_en",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_es",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_eu",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_hi",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_id",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_my",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_ru",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_sw",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_te",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xstorycloze_zh",
    "extractor_file": "xstorycloze.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xwinograd_en",
    "extractor_file": "xwinograd.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xwinograd_fr",
    "extractor_file": "xwinograd.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xwinograd_jp",
    "extractor_file": "xwinograd.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xwinograd_pt",
    "extractor_file": "xwinograd.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xwinograd_ru",
    "extractor_file": "xwinograd.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "xwinograd_zh",
    "extractor_file": "xwinograd.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "yahoo_answers_topics",
    "extractor_file": "yahoo.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  },
  {
    "task_name": "zhoblimp",
    "extractor_file": "zhoblimp.py",
    "verified": true,
    "wisent_evaluator": "log_likelihoods",
    "lm_eval_output_type": "multiple_choice",
    "lm_eval_metrics": [
      "acc"
    ],
    "expected_evaluator": "log_likelihoods",
    "match": "match",
    "notes": "Wisent uses log_likelihoods for multiple_choice - correct mapping"
  }
]