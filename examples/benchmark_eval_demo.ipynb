{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wisent-Guard: Benchmark Evaluation Demo\n",
        "\n",
        "This notebook demonstrates a full hallucination detection pipeline using Wisent-Guard on `truthfulqa_mc`.\n",
        "We load a benchmark, generate training data, train a classifier, and evaluate results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Install dependencies (if running for the first time)\n",
        "!pip install -e .\n",
        "!pip install datasets torch transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Imports and configuration\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "from wisent_guard.benchmarking.benchmark_runner import BenchmarkRunner\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Parameters\n",
        "DATASET_NAME = \"truthfulqa_mc\"\n",
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
        "LAYER = 15\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "OUTPUT_DIR = \"benchmark_results\"\n",
        "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Initialize and run benchmark\n",
        "runner = BenchmarkRunner()\n",
        "\n",
        "results = runner.run_benchmark(\n",
        "    benchmark_names=[DATASET_NAME],\n",
        "    model_name=MODEL_NAME,\n",
        "    layer=LAYER,\n",
        "    device=DEVICE,\n",
        "    model=None,\n",
        "    tokenizer=None,\n",
        "    output_dir=OUTPUT_DIR\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Display main results\n",
        "print(\"\\n\ud83d\udcca Evaluation Results:\")\n",
        "for task, metrics in results.items():\n",
        "    print(f\"\\nTask: {task}\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Load results from file (optional)\n",
        "result_file = Path(OUTPUT_DIR) / f\"{DATASET_NAME}_results.json\"\n",
        "if result_file.exists():\n",
        "    with open(result_file) as f:\n",
        "        saved = json.load(f)\n",
        "    print(\"\\n\ud83d\udcbe Saved Results:\")\n",
        "    print(json.dumps(saved, indent=2))\n",
        "else:\n",
        "    print(\"No saved results found. Please run the benchmark first.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}